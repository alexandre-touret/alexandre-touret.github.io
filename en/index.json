[{"categories":null,"content":"Picture Credit: Nick FEWINGS ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:0:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#"},{"categories":null,"content":" IntroductionIn today’s dynamic landscape, Distributed Tracing has emerged as an indispensable practice. It helps to understand what is under the hood of distributed transactions, providing answers to pivotal questions: What comprises these diverse requests? What contextual information accompanies them? How extensive is their duration? Since the introduction of Google’s Dapper, a plethora of tracing solutions has flooded the scene. Among them, OpenTelemetry has risen as the frontrunner. Other alternatives such as Elastic APM and DynaTrace are also available. This toolkit seamlessly aligns with APIs and synchronous transactions, catering to a broad spectrum of scenarios. However, what about asynchronous transactions? The necessity for clarity becomes even more pronounced in such cases. Particularly in architectures built upon messaging or event streaming brokers, attaining a holistic view of the entire transaction becomes arduous. Why does this challenge arise? It’s a consequence of functional transactions fragmenting into two loosely coupled subprocesses: Hopefully you can rope OpenTelemetry in it to shed light. What about the main concepts of Distributed Tracing? I will not dig into the concepts of Distributed tracing in this article. If you are interested in it, you can read my article on the Worldline Tech Blog. I will explain in this article how to set up and plug OpenTelementry to gather asynchronous transaction traces using Apache Camel and Artemis. The first part will use Jaeger and the second one, Tempo and Grafana to be more production ready. All the code snippets are part of this project on GitHub. (Normally) you can use and run it locally on your desktop. ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:1:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#introduction"},{"categories":null,"content":" Jaeger","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#jaeger"},{"categories":null,"content":" ArchitectureThe SPANs are broadcast and gathered through OpenTelemetry Collector. It finally sends them to Jaeger. Here is the architecture of such a platform: ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture"},{"categories":null,"content":" OpenTelemetry CollectorThe cornerstone of this architecture is the collector. It can be compared to Elastic LogStash or an ETL. It will help us get, transform and export telemetry data. Source: https://opentelemetry.io/docs/collector/ For our use case, the configuration is quite simple. First, here is the Docker Compose configuration: otel-collector: image: otel/opentelemetry-collector:0.75.0 container_name: otel-collector command: [ \"--config=/etc/otel-collector-config.yaml\" ] volumes: - ./docker/otel-collector-config.yaml:/etc/otel-collector-config.yaml ports: - \"1888:1888\" # pprof extension - \"8888:8888\" # Prometheus metrics exposed by the collector - \"8889:8889\" # Prometheus exporter metrics - \"13133:13133\" # health_check extension - \"4317:4317\" # OTLP gRPC receiver - \"55670:55679\" # zpages extension and the otel-collector-config.yaml: # (1) receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] # (2) exporters: # prometheus: # endpoint: \"0.0.0.0:8889\" # const_labels: # label1: value1 logging: jaeger: endpoint: jaeger:14250 tls: insecure: true # zipkin: # endpoint: http://zipkin:9411/api/v2/spans # tls: # insecure: true # (3) processors: batch: extensions: health_check: pprof: endpoint: :1888 zpages: endpoint: :55679 # (4) service: extensions: [pprof, zpages, health_check] pipelines: traces: receivers: [otlp] processors: [batch] exporters: [logging, jaeger] metrics: receivers: [otlp] processors: [batch] exporters: [logging] Short explanation If you want further information about this configuration, you can browse the documentation. For those who are impatient, here are a short explanation of this configuration file: Where to pull data? Where to store data? What to do with it? What are the workloads to activate? ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#opentelemetry-collector"},{"categories":null,"content":" What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. Libraries to addFor an Apache Camel based Java application, you need to add this starter first: \u003cdependency\u003e \u003cgroupId\u003eorg.apache.camel.springboot\u003c/groupId\u003e \u003cartifactId\u003ecamel-opentelemetry-starter\u003c/artifactId\u003e \u003c/dependency\u003e In case you set up a basic Spring Boot application, you only have to configure the agent (see below). What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration \u003cprofile\u003e \u003cid\u003eopentelemetry\u003c/id\u003e \u003cactivation\u003e \u003cproperty\u003e \u003cname\u003eapm\u003c/name\u003e \u003cvalue\u003eotel\u003c/value\u003e \u003c/property\u003e \u003c/activation\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003ecopy-javaagent\u003c/id\u003e \u003cphase\u003eprocess-resources\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003ecopy\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003cartifactItems\u003e \u003cartifactItem\u003e \u003cgroupId\u003eio.opentelemetry.javaagent\u003c/groupId\u003e \u003cartifactId\u003eopentelemetry-javaagent\u003c/artifactId\u003e \u003cversion\u003e${opentelemetry-agent.version}\u003c/version\u003e \u003coverWrite\u003etrue\u003c/overWrite\u003e \u003coutputDirectory\u003e${project.build.directory}/javaagents\u003c/outputDirectory\u003e \u003cdestFileName\u003ejavaagent.jar\u003c/destFileName\u003e \u003c/artifactItem\u003e \u003c/artifactItems\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cagents\u003e \u003cagent\u003e${project.build.directory}/javaagents/javaagent.jar\u003c/agent\u003e \u003c/agents\u003e \u003c!-- \u003csystemPropertyVariables\u003e--\u003e \u003c!-- \u003cotel.traces.sampler\u003eparentbased_traceidratio\u003c/otel.traces.sampler\u003e--\u003e \u003c!-- \u003cotel.traces.sampler.arg\u003e0.2\u003c/otel.traces.sampler.arg\u003e--\u003e \u003c!-- \u003c/systemPropertyVariables\u003e--\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/profile\u003e The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .routeId(\"consume-message\") .route","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code"},{"categories":null,"content":" What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .routeId(\"consume-message\") .route","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#libraries-to-add"},{"categories":null,"content":" What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .routeId(\"consume-message\") .route","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code-1"},{"categories":null,"content":" What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .routeId(\"consume-message\") .route","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#the-java-agent"},{"categories":null,"content":" What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .routeId(\"consume-message\") .route","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#how-is-made-the-glue-between-the-two-applications"},{"categories":null,"content":" DashboardTo get traces, I ran this dumb command to inject traces into Jaeger: while true ; http :9080/camel/test; end Now, you can browse Jaeger (http://localhost:16686) and query it to find trace insights: Number of different apps If you dig into one transaction, you will see the whole transaction: One transaction And now, you can correlate two sub transactions: Two sub transactions ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard"},{"categories":null,"content":" Tempo \u0026 GrafanaThis solution is pretty similar to the previous one. Instead of pushing all the data to Jaeger, we will use Tempo to store data and Grafana to render them. We don’t need to modify the configuration made in the existing Java applications. ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo--grafana"},{"categories":null,"content":" ArchitectureAs mentioned above, the architecture is quite the same. Now, we have the collector which broadcast data to Tempo. We will then configure Grafana to query to it to get traces. ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture-1"},{"categories":null,"content":" Collector configurationThe modification of the Collector is easy (for this example). We only have to specify the tempo URL. receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] exporters: otlp: endpoint: tempo:4317 tls: insecure: true service: pipelines: traces: receivers: [otlp] exporters: [otlp] ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#collector-configuration"},{"categories":null,"content":" Tempo configurationI used here the standard configuration provided in the documentation: server: http_listen_port: 3200 distributor: receivers: # this configuration will listen on all ports and protocols that tempo is capable of. jaeger: # the receives all come from the OpenTelemetry collector. more configuration information can protocols: # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver thrift_http: # grpc: # for a production deployment you should only enable the receivers you need! thrift_binary: thrift_compact: zipkin: otlp: protocols: http: grpc: opencensus: ingester: max_block_duration: 5m # cut the headblock when this much time passes. this is being set for demo purposes and should probably be left alone normally compactor: compaction: block_retention: 1h # overall Tempo trace retention. set for demo purposes metrics_generator: registry: external_labels: source: tempo cluster: docker-compose storage: path: /tmp/tempo/generator/wal remote_write: - url: http://prometheus:9090/api/v1/write send_exemplars: true storage: trace: backend: local # backend configuration to use wal: path: /tmp/tempo/wal # where to store the wal locally local: path: /tmp/tempo/blocks overrides: metrics_generator_processors: [service-graphs, span-metrics] # enables metrics generator search_enabled: true ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo-configuration"},{"categories":null,"content":" Grafana configurationNow we must configure Grafana to enable querying into our tempo instance. The configuration is made here using a configuration file provided during the startup The datasource file: apiVersion: 1 datasources: # Prometheus backend where metrics are sent - name: Prometheus type: prometheus uid: prometheus url: http://prometheus:9090 jsonData: httpMethod: GET version: 1 - name: Tempo type: tempo uid: tempo url: http://tempo:3200 jsonData: httpMethod: GET serviceMap: datasourceUid: 'prometheus' version: 1 ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#grafana-configuration"},{"categories":null,"content":" DashboardAs we have done before, we must start the infrastructure using Docker Compose: cd containers docker compose -f docker-compose-grafana.yml up Then, using the same rocket scientist maven commands, we can run the same commands and browse now Grafana (http://localhost:3000) to see our traces: Transactions Deep dive into one transaction ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:5","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard-1"},{"categories":null,"content":" ConclusionWe saw how to highlight asynchronous transactions and correlate them through OpenTelemetry and Jaeger or using Tempo \u0026 Grafana. It was voluntarily simple. If you want to dig into OpenTelemetry Collector configuration, you can read this article from Antik ANAND (Thanks to Nicolas FRANKËL for sharing it) and the official documentation. A noteworthy aspect of OpenTelemetry lies in its evolution into an industry-standard over time. For instance,Elastic APM is compatible with it. I then exposed how to enable this feature on Apache Camel applications. It can be easily reproduced with several stacks. Last but not least, which solution is the best? I have not made any benchmark of Distributed Tracing solutions. However, for a real life production setup, I would dive into Grafana and Tempo and check their features. I am particularly interested in mixing logs, traces to orchestrate efficient alerting mechanisms. ","date":"2023-09-05","objectID":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:4:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/en/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#conclusion"},{"categories":null,"content":"While chatting with one of my WL colleague, I stumbled upon Fish shell. I immediately liked its autocompletion and extensibility mechanisms. After many years using BASH and ZSH, I therefore decided to move on to this new shell. Unlike the others, it’s not POSIX-compatible. Furthermore, to get (at least) the same functionalities as OhMyZsh, I chose to install StarShip. I will then describe how I moved on and updated my existing tools such as SdkMan. ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:0:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#"},{"categories":null,"content":" FISH Installation OS I applied these commands on both Ubuntu20/WSL2 and Linux Mint. To install it, run this command: sudo apt install fish You must also use a font available on the NerdFonts website. By the way, you can also use the fonts available through your package manager. For instance, I chose using JetBrains Mono After downloading it, you can reload your font cache running this command: fc-cache -fv ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:1:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#fish-installation"},{"categories":null,"content":" StarShip installationI ran this command: curl -sS https://starship.rs/install.sh | sh How to update StarShip To update StarShip, you must use the same command. I also added the following command at the end of ~/.config/fish/config.fish: starship init fish | source Due to some WSL2 incompatibilities, I also chose to use the plain text presets running this command: starship preset plain-text-symbols -o ~/.config/starship.toml ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:2:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#starship-installation"},{"categories":null,"content":" SDKMAN updateAt this stage, SdkMan didn’t work at all. To put it alive again, I had to install Fisher and a SdkMan for fish plugin. ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:3:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#sdkman-update"},{"categories":null,"content":" Fisher installRun this command: curl -sL https://raw.githubusercontent.com/jorgebucaran/fisher/main/functions/fisher.fish | source \u0026\u0026 fisher install jorgebucaran/fisher ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:3:1","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#fisher-install"},{"categories":null,"content":" SdkMan for fish pluginRun this command: fisher install reitzig/sdkman-for-fish@v2.0.0 ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:3:2","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#sdkman-for-fish-plugin"},{"categories":null,"content":" Run SdkManRun this command: sdk ug Say yes and restart a shell. Now it should work. ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:3:3","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#run-sdkman"},{"categories":null,"content":" NVMI had the same issue with NVM. I then installed another plugin with Fisher: fisher install jorgebucaran/nvm.fish ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:4:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#nvm"},{"categories":null,"content":" GnuPGI use GnuPG for signing my GIT commits. Installing Fisher broke my setup. I then added this new configuration file $HOME/.config/fish/conf.d/config_gpgagent.fish with the following content: set -gx GPG_TTY /dev/pts/0 To activate it, restart your shell (again). ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:5:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#gnupg"},{"categories":null,"content":" ConclusionI can now use FISH for my daily job. As I said first, this article is only a reminder for my next setups (aka when I will broke my GNU/Linux boxes and try to restore them). Hope it will help you! ","date":"2023-07-21","objectID":"/en/2023/07/21/fish-shell/:6:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/en/2023/07/21/fish-shell/#conclusion"},{"categories":null,"content":" Once upon a time an API … Second Law of Consulting “No matter how it looks at first, it’s always a people problem” - Gerald M. Weinberg Once upon a time, the ACME Corporation was building a brand new IT product. It aimed at a new software to manage bookstores through a web interface and an API. In the first steps, the developers drew up a first roadmap of their API based on the expectations of their first customers. They therefore built and shipped a microservices platform and released their first service contract for their early adopters. Here is the design of this platform: The High level design More in depth To sum up To cut long story short, we have a microservices platform based on the Spring Boot/Cloud Stack exposed through an API Gateway and secured using OpenID Connect. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:1:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#once-upon-a-time-an-api-"},{"categories":null,"content":" The platform and its roadmapAfter shipping it into production, they drew up a roadmap for their existing customers to both improve the existing features and bring new ones. As of now, we could think everything is hunky-dory isn’t it? While engineers worked on improving the existing API, the sales representative have contracted with new customers. They enjoyed this product and its functionalities. However, they also ask for new requirements and concerns. Some of them are easy to apply, some not. For instance, a new customer asked the ACME engineers for getting a summary for every book and additional REST operations. Easy! However, last but not least, this customer would also get a list of authors for every book whereas the existing application only provides ONE author per book. This is a breaking change! What is a breaking change? A breaking change occurs when the backward compatibility is broken between two following versions. For instance, when you completely change the service contract on your API, a client which uses the old API definition is unable to use your new one. A common theoretical approach could be to apply versions on our APIs and adapt it according to the customer. Unfortunately, the devil is in the details. I will describe in this article attention points I struggled with in my last projects. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:2:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#the-platform-and-its-roadmap"},{"categories":null,"content":" What to version? How and where to apply it?After answering to the first question: Do I really need API versioning? you then have to answer to this new one: what should we consider versioning? You only have to version the service contract. In the case of a simple web application based on a GUI and an API Versioning is applied in the service contract of your API. If you change your database without impacting the APIs, why should you waste your time creating and managing a version of your API? It doesn’t make sense. On the other way around, when you evolve your service contract, you usually impact your database (e.g., see the example of breaking change above). Moreover, the version is usually specified on the “middleware” side, where your expose your API. I’ll come back to this point in a later section. If you want to dig into what is a breaking change and what to version, you can read this guide on the GitHub website. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:3:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#what-to-version-how-and-where-to-apply-it"},{"categories":null,"content":" How many versions must I handle?Tough question! Throughout my different experiences struggling with API versioning, the most acceptable trade-off for both the API provider and customer/client was to only handle two versions: the current and the deprecated one. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:3:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#how-many-versions-must-i-handle"},{"categories":null,"content":" Where?Now, you have to answer to this question: Where should I handle the version? On the Gateway? On Every Backend? On every service or on every set of services? Directly in the code managed by different packages. Usually, I prefer manage it on the gateway side and don’t bother with URL management on every backend. It could avoid maintenance on both code and tests for every release. However, you can’t have this approach on monolithic applications (see below). ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:3:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#where"},{"categories":null,"content":" How to define it?Here are three ways to define API versions: In the URL (e.g., /v1/api/books) In a HTTP header (e.g., X-API-VERSION: v1) In the content type (e.g., Accept: application/vnd.myname.v1+json) The last one is now deprecated. The RFC 9110 deprecates now custom usages of the accept HTTP header. I strongly prefer the first one. It is the most straightforward. For instance, if you provide your books API first version, you can declare this URL in your OpenAPI specification:/v1/api/books. The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it’s less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn’t specify the header? Is there any default version? Yes, you can read the documentation to answer these questions, but who (really) does? The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it’s less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn’t specify the header? Is there any default version? Yes, you can read the documentation, but who (really) does? The first solution (i.e., version in the URL) mandatorily conveys the associated version. It is so visible for all the stakeholders and could potentially avoir any mistakes or headaches while debugging. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:3:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#how-to-define-it"},{"categories":null,"content":" What about the main software/cloud providers?Before reinventing the wheel, let’s see how the main actors of our industry deal with this topic. I looked around and found three examples: ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:4:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#what-about-the-main-softwarecloud-providers"},{"categories":null,"content":" Google The version is specified in the URL It only represents the major versions which handle breaking changes ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:4:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#google"},{"categories":null,"content":" Spotify The version is specified in the URL The API version is still V1 … ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:4:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#spotify"},{"categories":null,"content":" Apple The version is specified in the URL The API version is still V1 … ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:4:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#apple"},{"categories":null,"content":" Appropriate (or not) technologiesIn my opinion, technologies based on the monolith pattern don’t fit handling properly API Versioning. If you are not eager to execute two versions of your monolith, you would have to provide both of the two versions within the same app and runtime. You see the point? You would therefore struggle with: packaging testing both of two releases for every deployment even if a new feature doesn’t impact the deprecated version removing, add new releases in the same source code,… And loosing your mind. In my opinion, best associated technologies are more modular whether during the development or deployment phases. For instance, if you built your app with Container based (Docker, Podman, K8S,..) stack, you would easily switch from one version to another, and sometimes you would be able to ship new features without impacting the oldest version. However, we need to set up our development and integration workflow to do that. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:5:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#appropriate-or-not-technologies"},{"categories":null,"content":" Configuration management \u0026 delivery automationWhen I dug into API versioning, I realised it impacts projects organisation and, by this way, the following items: The source code management: one version per branch or not? The release process: How to create releases properly? Fixes, merges,…: How to apply fixes among branches and versions? The delivery process: How to ship you versions? Yes it IS a big deal Here is the least bad approach I think it works while addressing all of these concerns: ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:6:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#configuration-management--delivery-automation"},{"categories":null,"content":" Source code configurationWhen you want to have two different versions in production, you must decouple your work in several GIT (what else) branches. For that, I usually put in place GitFlow. source: Atlassian Usually, using this workflow, we consider the develop branch serves as an integration branch. But, now we have two separate versions? Yes, but don’t forget we have a current version and a deprecated one. SemVer I base my versioning naming and numbers on SemVer To handle API versions, we can use release branches. You can easily declare versions regarding your API versions. For instance: release/book-api-1.0.1 release/book-api-2.0.1 We can so have the following workflow: Develop features in feature branches and merge them into the develop branch. Release and use major release numbers (or whatever) to identify breaking changes and your API version number Create binaries (see below) regarding the tags and release branches created Fix existing branches when you want to backport features brought by new features (e.g., when there is an impact on the database mapping), and release them using minor version numbers Apply fixes and create releases ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:6:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#source-code-configuration"},{"categories":null,"content":" Delivery processAs of now, we saw how to design, create and handle versions. But, how to ship them? If you based your source code management on top of GitFlow, you would be able now to deliver releases available from git tags and release branches. The good point is you can indeed build your binaries on top of these. The bad one, is you must design and automatise this whole process in a CI/CD pipeline. Don’t forget to share it to all the stakeholders, whether developers, integrators or project leaders who are often involved in version definition. Hold on, these programs must be executed against a configuration, aren’t they? Nowadays, if we respect the 12 factors during our design and implementation, the configuration is provided through environment variables. To cut long story short, your API versioning will also impact your configuration. Thus, it becomes mandatory to externalise it and version it. You can do it in different ways. You can, for example, deploy a configuration server. It will provide configuration key/values regarding the version. If you want a live example, you can get an example in a workshop I held this year at SnowcampIO. The configuration is managed by Spring Cloud Config. You can also handle your configuration in your Helm Charts if you deploy your app on top of Kubernetes. Your configuration values will be injected directly during the deployment. Obviously if it’s a monolith, it will be strongly difficult. Why? Because you will lose flexibility on version management and the capacity on deploying several versions of your service. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:6:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#delivery-process"},{"categories":null,"content":" Authorisation managementHere is another point to potentially address when we implement API versioning. When you apply an authorisation mechanism on your APIs using OAuthv2 or OpenID Connect, you would potentially have strong differences in your authorisation policies between two major releases. You would then restrict the usage of a version to specific clients or end users. One way to handle this is to use scopes stored in claims. In the use case we have been digging into, we can declare scopes such as: book:v1:write or number:v2:read to specify both the authorised action and the corresponding version. For example, here is a request to get an access_token from the v1 scopes: http --form post :8009/oauth2/token grant_type=\"client_credentials\" client_id=\"customer1\" client_secret=\"secret1\" scope=\"openid book:v1:write book:v1:write number:v1:read\" And the response could be: { \"access_token\": \"eyJraWQiOiIxNTk4NjZlMC0zNWRjLTQ5MDMtYmQ5MC1hMTM5ZDdjMmYyZjciLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJjdXN0b21lcjIiLCJhdWQiOiJjdXN0b21lcjIiLCJuYmYiOjE2NzI1MDQ0MTQsInNjb3BlIjpbImJvb2t2Mjp3cml0ZSIsIm51bWJlcnYyOnJlYWQiLCJvcGVuaWQiLCJib29rdjI6cmVhZCJdLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgwMDkiLCJleHAiOjE2Nz I1MDQ3MTQsImlhdCI6MTY3MjUwNDQxNH0.gAaDcOaORse0NPIauMVK_rhFATqdKCTvLl41HSr2y80JEj_EHN9bSO5kg2pgkz6KIiauFQ6CT1NJPUlqWO8jc8-e5rMjwWuscRb8flBeQNs4-AkJjbevJeCoQoCi_bewuJy7Y7jqOXiGxglgMBk-0pr5Lt85dkepRaBSSg9vgVnF_X6fyRjXVSXNIDJh7DQcQQ-Li0z5EkeHUIUcXByh19IfiFuw-HmMYXu9EzeewofYj9Gsb_7qI0Ubo2x7y6W2tvzmr2PxkyWbmoioZdY9K0 nP6btskFz2hLjkL_aS9fHJnhS6DS8Sz1J_t95SRUtUrBN8VjA6M-ofbYUi5Pb97Q\", \"expires_in\": 299, \"scope\": \"book:v1:write number:v1:read openid book:v1:read\", \"token_type\": \"Bearer\" } Next, you must validate every API call with the version exposed by your API gateway and the requested scope. When a client tries to reach an API version with inappropriate scopes (e.g., using book:v1:read scope for a client which only uses the v2). You will throw this error: { \"error\": \"invalid_scope\" } ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:7:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#authorisation-management"},{"categories":null,"content":" And now something completely different: How to avoid versioning while evolving your API?You probably understood that versioning is totally cumbersome. Before putting in place all of these practices, there’s another way to add functionalities on a NON-versioned API without impacting your existing customers. You can add new resources, operations and data without impacting your existing users. {: .notice–warning} With the help of serialization rules, your users would only use the data and operations they know and are confident with. You will therefore bring backward compatibility to your API. Just in case, you can anticipate API versioning by declaring a V1 prefix on your API URL and stick to it while it’s not mandatory to upgrade it. That’s how and why Spotify and Apple (see above) still stick to the V1. ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:8:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#and-now-something-completely-different-how-to-avoid-versioning-while-evolving-your-api"},{"categories":null,"content":" Wrap-upYou probably understood when getting into this topic that API versioning is a project management issue with consequences that requires tackling difficult technical ones. To sum up, you need to ask yourself these questions: Do I need it? Can I postpone API versioning by dealing with serialisation rules and just adding new data or operations? Is my architecture design compatible? Are my source code management and delivery practices compatible? After coping with all these points, if you must implement API versioning, you would need to onboard all the different stakeholders, not just developers, to be sure your whole development and delivery process is well aligned with practice. And I forgot: Good luck! ","date":"2023-03-27","objectID":"/en/2023/03/27/rest-api-versioning/:9:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/en/2023/03/27/rest-api-versioning/#wrap-up"},{"categories":null,"content":" Talks You can find below the slides and videos of my latest talks. Additionally, you can also get my slides on SpeakerDeck and videos on this Youtube playlist: ","date":"2022-07-02","objectID":"/en/talks/:0:0","series":null,"tags":null,"title":"Talks","uri":"/en/talks/#"},{"categories":null,"content":" TalksHere is a bunch of talks I presented at various tech events (meetups, conferences). You can find talks in French on the corresponding part of this blog. ","date":"2022-07-02","objectID":"/en/talks/:1:0","series":null,"tags":null,"title":"Talks","uri":"/en/talks/#talks"},{"categories":null,"content":" The Hitchhiker’s guide to software architecture design Abstract Designing a new platform is always tricky to set up. How to start? What is the best strategy to adopt while designing a platform? What kind of architecture should we deploy: event streaming, orchestration, or choreography? For a brand-new platform: “Donuts @ Home”, we will proceed a live architecture study. After analysing the customer needs, brainstorming, and exchanging our ideas, we will choose among all the potential solutions the least worst option. You will be asked to validate our design and the different implementation examples. At the end of this talk, you will have tips and tricks for thinking about it and starting working on architecture studies in complete peace of mind. Slides ","date":"2022-07-02","objectID":"/en/talks/:1:1","series":null,"tags":null,"title":"Talks","uri":"/en/talks/#the-hitchhikers-guide-to-software-architecture-design"},{"categories":null,"content":" Architecture Katas : Improve your system architecture design skills in a funny way Abstract How to learn architecture ? How to improve in this field ? How do we recognize a good or a bad architecture ? Plenty of books and training sessions address this subject. The best thing is to practice! In the same way as CodingDojos, I will present to you architecture katas. Ted NEWARD (http://blogs.tedneward.com/) created them. His idea came from the following observation : “How are we supposed to get great architects, if they only get the chance to architect fewer than a half-dozen times in their career?” One solution to this issue could be to practice regularly on several topics to gain experience. After a brief introduction of how to start designing an architecture, I will present architecture katas, and the conduct. To conclude, after this first try I will present the benefits I had benefited. Slides Video ","date":"2022-07-02","objectID":"/en/talks/:1:2","series":null,"tags":null,"title":"Talks","uri":"/en/talks/#architecture-katas--improve-your-system-architecture-design-skills-in-a-funny-way"},{"categories":null,"content":" Kubernetes \u0026 Co, beyond the hype: 10 tips for designers who want to create cloud native apps Abstract Kubernetes and cloud technologies are nowadays the new standard to deploy different cloud native applications: API, BATCHES, microservices and … monoliths! These technologies help to solve many issues but with some complexity. It could be difficult for developers and designers to identify the constraints of such architectures. In this presentation, you will (re)discover ten tips and pieces of advice I applied and found useful in my last JAVA projects (Spring, JEE). I will talk about: Application ecosystem Choice of technical solutions Development K8S design constraints And more! Slides Video ","date":"2022-07-02","objectID":"/en/talks/:1:3","series":null,"tags":null,"title":"Talks","uri":"/en/talks/#kubernetes--co-beyond-the-hype-10-tips-for-designers-who-want-to-create-cloud-native-apps"},{"categories":null,"content":"I expose on this blog my tests, technical musings and explorations. Among other things, it helps me reminding me what I did months/years ago and sharing it to the community. Obviously, I accept remarks, fixes (nobody is perfect). However, I reserve the right to publish or not a comment. Usually, I publish it except if I consider it as a troll. Stop You probably understood it’s a personal blog and not a professional one. I publish also articles on the Worldline Technical Blog. Finally, views and opinions exposed on this blog are my own and not of my employer. And now, have a good reading! 🙂 ","date":"2018-02-08","objectID":"/en/about/:0:0","series":null,"tags":null,"title":"About","uri":"/en/about/#"},{"categories":null,"content":"You can reach out to me through Twitter or LinkedIn. ","date":"2018-02-08","objectID":"/en/contact/:0:0","series":null,"tags":null,"title":"Contact","uri":"/en/contact/#"}]