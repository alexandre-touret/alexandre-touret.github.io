[{"categories":null,"content":"Il y a deux ans déjà, j’ai migré mon site Wordpress sur un site statique hébergé sur Github Pages. Ce dernier était basé sur Ruby, Jekyll et Minimal mistakes. Bien que le projet Minimal Mistakes ne donnait plus trop de signes de vie, le rendu convenait. Cependant, j’étais bloqué sur différents points: La gestion d’articles en anglais et français Le thème dark (inutile donc indispensable) Quelques fonctionnalités manquantes: par ex. MermaidJS J’ai donc décidé de le migrer sur Hugo. Ce générateur de site est basé sur Go et est très rapide d’exécution. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:0:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#"},{"categories":null,"content":" DémarrageJe n’ai pas migré le site comme indiquait la documentation. J’ai préféré créer un nouveau site et copier coller le contenu existant, à savoir les images et les articles. Je vous conseille de lire la documentation qui est bien faite. Ensuite, j’ai choisi le thème LoveIt. Pour l’installer, il suffit de cloner le repo dans le répertoire themes. git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:1:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#démarrage"},{"categories":null,"content":" Reprise de donnéesJ’ai copié les éléments suivants: Les fichiers statiques que j’avais à disposition (CNAME, robots.txt) dans le répertoire static Les images dansle répertoire static/assets/images Les posts et pages statiques ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:2:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#reprise-de-données"},{"categories":null,"content":" Travail sur les posts et images","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#travail-sur-les-posts-et-images"},{"categories":null,"content":" Les postsJ’ai ensuite modifié les noms des fichiers en enlevant les dates qui les préfixaient. Ensuite, j’ai modifié les en-têtes de chaque post. J’ai pu le faire en automatisant avec VS CODE. Voici le pattern que j’ai modifié: header: teaser: /assets/images/2018/02/2000px-cygwin_logo-svg.png en featuredImagePreview: /assets/images/2022/12/review.webp Ca c’était le plus facile… ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-posts"},{"categories":null,"content":" Les imagesDans chaque post, j’ai revu les images et leur positionnement. J’ai donc passé chaque article manuellement. C’était réellement fastidieux. Pour être totalement franc, je n’ai paas cherché à automatiser ça. Je pense qu’un script shell, python aurait pu faire l’affaire. Heureusement, je n’en avais pas une centaine… J’ai ajouté quand je pouvais l’en-tête suivant (adaptez le chemin vers l’image ;-) ): featuredImage: /assets/images/2022/12/review.webp images: [\"/assets/images/2022/12/review.webp\"] Le premier attribut permet d’avoir une image d’en-tête pour l’article. Le second permet d’avoir l’image lors d’un partage sur un réseau social (ex. Twitter) j’ai ajouté le code suivant pour centrer les images: {{\u003c style \"text-align:center\" \u003e}} ![dataflow](/assets/images/2022/08/maksym-tymchyk-vHO-yT1BDWk-unsplash.webp) {{\u003c style \u003e}} ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-images"},{"categories":null,"content":" ConfigurationPour garder les mêmes URLs, j’ai choisi de modifier le pattern d’ URL pour inclure la date. C’est un vieux reliquat de mon blog Wordpress. [languages.fr.permalinks] posts = '/:year/:month/:day/:filename/' Pour le reste, j’ai copié collé l’exemple fourni par le thème et renseigné les champs en fonction de ce que je voulais. J’ai ensuite adapté le multi langue pour avoir la possibilité de faire des articles en anglais et en français. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#configuration"},{"categories":null,"content":" Moteur de rechercheJ’utilise Lunr. Voici la configuration: [languages.en.params.search] enable = true type = \"lunr\" contentLength = 4000 placeholder = \"\" maxResultLength = 10 snippetLength = 50 highlightTag = \"em\" absoluteURL = false Il faut également penser à activer la sortie au format JSON: # Options to make hugo output files [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] page = [\"HTML\", \"MarkDown\"] section = [\"HTML\", \"RSS\"] taxonomy = [\"HTML\", \"RSS\"] taxonomyTerm = [\"HTML\"] ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#moteur-de-recherche"},{"categories":null,"content":" CommentairesA l’instar de mon blog avec Jekyll, j’utilise Utteranc.es. Voici la configuration: [params.page.comment.utterances] enable = true # owner/repo repo = \"alexandre-touret/alexandre-touret.github.io\" issueTerm = \"pathname\" label = \"\" lightTheme = \"github-light\" darkTheme = \"github-dark\" ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#commentaires"},{"categories":null,"content":" ConclusionVous aurez bien compris que ma motivation principale derrière cette migration était d’ avoir un support multi langue un peu sympa. Vous avez dans cet article les principales actions que j’ai réalisé. N’hésitez pas à regarder la configuration et les articles pour plus de détails. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:5:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#conclusion"},{"categories":null,"content":"You can read the English version below ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:0:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#"},{"categories":null,"content":" 2022 en quelques mots2023 est tout proche. Il est temps de faire un rapide bilan sur cette année 2022 (d’un point pro). Après avoir changé de projet en début d’année, j’ai pu, grâce à mon employeur Worldline, participer en tant que speaker à 8 conférences en français et anglais 2 meetups une présentation en ligne à Malt Academy J’ai également écrit 6 articles sur mon blog et 4 sur le blog d’ingénierie de Worldline. Je tiens à remercier mon employeur pour me permettre de vivre cette expérience ainsi que les organisatrices et organisateurs pour leur confiance et leur accueil (vous vous reconnaitrez). Merci également à toutes celles et ceux qui m’ont aidé également à monter en compétence en tant que speaker. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:1:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-en-quelques-mots"},{"categories":null,"content":" Et maintenant?Grâce à l’initiative 1Tech-Rel de Worldline, j’ai pu faire des super rencontres et progresser techniquement dans certains domaines : Préparer et proposer un sujet technique vous impose de le creuser à fond et de le maîtriser. Pour l’année prochaine, on a quelques idées. J’espère pouvoir communiquer là-dessus rapidement. Je vous souhaite en attendant un bon réveillon et si vous découvrez mon article en 2023 une bonne année. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:2:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#et-maintenant"},{"categories":null,"content":" 2022 in few words2023 is coming. It is time to review 2022 in a professional point of view. After moving on to another project early 2022, I had the opportunity on behalf of my company Worldline, to participate as a speaker to: 8 tech conferences 2 meetups 1 online presentation at Malt Academy I also wrote 6 articles on my blog and 4 on the Worldline engineering blog. Thank you to my employer for giving me this opportunity and the organisers for their trust and their hospitality. I’m pretty sure you will recognise yourselves. Thanks then to who helped me in my speaker journey. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:3:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-in-few-words"},{"categories":null,"content":" And now, something completely different?On behalf of the 2Worldline tech rel initiative, I had the chance to meet great people and move forward in some technical domains. Drawing up and submitting a talk about a technical topic requires you to dig into it and be proficient on it. I already have ideas for the next year. I hope communicating about it shortly. Best wishes for the new year eve and if you come across this article in 2023: Happy new year!! c’est comme des dev rel mais on essaye d’inclure au delà des devs (ex. les OPS, SRE). ↩︎ It is just like dev rel, but we would like to include all the tech crew (e.g., OPS \u0026 SRE) ↩︎ ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:4:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#and-now-something-completely-different"},{"categories":null,"content":"Pour ce dernier article de l’année 2022, voici un rapide retour d’expérience. Je suis actuellement en cours de préparation d’un workshop pour l’édition 2023 de SnowcampIO. J’aborderai dans ce dernier le versioning des APIs REST. Pour illustrer ce sujet ô combien épineux, j’ai réalisé une plateforme “microservices” en utilisant différents composants de la stack Spring. Container Tools Comments API Gateway Spring Cloud Gateway 2022.0.0-RC2 Bookstore API JAVA 17,Spring Boot 3.0.X ISBN API JAVA 17,Spring Boot 3.0.X Configuration Server Spring Cloud Config 2022.0.0-RC2 Database PostgreSQL Authorization Server JAVA 17,Spring Boot 3.0.X, Spring Authorization Server 1.0.0 En résumé, j’utilise Spring Boot, Cloud, Security, Authorization Server, Circuit Breaker, Spring Data,… J’ai démarré le développement avant l’annonce officielle de la version 3.0 de Spring Boot. Ce n’était pas réellement obligatoire pour cet atelier, mais j’ai souhaité quand même migrer cette application dans la dernière version de Spring Boot/Framework. Je vais décrire dans cet article comment j’ai réussi à migrer toute cette stack et les choix que j’ai fait pour que ça fonctionne. Bien évidemment, cette application n’est pas une “vraie” application en production. Par exemple, je n’ai qu’une seule entité JPA… Cependant, je la trouve représentative et espère (très modestement) que mon retour d’expérience pourra servir. La Pull Request correspondante est disponible sur GitHub. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:0:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#"},{"categories":null,"content":" Pré-requisUne documentation existe. Vous pouvez la consulter ici. Il existe aussi plusieurs articles sur le blog du projet Spring. Voici un exemple. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:1:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#pré-requis"},{"categories":null,"content":" Dépendances et configuration des plugins","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#dépendances-et-configuration-des-plugins"},{"categories":null,"content":" JDKPour Spring Boot 3, il faut impérativement utiliser un JDK \u003e=17. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:1","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#jdk"},{"categories":null,"content":" Mises à jourL’une des premières actions à réaliser est de migrer votre application vers la version 2.7. À l’heure où j’écris cet article, la version de Spring Cloud est encore en version RC. J’ai donc dû ajouter le repository “milestone” de Spring: repositories { maven { url 'https://repo.spring.io/milestone' } mavenCentral() } Ensuite, j’ai utilisé les versions suivantes pour les différents composants spring: Spring Boot : 3.0.0 Spring Cloud : 2022.0.0-RC2 Spring Dependency Management : 1.1.0 Dans mon application, j’utilisais certains plugins Gradle pour la génération du code notamment OpenAPIGenerator. Pour ce dernier, j’ai ajouté un paramètre pour le rendre compatible avec spring boot 3: useSpringBoot3 : \"true\" Bref, il faut impérativement tous les mettre à jour et vérifier la compatibilité ! ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:2","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#mises-à-jour"},{"categories":null,"content":" Ajout de nouvelles dépendancesPour vérifier la pertinence de certaines propriétés dans la nouvelle version, Spring a mis à disposition ce plugin: runtimeOnly 'org.springframework.boot:spring-boot-properties-migrator' Il permet de notifier à l’exécution si un paramètre est déprécié ou totalement inutile. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:3:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#ajout-de-nouvelles-dépendances"},{"categories":null,"content":" Migration namespace javax vers jakartaeeSelon votre code, les dépendances que vous pouvez avoir, cette étape pourra aller du renommage des import javax vers jakarta à d’innombrables maux de tête. Si vous utilisez Spring Boot au-dessus d’un Tomcat (c.-à-d. en mode old school), il vous faudra mettre à jour le conteneur de servlet à une version compatible. Dans mon application, je n’ai eu qu’à modifier les imports dans les entités, filtres et méthodes annotées par l’annotation @PostConstruct(). import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.GeneratedValue; import jakarta.persistence.GenerationType; ... Sur ce sujet, Jetbrains a publié un tutoriel sur la migration vers Jakarta. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:4:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#migration-namespace-javax-vers-jakartaee"},{"categories":null,"content":" Distributed Tracing et observabilitéSpring embarque désormais plusieurs fonctionnalités liées à l’observabilité sous forme de starters. Dans mon cas, j’avais embarqué opentracing (qui était déprécié depuis quelques temps) et me connectait sur Jaeger. J’ai suivi cet article paru sur le blog de Spring. J’ai par conséquent basculé sur Zipkin (pour mon Workshop, l’utilisation du distributed tracing est un peu la cerise sur le gâteau). Voici les starters que j’ai intégrés : implementation 'io.micrometer:micrometer-tracing-bridge-brave' implementation 'io.zipkin.reporter2:zipkin-reporter-brave' implementation 'io.opentelemetry:opentelemetry-exporter-zipkin' implementation 'org.springframework.boot:spring-boot-starter-aop' J’ai par la suite intégré les propriétés suivantes dans la configuration: spring: zipkin: base-url: http://localhost:9411 sender: type: web management: tracing: sampling: probability: 1.0 metrics: distribution: percentiles-histogram: http: server: requests: true Je pense que j’aurai pu faire fonctionner Jaeger. Je n’ai pas voulu perdre de temps (SnowcampIO arrive bientôt…). ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:5:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#distributed-tracing-et-observabilité"},{"categories":null,"content":" SecuritéJ’ai eu quelques soucis après avoir mis à jour Spring Authorization Server et Spring Security. Je pense que la version précédente de Spring était plus permissive sur l’injection et le nom des beans chargés dans les classes Configuration. J’ai donc revu la validation côté gateway et plus particulièrement la validation du jeton JWT. J’ai dû notamment ajouter le paramètre jwk-set-uri qui est obligatoire maintenant : resourceserver: jwt: jwk-set-uri: http://localhost:8009 Je n’ai pas eu de réels problèmes coté Authorization Server car j’avais déjà migré vers la version 0.4.0. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:6:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#securité"},{"categories":null,"content":" ConclusionVous l’aurez compris, si vous faites l’effort de suivre régulièrement les versions de Spring, vous devriez venir à bout facilement de la migration vers la dernière version de Spring. Néanmoins, sur des projets conséquents (et je ne parle pas de ceux où il n’y a de tests automatisés…) ça peut s’avérer coûteux. Certaines actions et contournements peuvent prendre du temps (ex. javax –\u003e jakarta). Enfin, je vous conseille d’attendre la première version mineure et la version définitive de Spring Cloud avant de vous lancer pour “de vrai”. Bien que Spring ait fait un effort de documentation pour la migration, il est plus sage d’attendre que les premiers correctifs soient publiés avant de vous lancer. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:7:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#conclusion"},{"categories":null,"content":"Après trois ans d’inactivité pour des raisons que l’on connait malheureusement toutes et tous, Devoxx Belgium était de retour à Anvers. Je n’avais jamais participé (en vrai) à une conférence internationale. C’était donc une première pour moi. Pour y aller, j’ai eu trois fois de la chance: J’ai eu cette opportunité grâce à Worldline - mon employeur J’ai réussi à avoir un billet pendant les cinq minutes où se sont vendus les billets lors du premier batch Ma présentation au format Quickie a été retenue. J’ai présenté un talk à Devoxx!!!!!!! Voici mon retour d’expérience des trois jours de conférence. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:0:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#"},{"categories":null,"content":" Impressions généralesTout d’abord, j’ai pu assister à de nombreux Devoxx France. J’ai cru naïvement que les deux évènements se ressembleraient. Je me suis trompé. Je ne dirai pas lequel est le meilleur1. Je ne saurais le dire. On est sur un autre type de conférence. Il y a un peu moins de feedback de la communauté, même si il y a eu une présentation de Doctolib et plus de présentations réalisées par des grands acteurs du marché ou par des grands speakers internationaux (ex. Simon Ritter, Simon Brown ou James Gosling). Aussi, alors que la ligne éditoriale de Devoxx France s’est tournée au fil des années sur d’autres langages et plateformes telles que NodeJS, Go, Scala, ici on est dans du Java pur et dur. Les (très) grands speakers de l’écosystème sont présents et on fait des super talks: James Gosling, Simon Ritter, Mario Fusco, Gavin King ou José Paumard. En résumé, le coeur de la communauté Java bat à Anvers pendant une semaine. Une majorité de Java champions sont +/- présents et nous font partager leur expertise. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:1:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#impressions-générales"},{"categories":null,"content":" Les tendancesLes grandes tendances étaient: L’IA et les applications Le projet Loom GraalVM ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:2:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#les-tendances"},{"categories":null,"content":" Quelques conférencesL’ensemble des conférences est déjà publié sur Youtube. N’ hésitez pas à les consulter. Il y a beaucoup de talks de qualité. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#quelques-conférences"},{"categories":null,"content":" Artificial Intelligence: You Are Here by Alan D ThompsonLe Dr Alan D. Thompson est un expert en intelligence artificielle. Il nous a donné une présentation pendant la keynote sur ce que l’ IA peut réellement faire de nos jours. C’est de plus en plus utilisé dans notre industrie au travers de Github Copilot, Codegeex,… Après nous avoir rappellé la timeline de l’adoption de l’ IA, il a illustré avec des peintures déssinées par une IA comment un ordinateur peut maintenant comprendre une phrase en langage naturel et la traduire en image. Il a également présenté le langage de modélisation GPT3. Vous pouvez trouver la vidéo ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:1","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#artificial-intelligence-you-are-here-by-alan-d-thompson"},{"categories":null,"content":" Revolutionizing Java-Based Applications with GraalVM by Alina Yurenko and Thomas WuerthingerDans cette présentation, les présentateurs d’Oracle ont abordé une autre grande tendance du marché: le retour au natif qui permet de limiter l’impact sur le démarrage, la mémoire et la taille des packages. Au travers d’un exemple basé sur Micronaut, ils ont expliqué comment GraalVM peut répondre à ces enjeux. Ils ont également démystifié plusieurs mythes liés à GraalVM. Par exemple, GraalVM supporte la réflexion (… et parfois non). On peut utiliser également [Java Flight Recorder](Java Flight Recorder) pendant la compilation. Le support à l’exécution des applications est bientôt prévu. La developer experience était également à l’ordre du jour. Comment offrir une bonne expérience alors que la compilation prend plus de temps? Pour répondre à cette épineuse question, ils ont conseillé de gardé le mode JIT avec une JVM pendant le développement et d’utiliser l’ AOT pour le déploiement final. Ceci permet de disposer d’une machine puissante et de garantir la compatibilité matérielle et OS de la machine de production. La vidéo est disponible ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:4:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#revolutionizing-java-based-applications-with-graalvm-by-alina-yurenko-and-thomas-wuerthinger"},{"categories":null,"content":" The lost art of software design by Simon BrownJ’utilise le modèle C4 depuis plusieurs années. Je l’ai même présenté très brièvement dans mon talk. Aussi, j’ai été très impressionné quand j’ai pu assister à la présentation de Simon Brown sur la conception logicielle. Il a expliqué pourquoi la conception n’était pas conflictuelle avec les méthodes agiles. Ca permet notamment d’ identtifier et de gérer les risques. Au delà du modèle C4, il a montré comment identifier et évaluer les différents risques avec le “Risk Storming”. Enfin, il a répondu à la question à un million d’euros: “Quand arrêter la conception?” Vous trouverez la réponse ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-lost-art-of-software-design-by-simon-brown"},{"categories":null,"content":" Ahead Of Time and Native in Spring Boot 3.0 by Stéphane Nicoll \u0026 Brian ClozelUne autre conférence qui met en avant GraalVM! Cette fois on abordait le support de l’ AOT et du mode natif dans la future version de Spring Boot. Les présentateurs ont expliqués comment Spring supportait le mode natif: le processus appliqué, la gestion des métadonnées et l’analyse réalisée. En (très très bref) résumé, l’ AOT génère des sources dont le chargement des Bean Definition. Ils ont également pointé du doigt des changements que je considère bloquants: On ne peut pas utiliser changer les profils au runtime La surcharge des propriétés et variable d’environnement n’est pas possible à l’exécution On ne peut pas utiliser de Java Agent. Pour ce dernier point, cela risque de poser de nombreux soucis que ça soit l’utilisation d’un APM tel que Dynatrace ou le support de l’AOP. A la fin de cette présentation, ils ont donné quelques recommendations. Parmi celles-ci: Exécuter en développement l’application en mode AOT avec une JVM Exécuter les tests en mode natif Vous trouverez la vidéo ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:1","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#ahead-of-time-and-native-in-spring-boot-30-by-stéphane-nicoll--brian-clozel"},{"categories":null,"content":" The Art of Java Language Pattern Matching by Simon RitterSimon Ritter a exploré toutes les possibilité du pattern matching en Java. Toutes les fonctionnalités ne sont pas encore disponibles. On peut néanmoins faire beaucoup de choses. Après un rappel sur les nouvelles fonctionnalités depuis le JDK11 (Sealed classes, Records), Simon Ritter a illustré leur utilisation dans ce contexte. Si vous voulez tout connaître sur cette fonctionnalité, je vous conseille fortement de regarder ce talk. Voici la vidéo ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:2","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-art-of-java-language-pattern-matching-by-simon-ritter"},{"categories":null,"content":" ConclusionVoila les quelques talks qui m’ont interpellé. Il y en a beaucoup d’autres tels que ceux de Julien TOPCU ou Marcy ERICKA CHARELLOIS. Cette première participation était très enrichissante. J’ai eu à plusieurs reprises l’impression d’avoir l’information à la source (ex. pour Spring). Si j’ai autant de chance, je pense rééditer l’expérience l’année prochaine. En tant que speaker pour une conférence ou un workshop? Seul l’avenir nous le dira! Vous savez, moi je ne crois pas qu’il y aient de bonnes ou mauvaises conférences. Moi, si je devais résumer ma vie aujourd’hui avec vous, je dirais que c’est d’abord des rencontres, des gens qui m’ont tendu la main… ↩︎ ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:6:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#conclusion"},{"categories":null,"content":"Dans mon dernier article, j’ai tenté de faire un état des lieux des solutions possibles pour implémenter des batchs cloud natifs. J’ai par la suite testé plus en détails les jobs et cron jobs Kubernetes en essayant d’avoir une vue OPS sur ce sujet. Le principal inconvénient (qui ne l’est pas dans certains cas) des jobs est qu’on ne peut pas les rejouer. Si ces derniers sont terminés avec succès - Vous allez me dire, il faut bien les coder - mais qu’on souhaite les rejouer pour diverses raisons, on doit les supprimer et relancer. J’ai vu plusieurs posts sur StackOverflow à ce sujet, je n’ai pas trouvé de solutions satisfaisantes relatifs à ce sujet. Attention, je ne dis pas que les jobs et cron jobs ne doivent pas être utilisés. Loin de là. Je pense que si vous avez besoin d’un traitement sans chaînage d’actions, sans rejeu, les jobs et cron jobs sont de bonnes options. Le monitoring et reporting des actions réalisées peut se faire par l’observabilité mise en place dans votre cluster K8S. Après plusieurs recherches, je suis tombé sur Spring Data Flow. L’offre de ce module de Spring Cloud va au delà des batchs. Il permet notamment de gérer le streaming via une interface graphique ou via son API. Dans cet article, je vais implémenter un exemple et le déployer dans Minikube. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:0:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#"},{"categories":null,"content":" Installation et configuration de MinikubeL’installation de minikube est décrite sur le site officiel. Pour l’installer, j’ai exécuté les commandes suivantes: curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Au premier démarrage, vous finirez l’installation minikube start ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-et-configuration-de-minikube"},{"categories":null,"content":" Installation de Spring Cloud Data FlowPour installer Spring Cloud Data Flow directement dans Kubernetes, vous pouvez exécuter les commandes suivantes: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release bitnami/spring-cloud-dataflow Après quelques minutes de téléchargement, vous devriez avoir le retour suivante à l’exécution de la commande kubectl get pods kubectl get pods ~ » kubectl get pods NAME READY STATUS RESTARTS AGE dataflow-mariadb-0 1/1 Running 1 (24h ago) 24h dataflow-rabbitmq-0 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-server-75db59d6cb-lrwp8 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-skipper-9db568cf4-rzsqq 1/1 Running 1 (24h ago) 24h ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-de-spring-cloud-data-flow"},{"categories":null,"content":" Accès au dashboardPour accéder au dashboard de Spring Cloud Data Flow, vous pouvez lancer les commandes suivantes: export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services dataflow-spring-cloud-dataflow-server) kubectl port-forward --namespace default svc/dataflow-spring-cloud-dataflow-server ${SERVICE_PORT}:${SERVICE_PORT} Ensuite, vous pourrez accéder à la console web via l’URL http://localhost:8080/dashboard. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#accès-au-dashboard"},{"categories":null,"content":" Développement d’une TaskJ’ai crée une simple task qui va rechercher la nationalité d’un prénom. Pour ceci, j’utilise l’API https://api.nationalize.io/. On passe un prénom en paramètre et on obtient une liste de nationalités possibles avec leurs probabilités. Vous trouverez les sources de cet exemple sur mon Github. Aussi, la documentation est bien faite, il suffit de la lire. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#développement-dune-task"},{"categories":null,"content":" InitialisationJ’ai initié un projet Spring avec les dépendances suivantes: dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.cloud:spring-cloud-starter-task' developmentOnly 'org.springframework.boot:spring-boot-devtools' testImplementation 'org.springframework.boot:spring-boot-starter-test' implementation 'org.springframework.boot:spring-boot-starter-jdbc' runtimeOnly 'org.mariadb.jdbc:mariadb-java-client' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } Attention, les starters et dépendances JDBC/MariaDB sont indispensables pour que votre tâche puisse enregistrer le statut des exécutions. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#initialisation"},{"categories":null,"content":" Construction de la tâcheUne tâche se crée facilement en annotation une classe “Configuration” par l’annotation @EnableTask @Configuration @EnableTask public class TaskConfiguration { ... } Ensuite l’essentiel du job s’effectue dans la construction d’un bean CommandLineRunner : @Bean public CommandLineRunner createCommandLineRunner(RestTemplate restTemplate) { return args -\u003e { var commandLinePropertySource = new SimpleCommandLinePropertySource(args); var entity = restTemplate.getForEntity(\"https://api.nationalize.io/?name=\" + Optional.ofNullable(commandLinePropertySource.getProperty(\"name\")).orElse(\"BLANK\"), NationalizeResponseDTO.class); LOGGER.info(\"RESPONSE[{}]: {}\", entity.getStatusCode(), entity.getBody()); }; } Dans mon exemple, j’affiche dans la sortie standard le payload de l’API ainsi que le code HTTP de la réponse. Voici un exemple d’exécution : 2022-08-12 15:11:07.885 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2022-08-12 15:11:07.894 INFO 1 --- [ main] i.t.b.cloudtask.CloudTaskApplication : Started CloudTaskApplication in 17.704 seconds (JVM running for 19.18) 2022-08-12 15:11:10.722 INFO 1 --- [ main] i.t.batch.cloudtask.TaskConfiguration : RESPONSE[200 OK]: NationalizeResponseDTO{name='Alexandre', countries=[CountryDTO{countryId='BR', pr.... ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#construction-de-la-tâche"},{"categories":null,"content":" PackagingIci rien de nouveau, il suffit de lancer la commande: ./gradlew build ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#packaging"},{"categories":null,"content":" Déploiement","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#déploiement"},{"categories":null,"content":" Création et déploiement de l’image DockerPour déployer notre toute nouvelle tâche, nous allons d’abord créer l’image Docker avec buildpack. Tout d’abord on va se brancher sur minikube pour que notre image soit déployée dans le repository de minikube eval $(minikube docker-env) Ensuite, il nous reste à créer l’image Docker ./gradlew bootBuildImage --imageName=info.touret/cloud-task:latest Pour vérifier que votre image est bien présente dans minikube, vous pouvez exécuter la commande suivante: minikube image ls | grep cloud-task info.touret/cloud-task:latest ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-et-déploiement-de-limage-docker"},{"categories":null,"content":" Création de l’applicationAvant de créer la tâche dans l’interface, il faut d’abord référencer l’image Docker en créer une application: Il faut déclarer l’image Docker avec le formalisme présenté dans la capture d’écran. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-de-lapplication"},{"categories":null,"content":" Création de la tâcheVoici les différentes actions que j’ai réalisé via l’interface: Vous trouverez plus de détails dans la documentation officielle. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-de-la-tâche"},{"categories":null,"content":" ExécutionMaintenant, il nous est possible de lancer notre tâche. Vous trouverez dans les copies d’écran ci-dessous les différentes actions que j’ai réalisé pour exécuter ma toute nouvelle tâche. J’ai pu également accéder aux logs. Il est également important de noter qu’ après l’exécution d’une tâche, le POD est toujours au statut RUNNING afin que Kubernetes ne redémarre pas automatiquement le traitement. kubectl get pods | grep cloud-task cloud-task-7mp72gzpwo 1/1 Running 0 57m cloud-task-pymdkr182p 1/1 Running 0 65m A chaque exécution il y aura donc un pod d’alloué. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:4:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#exécution"},{"categories":null,"content":" Aller plus loinParmi les fonctionnalités que j’ai découvert, on peut : relancer un traitement le programmer nettoyer les exécutions les pistes d’audit le chaînage des différentes tâches Gros inconvénient pour le nettoyage: je n’ai pas constaté un impact dans les pods alloués. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:5:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#aller-plus-loin"},{"categories":null,"content":" ConclusionPour résumer, je vais me risquer à comparer les deux solutions jobs/cron jobs Kubernetes et une solution basée sur Spring Cloud Dataflow. Je vais donc utiliser la liste des caractéristiques présentée par M. Richards et N. Ford dans leur livre: Fundamentals of Software Architecture1. Bien évidemment, cette notation est purement personnelle. Vous noterez que selon où on positionne le curseur, l’une des deux solutions peut s’avérer meilleure (ou pas). Bref, tout dépend de vos contraintes et de ce que vous souhaitez en faire. A mon avis, une solution telle que Spring Cloud Dataflow s’inscrit parfaitement pour des traitements mixtes (streaming, batch) et pour des traitements Big Data. N’hésitez pas à me donner votre avis (sans troller svp) en commentaire ou si ça concerne l’exemple, directement dans Github. Architecture characteristic K8s job rating Spring Cloud Dataflow rating Partitioning type Domain \u0026 technical Domain \u0026 technical Number of quanta 2 1 1 to many Deployability ⭐⭐⭐⭐ ⭐⭐⭐⭐ Elasticity ⭐⭐⭐ ⭐⭐⭐⭐ Evolutionary ⭐⭐⭐ ⭐⭐⭐⭐ Fault Tolerance ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ Modularity ⭐⭐⭐ ⭐⭐⭐⭐⭐ Overall cost ⭐⭐⭐⭐ ⭐⭐⭐ Performance ⭐⭐⭐⭐⭐ ⭐⭐⭐ Reliability ⭐⭐⭐⭐ ⭐⭐⭐ Scalability ⭐⭐⭐⭐ ⭐⭐⭐⭐ Simplicity ⭐⭐⭐⭐⭐ ⭐⭐⭐ Testability ⭐⭐⭐ ⭐⭐⭐⭐ A lire absolument! ↩︎ ~ Nombre de livrables indépendants fortement couplés ↩︎ ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:6:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#conclusion"},{"categories":null,"content":" Talks Vous trouverez ci-dessous les slides et vidéos de mes derniers talks. Vous pourrez également retrouver l’ensemble des slides sur SpeakerDeck ainsi que les vidéos dans cette playlist Youtube: ","date":"2022-07-02","objectID":"/talks/:0:0","series":null,"tags":null,"title":"Conférences","uri":"/talks/#"},{"categories":null,"content":" ConférencesVoici une liste non exhaustive des sujets qu’il m’a été donné de présenter à différents évènements (meetups, conférences). Vous pourrez également retrouver les conférences que j’ai pu donner en anglais ici. ","date":"2022-07-02","objectID":"/talks/:1:0","series":null,"tags":null,"title":"Conférences","uri":"/talks/#conférences"},{"categories":null,"content":" Une plateforme à concevoir, deux architectes: trois possibilités Résumé La conception d’une plateforme est toujours délicate à initier. Comment démarrer? Quelle est la démarche à adopter pour concevoir une architecture? Quel est le modèle à appliquer: event streaming, orchestration ou chorégraphie? Au travers d’un besoin utilisateur, nous prendrons notre “casquette” d’architecte et déroulerons devant vous une étude pour une toute nouvelle plateforme “Donut @ Home”. Après avoir analysé le besoin, confronté nos idées et convictions devant vous, nous choisirons, parmi toutes les solutions possibles, la “moins pire”. Nous vous solliciterons pour valider notre conception et les exemples d’implémentation possibles. A la fin de cette présentation, vous aurez des clés pour penser et démarrer les études de vos architectures en toute sérénité (ou presque). Slides Vidéo ","date":"2022-07-02","objectID":"/talks/:1:1","series":null,"tags":null,"title":"Conférences","uri":"/talks/#une-plateforme-à-concevoir-deux-architectes-trois-possibilités"},{"categories":null,"content":" Améliorer les compétences et les infrastructures avec les katas d’architecture Résumé Comment devenir architecte ? Comment progresser dans ce domaine ? A quoi reconnaît-on une bonne ou une mauvaise architecture ? Il existe certes un grand nombre d’ ouvrages et formations sur le sujet, mais le mieux est quand même de pratiquer ! Une solution à ce problème pourrait être donc de s’entraîner régulièrement sur des sujets différents pour gagner en expérience. Pour rendre l’apprentissage de l’architecture et de la conception plus empirique - et plus ludique, nous allons découvrir les Katas d’Architecture au travers d’un retour d’expérience. Nous découvrirons comment les mettre en œuvre ainsi que les bénéfices que j’en ai tirés. Slides Vidéo ","date":"2022-07-02","objectID":"/talks/:1:2","series":null,"tags":null,"title":"Conférences","uri":"/talks/#améliorer-les-compétences-et-les-infrastructures-avec-les-katas-darchitecture"},{"categories":null,"content":" Le versioning des APIs REST par la pratique (Workshop) Résumé Quand on souhaite publier des APIs avec par exemple, une solution d’ API Management, on évoque régulièrement le versioning. Cette pratique répond à des contraintes projet mais apporte malheureusement son lot de complexité. Imaginez, vous travaillez sur un produit qui expose des APIs à plusieurs clients. Vous devez leur proposer des évolutions et nouvelles fonctionnalités tout en maîtrisant l’existant. Comment faire évoluer et proposer vos APIs à certains clients sans pénaliser les autres? Quelle stratégie adopter? Quelles solutions techniques peut-on mettre en place simplement? Lors de cet atelier, vous (re)découvrirez et mettrez en pratique des conseils que j’ai pu mettre en oeuvre et qui m’ont aidé lors de mes derniers projets. Au travers d’un cas concret basé sur une architecture microservices, nous définirons la stratégie à mettre en œuvre, les différentes possibilités d’implémentation ainsi que leurs contraintes. Nous les challengerons ensuite en apportant différentes évolutions (ajout d’un nouveau client ou de nouvelles fonctionnalités). A l’issue de cet atelier, nous aurons une vue complète et mis en pratique différentes manières d’appréhender le versioning d’APIs. Slides Workshop ","date":"2022-07-02","objectID":"/talks/:1:3","series":null,"tags":null,"title":"Conférences","uri":"/talks/#le-versioning-des-apis-rest-par-la-pratique-workshop"},{"categories":null,"content":" Java dans le cloud: avec Spring ou Quarkus? Résumé Né au début des années 2000, le framework Spring a mis en avant la facilité de développement. Plus récemment, il a su s’adapter aux contraintes techniques liées aux applications cloud-natives. De son coté, Quarkus est plus récent, il est né en 2019 sur la base de MicroProfile avec un objectif clair : tirer le meilleur parti des plateformes Kubernetes en se concentrant sur un démarrage rapide et une faible empreinte mémoire. Nous avons donc un champion et un outsider ! Maintenant lequel choisir en fonction de vos besoins et votre contexte ? Spring ou Quarkus ? Au travers d’une démonstration “live” nous présenterons un cas contret basé sur notre expérience. Ce dernier sera implémenté avec chacune des deux stacks. Enfin, nous vous mettrons à contribution au travers de sondages pour dynamiser ensemble notre réflexion. Slides Vidéo ","date":"2022-07-02","objectID":"/talks/:1:4","series":null,"tags":null,"title":"Conférences","uri":"/talks/#java-dans-le-cloud-avec-spring-ou-quarkus"},{"categories":null,"content":"Au début de l’année, j’ai interpelé mes managers: “J’ai été retenu au Camping des Speakers”! Unanimement, j’ai eu la même réponse: “Le QUOI ???!!!” Oui j’en conviens le titre peut paraître à première vue tout sauf sérieux. Il faut dire que le lieu est un camping en Bretagne dans le golfe du Morbihan. Et pourtant ! La programmation était de qualité et a tenu ses promesses. Je vais essayer de retranscrire quelques sujets qui ont retenu mon attention. J’en oublierai sans doute beaucoup. Je m’en excuse d’avance. Tout d’abord, l’intérêt de cette conférence réside, selon moi, au-delà des talks donnés. Le cadre atypique (un camping dans le Morbihan), des conférences slideless données en extérieur en mode “feu de camp” a eu le même effet, enfin il me semble, sur tous les participantes et participants qu’ils soient speakers ou non. Chose originale que j’ai pu constater lors de ces talks, on passait beaucoup de temps à échanger avec les speakers pendant leur présentation. C’était génial pour les spectateurs, peut-être un peu moins pour la gestion du timing :). Vous l’aurez compris. L’environnement mis en place par les organisateurs permettait de réels échanges… Et on avait le temps de le faire. Ensuite, la programmation donnait la part belle à des sujets non techniques. Par exemple, j’ai enfin pu voir la conférence de mon ancienne collègue Fanny Klauk : “Rendez l’agilité aux développeur(se)s !” qui, au travers d’une histoire dont est le héros, a mis en évidence les travers des (grosses) organisations. Les solutions sont “naturelles” dans la théorie, mais permettent de donner plus d’agilité et de sens aux développeurs. Un autre sujet “non technique” m’a très intéressé, c’était “Le voyage du héros de l’IT” d’ Olivier Beautier. Après nous avoir présenté le guide du scénariste de C. VOGLER, on a construit une histoire d’une développeuse. Au travers de cette histoire, on a pu voir entre autres l’importance du mentorat dans notre métier. Puis, j’ai vu la conférence d’A. PENA “Java à la vitesse de la lumière grâce au Graal !”. Il a présenté GraalVM, une comparaison avec HotSpot et les perspectives d’évolution de cette JVM. Nul doute que le mode natif sera de plus en plus utilisé dans les prochaines années. Ensuite, avec D. APARICIO, nous avons pu voir les travers d’une (mauvaise) modélisation de données au travers de son retour d’expérience “Au secours 😨! J’ai un homonyme !! “ Il a illustré l’importance de bien modéliser pour ne pas avoir que le nom et prénom comme identifiants dans un système par ex. ainsi que les impacts dans la vie courante. Vous l’aurez compris, il y avait beaucoup de conférences intéressantes qui donnaient lieu à des discussions tout aussi passionnantes. Last but not least, j’ai eu la chance de présenter les katas d’architecture, comment les mettre en œuvre et quels sont les bénéfices qu’on peut en tirer. Je tiens à remercier Julien TOPCU et Craft Records pour l’accompagnement dans la préparation. Vous pourrez trouver les slides sur ma page speackerdeck. Pour finir, je conseille cette expérience à toutes les personnes qui veulent apprendre, échanger dans un environnement convivial et zen. Si vous n’êtes toujours pas convaincu, sachez qu’il y a également des mashmallows. Avec cet argument massue, on devrait convaincre les plus sceptiques! ;) Camping des speakers spirit Merci encore aux organisatrices et organisateurs pour ces deux jours. À l’année prochaine !! ","date":"2022-06-10","objectID":"/2022/06/10/camping-speakers-2022/:0:0","series":null,"tags":["conference"],"title":"Mon Camping des Speakers","uri":"/2022/06/10/camping-speakers-2022/#"},{"categories":null,"content":"Quand on parle du Cloud et de Kubernetes, généralement on pense aux APIs. Mais qu’en est-il des batchs? Oui, depuis plusieurs années, on pensait les éradiquer, mais ils sont encore là et on en a encore besoin pour quelques années encore. Ils ont même eu une deuxième jeunesse avec le Big Data et l’explosion des volumétries dans l’IT. Je vais essayer de faire un tour d’horizon dans cet article des batchs dans un environnement Cloud et plus particulièrement dans Kubernetes. Les exemples présentés dans cet article seront (sans doute) approfondis dans un second article et d’ores et déjà disponibles dans mon GitHub. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:0:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#"},{"categories":null,"content":" Pourquoi des batchs dans le Cloud?A ce titre un peu provocateur, j’ajouterais aussi “Pourquoi des batchs dans Kubernetes ?”. Oui, aujourd’hui encore,comme j’ai pu l’indiquer précédemment, on doit créer des traitements batchs. A coté des APIs qui représentent le cas d’utilisation “standard” du Cloud, on peut également avoir à traiter des fichiers volumineux allant de plusieurs centaines de Mo à quelques Go. Parmi les cas d’utilisation qui nécessitent ce genre de traitement, on pourra avoir: Les reprises de données (suite à des erreurs ou lors d’une initialisation) Traitement suite à une réception de fichiers (par ex. traitement de fichiers OPENDATA) Si vous êtes déjà passé sur le Cloud pour vos applications transactionnelles, vous vous poserez cette question: Puis-je également déployer des batchs? ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-des-batchs-dans-le-cloud"},{"categories":null,"content":" Pourquoi se poser cette question?Les réponses sont multiples. Elles sont tout d’abord liées à une rationalisation des environnements. Vous avez votre application dans le cloud, votre base de données y est également gérée pour éviter la latence réseau. Vous devez donc déployer des traitements tiers au plus proche de celle-ci pour vous soustraire des mêmes soucis. De plus, l’écosystème lié au cloud offre des technologies et pratiques qui rendent la vie plus simple (si, si, je vous assure) aux développeurs et ops. Le déploiement via l’Infra As Code est un bon exemple : Avoir toute l’infrastructure liée aux traitements batchs et transactionnels versionnées et instantiables à la demande est quelque chose dont on a du mal à se passer! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-se-poser-cette-question"},{"categories":null,"content":" Difficulté(s) par rapport aux APIsQuand on déploie une API dans le cloud, généralement tout va bien. On peut voir rapidement que cet environnement convient bien à ce genre de traitements. Pour les batchs, c’est une autre affaire! Selon les sociétés, il peut y avoir un fort historique et beaucoup plus d’exigences que pour les APIs. Ces dernières pourront être liées aux performances, à la qualité de service ou plus simplement à l’utilisation. Il faut donc, à l’instar de toute architecture, déterminer quel sera l’environnement technique de ce type de traitement. Cette fois, on aura à concilier performances, fichiers volumineux et reprises sur erreur. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#difficultés-par-rapport-aux-apis"},{"categories":null,"content":" Quelques technologiesOn pourra retrouver dans notre future architecture les briques suivantes: Une passerelle de fichiers (File Gateway) pour permettre l’envoi des fichiers de manière sécurisée Un stockage objet pour la distribution de fichiers ou l’archivage. Les éléments nécessaires à l’API : bases de données, HSMs, Cluster Kubernetes,… ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#quelques-technologies"},{"categories":null,"content":" Modes de déclenchementSi on regarde de plus près les exigences techniques liées aux cas d’utilisation, on pourrait résumer les différents modes de déclenchement de la manière suivante: Traitement sur réception de fichiers Traitement déclenché par un ordonnanceur/orchestrateur centralisé (ex. https://dkron.io/) de manière régulière ou non. Traitement déclenché par CRON (qui est un ordonnanceur, mais un peu plus roots) J’ai volontairement exclu les traitements sur présence de messages (ex. Kafka). Je les considère plus liés au monde transactionnel. Dans les paragraphes suivants, je vais décrire des solutions d’architecture qui permettent de déployer ces traitements dans Kubernetes. J’aborderai sans doute un exemple dans un autre article ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:3:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#modes-de-déclenchement"},{"categories":null,"content":" ContraintesDès qu’on s’aventure dans ce type de conception, nous aurons, au-delà des 12 factors, les contraintes suivantes à traiter: ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#contraintes"},{"categories":null,"content":" Gestion des erreurs et indisponibilitésDans un cluster Kubernetes, le crash d’un POD n’est pas rédhibitoire. Le cluster permet de redémarrer immédiatement une autre instance. Pour les APIs, ce n’est pas un problème. Pour les batchs, c’est une autre paire de manches. Quid du crash en plein milieu du traitement d’un fichier? Il faut donc penser à ce cas (et à d’autres) et archiver les fichiers pour un éventuel rejeu. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-erreurs-et-indisponibilités"},{"categories":null,"content":" Données et idempotence des traitementsIdéalement, les fichiers doivent avoir des lignes indépendantes qui peuvent être insérées individuellement et dans n’importe quel ordre. Aussi, chaque modification et traitement de données doivent être idempotentes. Pourquoi? Pas seulement par ce que c’est sympa et l’état de l’art, mais dans ce nouvel environnement, vous ne pourrez pas forcément garantir l’ordre des traitements. L’une des solutions potentielles de traitement est de découpler la lecture et l’insertion par du queueing (Artemis, Kafka - oui ce n’est pas du queuing, mais vous avez compris…). Dans ce cas, si votre traitement n’est pas idempotent, vous devrez lutter avec des doublons en base. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#données-et-idempotence-des-traitements"},{"categories":null,"content":" Gestion des ressourcesImaginez, vous recevez un fichier de 1Go. Vos ressources systèmes sont des PODs avec un 1 Go de RAM. Vous voyez le soucis? Cet exemple, qui n’est pas trop éloigné de la réalité, mets en évidence l’une des contraintes techniques que vous devrez prendre dès le début de votre conception. L’une des solutions serait, par exemple, le traitement quasi systématique du streaming de fichiers et l’obligation d’avoir des fichiers avec des lignes de données indépendantes (c.-à-d. sans avoir à faire de liens inter lignes pendant le traitement). ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:3","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-ressources"},{"categories":null,"content":" Traitement sur réception de fichiersDans ce cas, nous avons un processus qui est déclenché lors de la réception d’un fichier. Nous pourrons par exemple avec ce genre d’architecture un fichier qui est envoyé dans espace de stockage objet. Ce dernier est ensuite traité par un programme. J’ai fait le choix ici de mettre en oeuvre un couplage lâche (on ne se refait pas) entre l’espace de réception de fichiers et le traitement. Je traite ici le risque de crash d’un POD en gardant systématiquement les fichiers dans un stockage objet. De cette manière, si le traitement a échoué, un autre POD pourra le télécharger et rejouer le processus batch. Ce découplage permet de gérer facilement la scalabilité et les arrêts/relances de PODs. Batch démarré sur présence de fichier Dans ce cas, le batch pourra être déployé sous la forme d’un déploiement Kubernetes. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:5:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-sur-réception-de-fichiers"},{"categories":null,"content":" Traitement déclenché à distance (par ex. par un orchestrateur de traitements)Maintenant, on va aborder les traitements qui sont lancés par un ordonnanceur tiers ou tout simplement lancé à distance. Généralement, dans le monde de l’entreprise, la planification des traitements est centralisée au lieu de laisser de le faire sur chaque machine avec des CRON Jobs. Dans ce cas, on a deux manières de procéder: Avoir un traitement qui fournit une API permettant de démarrer des traitements et d’avoir leurs statuts. Lancer des jobs. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-déclenché-à-distance-par-ex-par-un-orchestrateur-de-traitements"},{"categories":null,"content":" Avec une APIIci, on conçoit les batchs comme des WEBAPPS qui fournissent des traitements batchs sur demande via des APIs. La contrainte est qu’à l’instar de la solution précédente, le programme tourne toujours et n’est vraiment utile que lorsqu’il est appelé via un endpoint REST. Ce modèle de conception peut être utilisé à mon avis si la fréquence est forte et si l’intégration d’un Job Kubernetes est problématique pour vous (voir ci-dessous). L’un des avantages que l’on pourra trouver est que le mode de déploiement est assez simple et similaire aux APIs. Batch démarré par une API ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-une-api"},{"categories":null,"content":" Avec des jobsSi votre ordonnanceur peut exécuter le client kubectl, vous pourrez considérer les jobs kubernetes. En résumé, ils permettent de créer un POD et exécute une action en gérant les erreurs potentielles jusqu’à complétion du traitement. Par exemple, voici un job permettant de faire un “Hello World!”: apiVersion: batch/v1 kind: Job metadata: name: hello-world spec: template: spec: containers: - name: helloworld image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: Never backoffLimit: 4 Une fois déployé avec Helm, vous pouvez les voir avec la commande kubectl get jobs minikube kubectl -- get jobs NAME COMPLETIONS DURATION AGE hello-world 0/1 25s 25s Pour les logs et voir le résultat de la commande lancé, cela se passe d’une manière assez habituelle: minikube kubectl -- logs hello-world-zx4wh Hello World! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-des-jobs"},{"categories":null,"content":" Traitement déclenché par CRONMaintenant, on va laisser le soin au Cluster Kubernetes de lancer les différents traitements via une CRON. Bien que je ne suis pas trop fan de ne pas centraliser l’ordonnancement, cela peut être très utile si votre plateforme est centrée sur Kubernetes. Si vous êtes dans ce cas-là, vous pouvez utiliser l’objet CronJob qui n’est ni plus ni moins qu’un Job exécuté de manière périodique. apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \"* * * * *\" jobTemplate: spec: template: spec: containers: - name: helloworld-cron image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: OnFailure ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:7:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-déclenché-par-cron"},{"categories":null,"content":" Panorama des solutions logicielles possiblesUne fois qu’on s’est posé toutes (en tout cas certaines) les questions possibles sur nos exigences techniques et la conception, on peut voir quelles sont les technologies possibles pour implémenter des batchs “cloud natifs”. Ça ne sera pas une surprise, je vais m’attarder à la plateforme Java. Il est bien évidemment possible d’utiliser d’autres langages et frameworks tels que Go. En Java, vous avez le choix entre différents frameworks : [Spring avec spring batch]([Spring avec spring batch et/ou integration ) et/ou integration Camel qui peut être utilisé avec Spring ou Quarkus Quarkus avec la JSR 352 Si vous allez du côté du BigData, vous pouvez aussi envisager d’utiliser des technologies telles qu’Apache Spark. Ces dernières vous permettront de découper “plus facilement” vos traitements. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:8:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#panorama-des-solutions-logicielles-possibles"},{"categories":null,"content":" Le diable se cache dans les détailsDéployer un batch dans Kubernetes peut se faire assez facilement (en développement) une fois qu’on a compris quelques principes. Cependant, les soucis peuvent survenir une fois arrivé en production. La gestion des erreurs est beaucoup plus complexe que les APIs. Il vous faudra donc définir avec les différentes parties prenantes quel est le meilleur fonctionnement (rejeu) en production. Il vous faudra ainsi bien identifier et évaluer les risques liés à votre application et voir quelles sont les actions à mener. Aussi, si vous devez manipuler des fichiers volumineux, il faudra faire attention au système de fichiers utilisé et ses performances. Habituellement, avec ce type d’architecture, on utilise généralement du SAN. En fonction de vos exigences, un stockage block pourra être plus adapté. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:9:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#le-diable-se-cache-dans-les-détails"},{"categories":null,"content":" ConclusionPour conclure cet article, vous aurez compris que le sujet des batchs dans Kubernetes peut s’avérer assez complexe à gérer. Au-delà des technologies qui peuvent faire le job (désolé du mauvais jeu de mots), il vous faudra faire très attention à tout l’environnement dans lequel votre programme devra interagir. Les bases, le réseau, les performances de votre matériel seront des prérequis indispensables. Aussi, il vous faudra faire attention à la manière dont sont transmises les données et dont vous les traitez. Bref, il faut étudier la solution dans son ensemble du développement à l’exploitation pour s’assurer de ne rien oublier. Enfin, cet article n’est bien évidemment pas exhaustif que cela soit sur les solutions ou les contraintes à adresser. J’ai néanmoins essayé d’apporter quelques cas concrets et retours d’expérience. J’essaierai de détailler un cas concret dans un prochain article. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:10:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#conclusion"},{"categories":null,"content":" L’analyse des risques: kezako ?Souvent utilisée dans la prise de décision, l’analyse des risques a plusieurs objectifs : Permettre de pondérer des risques potentiels Faciliter la prise de décision sur les actions à réaliser pour les prévenir ou tout du moins les atténuer. Mais d’abord, revenons aux bases. Comment identifier un risque ? Selon Wikipedia, voici la définition: Le risque est la possibilité de survenue d’un événement indésirable, la probabilité d’occurrence d’un péril probable ou d’un aléa. Bien évidemment, on a les risques inconnus et ceux qui sont connus. Le préalable à toute gestion de risque (tout du moins pour la définition d’architecture) est de capitaliser les connaissances et retours d’expérience qui viennent du terrain. On va donc oublier les risques inconnus dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#lanalyse-des-risques-kezako-"},{"categories":null,"content":" Comment les définir ?Tout d’abord, il faut connaître les SLOs de la plateforme qu’on souhaite concevoir. Pourquoi ? Pour vérifier si les risques qu’on identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne électrique sera faiblement impactante pour une application avec une disponibilité \u003c 70%. La réalisation des SLIs et SLOs est un préalable pour définir le “budget d’erreur”. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les atténuer. Ensuite, pour chaque risque qu’on identifiera (souvent à partir de notre expérience), on tâchera de définir les caractéristiques suivantes: Cause Probabilité Conséquence (gravité) Un exempleLa base de données est indisponible Cause: Système de fichier plein Probabilité: faible Conséquence: forte (toute la plateforme est HS) Pour déterminer la cause, il y a plusieurs méthodes, l’une des plus célèbres est celle des cinq pourquoi. Elle permet d’accéder à la cause du problème. Pour établir la probabilité, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour déterminer quels ont été les différents incidents. Pour chacun, vous devrez définir ces trois caractéristiques : cause, probabilité, conséquence. A coté de ça, vous aurez à identifier si possible le temps d’indisponibilité du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#comment-les-définir-"},{"categories":null,"content":" Comment les définir ?Tout d’abord, il faut connaître les SLOs de la plateforme qu’on souhaite concevoir. Pourquoi ? Pour vérifier si les risques qu’on identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne électrique sera faiblement impactante pour une application avec une disponibilité \u003c 70%. La réalisation des SLIs et SLOs est un préalable pour définir le “budget d’erreur”. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les atténuer. Ensuite, pour chaque risque qu’on identifiera (souvent à partir de notre expérience), on tâchera de définir les caractéristiques suivantes: Cause Probabilité Conséquence (gravité) Un exempleLa base de données est indisponible Cause: Système de fichier plein Probabilité: faible Conséquence: forte (toute la plateforme est HS) Pour déterminer la cause, il y a plusieurs méthodes, l’une des plus célèbres est celle des cinq pourquoi. Elle permet d’accéder à la cause du problème. Pour établir la probabilité, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour déterminer quels ont été les différents incidents. Pour chacun, vous devrez définir ces trois caractéristiques : cause, probabilité, conséquence. A coté de ça, vous aurez à identifier si possible le temps d’indisponibilité du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple"},{"categories":null,"content":" SynthèseUne fois ce travail de fourmi réalisé, vous pourrez le synthétiser dans un premier temps avec ce formalisme souvent repris dans la gestion de projet: En résumé, les actions qui sont oranges ou rouges doivent être traitées et avoir un plan d’action. Prenons la définition d’une plateforme: Si votre API doit traiter de manière régulière des PAYLOADs volumineux (bon déjà, la ce n’est pas top). Le temps de traitement peut être très long et bloquer certaines ressources (ex. des sous transactions). Dans ce cas, la probabilité sera à probable et l’impact sera modéré ou majeur. Par conséquent, vous devrez le prendre en compte avec par exemple un circuit breaker. Pour aller encore plus loin, vous pouvez également évaluer les risques en fonction de votre budget d’erreur: Est-ce que l’erreur peut rentrer dans mon budget ou pas? Bref, est-ce acceptable? Si vous allez plus loin, je vous conseille la formation Coursera Site Reliability Engineering: Measuring and Managing Reliability. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#synthèse"},{"categories":null,"content":" OK, j’ai identifié les risques potentiels. Qu’est-ce que j’en fait maintenant?C’est là que démarre réellement le travail d’architecture: vous devrez évaluer chaque risque en fonction des exigences fonctionnelles et technique pour savoir si elles valent la peine d’être prises en considération. Si vous avez des risques de faible impact, vous pourrez soit les “mettre sous contrôle” pour traiter d’autres problèmes, soit les traiter car ils sont “faciles” à traiter et vous permettront d’agrandir votre “budget d’erreur”. Dans ce dernier cas, vous aurez la possibilité de laisser “de coté” d’autres erreurs plus compliquées à traiter car elles rentreront dans votre budget. Bref, c’est un vrai travail de fourmi qui se base sur l’expérience du terrain. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:2:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#ok-jai-identifié-les-risques-potentiels-quest-ce-que-jen-fait-maintenant"},{"categories":null,"content":" Quid de l’architecture?Si vous avez évalué les risques correctement, vous pourrez ne traiter que les risques qui en valent la peine et par conséquent n’ ajouter de la complexité que là ou ça en vaut la peine! Oui, ce travail préalable permet de faire simple! ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:3:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#_quid_-de-larchitecture"},{"categories":null,"content":" Un exemple concretSi je reprends l’application bookstore que j’ai décrite dans un précédent article: ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple-concret"},{"categories":null,"content":" Exigences techniques SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#exigences-techniques"},{"categories":null,"content":" Exigences techniques SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#slos--slis"},{"categories":null,"content":" Exigences techniques SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#volumétrie"},{"categories":null,"content":" Risques identifiésSans aller dans le détail, voici quelques risques que l’on peut identifier de prime abord dans cette architecture: Indisponibilité du service bookstore Indisponibilité du service booknumber Indisponibilité de la base de données à cause d’une forte volumétrie En vous basant sur l’expérience de vos OPS, vous pourrez également ajouter des risques liés à l’infrastructure (routeurs, DNS, …). Je ne vais pas les aborder dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#risques-identifiés"},{"categories":null,"content":" Qualification des risquesVoici une rapide qualification: Risques Probabilité Impact Indisponibilité du service bookstore Probable Majeur Indisponibilité du service booknumber Probable Majeur Indisponibilité de la base de données Possible Catastrophique Si on se réfère au premier diagramme, il est obligatoire de les prendre en compte. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:3","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#qualification-des-risques"},{"categories":null,"content":" Solutions d’architecture pour leur prise en compteUne fois les risques identifiés, on peut tout d’abord les confronter à notre budget d’erreur pour valider leur prise en compte dans notre conception. Dans notre cas, on va prendre le postulat qu’il faut réellement les prendre en considération et trouver une solution adaptée. Voici des exemples de solutions qui permettraient de faire descendre leur impact our leur probabilité. Risques Probabilité Impact Action/Solutions possibles Indisponibilité du service bookstore Possible Majeur Load balancing avec deux instances, Utilisation Kubernetes,. Indisponibilité du service booknumber Possible Majeur Sur le service book-number: Load balancing avec deux instances, Utilisation Kubernetes,.. Sur le service bookstore: Mettre en place un circuit breaker basé sur le timeout d’appel vers le service book-number pour garantir la SLO Indisponibilité de la base de données Possible Catastrophique - Réalisation d’un benchmark pour s’assurer qu’une instance est suffisante. - Sinon mise en place mécanisme HA ou changement de technologie ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:4","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#solutions-darchitecture-pour-leur-prise-en-compte"},{"categories":null,"content":" ConclusionL’analyse des risques n’est pas récente et n’a pas été inventée par le monde de l’informatique. Elle est d’abord apparue dans la gestion de projets et fait désormais partie prenante de la définition d’architectures (enfin ça commence…). Il ne faut pas la voir seulement pour un outil de “GO-NO GO” de réunion de cellule de crise mais comme une aide à la décision pour la conception des systèmes. Il a toute sa place à coté des différentes caractéristiques que vous devrez prendre en compte ( sécurité, modularité, …). J’ai essayé de décrire comment les identifier et trouver une solution adaptée dans l’exemple. Bien évidemment, il n’est pas complet. Je pense néanmoins qu’il permet d’avoir une idée sur le sujet. Le principal avantage d’utiliser à la fois les SLOs/SLIs, le budget d’erreur et l’analyse des risques est de n’apporter de la complexité que là où c’est nécessaire. Pour certains un benchmark sera souvent utile pour confirmer votre décision. Si vous voulez aller plus loin, je vous conseille dans un premier temps de lire “Fundamentals of Software Architecture”. Ce sujet y est abordé. Enfin,si ce sujet vous intéresse, vous pouvez vous projeter au délà de l’informatique en lisant les analyses de risques réalisées par le Ministère des finances. Bonne lecture ;-) ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:5:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#conclusion"},{"categories":null,"content":" Pourquoi mettre en oeuvre des GITHUB ACTIONS ?Comme j’ai pu l’expliquer dans mon précédent article, je suis passé de Wordpress à GITHUB Pages. Une fois le site déployé une première fois, on voit qu’on a perdu pas mal d’automatisations qui sont réalisées par défaut dans Wordpress. Par exemple, vous devez construire votre site, publier des nouveaux articles et vérifier que tout est OK. J’ai donc mis en oeuvre des GITHUB ACTIONS pour automatiser le plus d’actions possibles et me passer de tâches manuelles souvent rébarbatives. Si vous souhaitez découvrir les GITHUB ACTIONS, je vous conseille ce site. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:1:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pourquoi-mettre-en-oeuvre-des-github-actions-"},{"categories":null,"content":" Construction du site et déploiementDès qu’on touche à Jekyll et à l’hébergement sur Github Pages, on tombe sur certaines actions à réaliser telles que celle-ci: bundle exec jekyll build J’ai donc réalisé les workflows suivants: ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#construction-du-site-et-déploiement"},{"categories":null,"content":" Pour une feature branch (dans une Pull Request)Je construis le site sans le déployer pour vérifier que la construction est correcte. name: Build Jekyll site on: push: branches-ignore: (1) - main - gh-pages jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: 📂 setup (2) uses: actions/checkout@v2 # include the lines below if you are using jekyll-last-modified-at # or if you would otherwise need to fetch the full commit history # however this may be very slow for large repositories! # with: # fetch-depth: '0' - name: 💎 setup ruby (3) uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: 🔨 install dependencies \u0026 build site (4) run: bundle exec jekyll build Voila ce que ce workflow réalise: Il est exécuté à chaque push excepté sur les branches main et gh-pages. Ce sont les branches que j’utilise pour le déploiement du site après la validation d’une pull request. Checkout du projet Initialisation de Ruby et téléchargement des dépendances comme le ferait la commande bundle install. Construction du site ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:1","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-une-feature-branch-dans-une-pull-request"},{"categories":null,"content":" Pour la branche mainUne fois que la pull request est validée, le code est poussé dans la branche main. On y exécute le code suivant: name: Build and deploy Jekyll site to GitHub Pages on: push: branches: - main jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: 📂 setup uses: actions/checkout@v2 - name: 💎 setup ruby uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: 🔨 install dependencies \u0026 build site run: bundle exec jekyll build - name: 🚀 deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./_site Ce dernier reprend le code du workflow suivant ( oui, j’aurai pu faire des workflows réutilisables… ) et ajoute l’étape de déploiement. Le code généré sera copié dans la branche gh-pages. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:2","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-la-branche-main"},{"categories":null,"content":" Publication d’un articlePour rédiger un article, j’utilise le mécanisme de feature branch et pull request. Pour automatiser la publication, le nommage des articles avec la date etc, j’ai mis en oeuvre le workflow suivant: Je rédige les articles (comme celui-ci) et les positionne dans le répertoire _drafts: ls -R _drafts quelques-github-actions-utiles-pour-un-site-jekyll-heberge-sur-github-io.md J’associe un milestone à la pull request. Dès que ces derniers sont terminés, le workflow décrit ci-dessous est exécuté. Il permet, via un script python de: Identifier les articles dans le répertoire _drafts Vérifier que la date de publication spéficié dans l’en-tête est antérieure à la date courante (now()) Copier le fichier dans le répertoire _posts en le renommant avec la date en préfixe. name: Publish Drafts on: (1) milestone: types: [closed] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: ref: main (2) - name: 📂 setup python uses: actions/setup-python@v2 with: python-version: '3.7.7' # install the python version needed - name: 💎 install python packages (3) run: | python -m pip install --upgrade pip - name: 🔨 execute py script (4) run: python publish_drafts.py - name: 🔨 commit files (5) run: | if git ls-files -o --exclude-standard; then git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add -A git commit -m \"publish drafts\" -a git push origin main else echo \"No file to publish\" fi Explication: Déclenchement manuel ou à la clôture d’un milestone Récupération de la branche main Installation de packages python Exécution du script python réalisé pour l’occasion Commit et push Une fois ce workflow réalisé, le workflow vu précédemment est automatiquement lancé et le site est généré une nouvelle fois. Bon ça fait deux constructions, mais au vu du temps pris, c’est négligeable. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:3:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#publication-dun-article"},{"categories":null,"content":" UptimeJ’aurai pu utiliser un tiers service tel que uptime robot. Pour mon besoin, j’ai préféré opter pour un appel régulier du site et une vérification du code HTTP (200). # This is a basic workflow to help you get started with Actions name: Uptime Monitoring on: schedule: (1) - cron: '*/60 * * * *' jobs: ping_site: runs-on: ubuntu-latest name: Ping the site steps: - name: Check the site id: hello uses: srt32/uptime@master with: url-to-hit: \"https://blog.touret.info/robots.txt\" (2) expected-statuses: \"200,301\" Explications Déclenchement toutes les heures de ce workflow J’ai utilisé une GITHUB ACTION existante qui ping une URL et vérifie le code retour. Dans mon cas, j’ai utilisé l’URL du fichier robots.txt et je vérifie le code retour. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:4:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#uptime"},{"categories":null,"content":" ConclusionJ’ai réussi à plus ou moins automatiser tout le cycle de construction d’articles. C’est encore perfectible et loin de certaines fonctionnalités de Wordpress, mais je n’en ai pas réellement besoin. Si vous souhaitez réutiliser ces workflows et les intégrer dans sites, vous pouvez les récupérer sur ce repo GITHUB. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:5:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#conclusion"},{"categories":null,"content":"L’idée me trottait dans la tête depuis quelques mois environ: migrer mon blog de Wordpress vers un site basé sur Jekyll et hébergé directement sur Github. La date de renouvellement de ma souscription Wordpress arrivant à terme, je me suis décidé à franchir le pas. s ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:0:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#"},{"categories":null,"content":" L’ hébergement de sites web sur GithubGithub permet via son service Github Pages d’héberger des sites statiques (c.-à-d. pas de base de données derrière) en permettant d’associer son nom de domaine. Le certificat est automatiquement généré. Pour avoir un look un peu sympa, j’ai donc mis en oeuvre les outils suivants: Jekyll Minimal Mistakes Github Actions pour construire le site Markdown pour écrire les différents articles ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:1:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#l-hébergement-de-sites-web-sur-github"},{"categories":null,"content":" Démarrer avec Minimal MistakesJe vous conseille d’aller sur ce site. Tout est bien détaillé est c’est réalisable en quelques minutes seulement. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:2:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#démarrer-avec-minimal-mistakes"},{"categories":null,"content":" Migration des donnéesSans surprise, c’est la partie la moins drôle. Il faut en résumé: Exporter les données si vous êtes hébergé sur wordpress.com Les ré-importer dans une instance locale Les exporter au format Jekyll Copier le contenu généré dans un nouveau site ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#migration-des-données"},{"categories":null,"content":" Exporter les donnéesAfin d’exporter les données et de les transformer, il faut d’abord exporter les données (articles + médias) de votre site Wordpress Vous obtiendrez deux archives: la première pour les articles, la deuxième pour les médias. Création d’une instance Wordpress pour convertir les données au format Jekyll ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#exporter-les-données"},{"categories":null,"content":" Importer les donnéesPour faire simple, j’utilise Docker pour monter une architecture Wordpress sur mon poste. Il faut pour ça créer un fichier docker-compose.yml et insérer le contenu suivant: version: \"3.9\" services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - wordpress_data:/var/www/html ports: - \"8000:80\" restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: db_data: {} wordpress_data: {} Ensuite, vous aurez à lancer la commande suivante: docker-compose up Une fois lancé, vous aurez à une instance Wordpress via cette URL : http://localhost:8000 Ensuite, il faut installer l’extension jekyll-exporter. La procédure peut prendre un peu de temps. Une fois effectuée, vous aurez une archive ZIP contenant un site Jekyll avec les images et articles associés. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#importer-les-données"},{"categories":null,"content":" Création du siteEn attendant que ça se termine, j’ai crée un site jekyll avec le starter du thème minimal mistakes. J’ai ensuite copié les articles (répertoire /_posts) et images (/assets/img). Au premier lancement des commandes suivantes: bundle install bundle exec jekyll serve J’ai eu quelques erreurs. J’ai donc eu à nettoyer les fichiers via des recherche/remplace dans un éditeur Par exemple, j’ai supprimé les références author et layout author: admin layout: post J’ai également ajouté pour certains articles une image pour le teaser Exemple: featuredImagePreview: /assets/images/2021/07/rest-book-architecture.png ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#création-du-site"},{"categories":null,"content":" URL des pages et compatibilité Wordpress \u003c\u003e JekyllPour me faciliter la vie dans les URLS et liens en tout genre, j’ai gardé le format des URLS de Wordpress. Pour que ça soit le format par défaut de Jekyll, il faut modifier le paramètre permalink dans le fichier _config.yml permalink: /:year/:month/:day/:title/ ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#url-des-pages-et-compatibilité-wordpress--jekyll"},{"categories":null,"content":" Flux RSS pour un tag donnéJ’utilisais une petite spécificité de Wordpress: la création d’un flux RSS pour un tag donné. Pour le mettre en place dans Jekyll, il faut configurer le plugin jekyll-feed avec les propriété suivantes: feed: tags: true ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#flux-rss-pour-un-tag-donné"},{"categories":null,"content":" Et maintenant ?J’ai sans doute oublié quelques renommages/suppressions réalisés ici et là. Néanmoins, le principal est évoqué dans cet article. Il ne vous reste plus qu’à éplucher la documentation du thème et de jekyll pour finaliser l’ installation de votre nouveau site. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:5:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#et-maintenant-"},{"categories":null,"content":"Il y a quelques mois déjà, je discutais avec un collègue d’ observabilité, opentracing, … avec Quarkus. On est tombé sur un super exemple réalisé par Antonio Concalves. Ce projet démontre les capacités de Quarkus sur les sujets suivants: Circuit Breaker Observabilité OpenTracing Tests … Et la on peut se demander quid de Spring? Je me doutais que ces fonctionnalités étaient soient disponibles par défaut soient facilement intégrables vu la richesse de l’écosystème. J’ai donc réalisé un clone de ce projet basé sur Spring Boot/Cloud. Je ne vais pas détailler plus que ça les différentes fonctionnalités, vous pouvez vous référer au fichier README. Il est suffisamment détaillé pour que vous puissiez exécuter et les mettre en œuvre. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:0:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#"},{"categories":null,"content":" Architecture de l’applicationVous trouverez ci-dessous un schéma d’architecture de l’application au format C4. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:1:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#architecture-de-lapplication"},{"categories":null,"content":" Circuit BreakerLors des appels entre le bookstore et le booknumberservice, il peut être intéressant d’ implémenter un circuit breaker pour pallier aux indisponibilités de ce dernier. Avec Spring, on peut utiliser Resilience4J au travers de Spring Cloud. Tout ceci se fait de manière programmatique Il faut tout d’abord configurer les circuit breakers au travers d’une classe Configuration. @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createDefaultCustomizer() { return factory -\u003e factory.configureDefault(id -\u003e new Resilience4JConfigBuilder(id) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()) .circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .build()); } /** * Creates a circuit breaker customizer applying a timeout specified by the \u003ccode\u003ebooknumbers.api.timeout_sec\u003c/code\u003e property. * This customizer could be reached using this id: \u003ccode\u003eslowNumbers\u003c/code\u003e * @return the circuit breaker customizer to apply when calling to numbers api */ @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createSlowNumbersAPICallCustomizer() { return factory -\u003e factory.configure(builder -\u003e builder.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()), \"slowNumbers\"); } Grâce à ces instanciations, on référence les différents circuit breakers. Maintenant, on peut les utiliser dans le code de la manière suivante: public Book registerBook(@Valid Book book) { circuitBreakerFactory.create(\"slowNumbers\").run( () -\u003e persistBook(book), throwable -\u003e fallbackPersistBook(book) ); return bookRepository.save(book); } Maintenant, il ne reste plus qu’à créer une méthode de « fallback » utilisée si un service est indisponible. Cette dernière nous permettra, par exemple, de mettre le payload dans un fichier pour futur traitement batch. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:2:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#circuit-breaker"},{"categories":null,"content":" ObservabilitéL’observabilité est sans contexte la pierre angulaire (oui, rien que ça…) de toute application cloud native. Sans ça, pas de scalabilité, de redémarrage automatique,etc. Les architectures de ce type d’applications sont idempotentes. On a donc besoin d’avoir toutes les informations à notre disposition. Heureusement, Spring fournit par le biais d’ Actuator toutes les informations nécessaires. Ces dernières pourront soit être utilisées par Kubernetes (ex. le livenessProbe) ou agrégées dans une base de données Prometheus. Pour activer certaines métriques d’actuator, il suffit de : Ajouter la/les dépendance(s) dependencies { [...] implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'io.micrometer:micrometer-registry-prometheus' [...] } Spécifier la configuration adéquate: management: endpoints: enabled-by-default: true web: exposure: include: '*' jmx: exposure: include: '*' endpoint: health: show-details: always enabled: true probes: enabled: true shutdown: enabled: true prometheus: enabled: true metrics: enabled: true health: livenessstate: enabled: true readinessstate: enabled: true datasource: enabled: true metrics: web: client: request: autotime: enabled: true ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:3:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#observabilité"},{"categories":null,"content":" OpenTracingSur les applications distribuées, il peut s’avérer compliqué de concentrer les logs et de les corréler. Certes, avec un ID de corrélation, on peut avoir certaines informations. Cependant, il faut que les logs soient bien positionnées dans le code. On peut également passer à travers de certaines informations (ex. connexion aux bases de données, temps d’exécution des APIS,…). Je ne vous parle pas des soucis de volumétrie engendrées par des index Elasticsearch/Splunk sur des applications à forte volumétrie. Depuis quelques temps, le CNCF propose un projet (encore en incubation) : OpenTracing. Ce dernier fait désormais partie d’OpenTelemetry. Grâce à cet librairie, nous allons pouvoir tracer toutes les transactions de notre application microservices et pouvoir réaliser une corrélation « out of the box » grâce à l’intégration avec Jaeger. Pour activer la fonctionnalité il suffit d’ajouter la dépendance au classpath: implementation 'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter:3.3.1' et de configurer l’URL de Jaeger dans l’application # Default values opentracing: jaeger: udp-sender: host: localhost port: 6831 enabled: true Une fois l’application reconstruite et redémarrée, vous pourrez visualiser les transactions dans JAEGER: ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:4:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#opentracing"},{"categories":null,"content":" ConclusionJe ne vais pas exposer l’implémentation des tests unitaires et d’intégration. Si vous voulez voir comment j’ai réussi à mocker simplement les appels REST à une API distante, vous pouvez regarder cette classe pour voir une utilisation du MockServer. Aussi, n’hésitez pas à cloner, tester ce projet et me donner votre retour. J’essaierai de le mettre à jour au fur et à mesure de mes découvertes (par ex. OpenTelemetry). ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:5:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#conclusion"},{"categories":null,"content":"Quand vous avez une API, et a fortiori une application, il peut être parfois nécessaire de passer l’application en mode « maintenance ». Pour certaines applications il est parfois inutile de le traiter au niveau applicatif, car ça peut être pris géré par certaines couches de sécurité ou frontaux web par ex. (Apache HTTPD, WAF) Kubernetes a introduit ( ou popularisé ) les notions de « probes » et plus particulièrement les livenessProbes et readinessProbes. Le premier nous indique si l’application est en état de fonctionnement, le second nous permet de savoir si cette dernière est apte à recevoir des requêtes (ex. lors d’un démarrage). Je vais exposer dans cet article comment utiliser au mieux ces probes et les APIs SPRING pour intégrer dans une API un mode « maintenance » ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:0:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#"},{"categories":null,"content":" Stack utiliséeDans l’exemple que j’ai développé, j’ai pu utiliser les briques suivantes: OpenJDK 11.0.10 Spring Boot 2.5.0 (web, actuator) Maven 3.8.1 Bref, rien de neuf à l’horizon 🙂 ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:1:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#stack-utilisée"},{"categories":null,"content":" Configuration de Spring ActuatorPour activer les différents probes, vous devez activer Actuator. Dans le fichier pom.xml, vous devez ajouter le starter correspondant: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e Puis vous devez déclarer ces differentes propriétés: management.endpoints.enabled-by-default=true management.health.livenessstate.enabled=true management.health.readinessstate.enabled=true management.endpoint.health.show-details=always management.endpoint.health.probes.enabled=true management.endpoint.health.enabled=true Après avoir redémarré votre application, vous pourrez connaître son statut grâce à un appel HTTP curl -s http://localhost:8080/actuator/health/readiness ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:2:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#configuration-de-spring-actuator"},{"categories":null,"content":" Comment récupérer le statut des probes?Avec Spring, vous pouvez modifier les différents statuts avec les classes ApplicationEventPublisher et ApplicationAvailability. Par exemple, pour connaître le statut Readiness vous pouvez exécuter le code suivant: @ApiResponses(value = { @ApiResponse(responseCode = \"200\", description = \"Checks if the application in under maitenance\")}) @GetMapping public ResponseEntity\u003cMaintenanceDTO\u003e retreiveInMaintenance() { var lastChangeEvent = availability.getLastChangeEvent(ReadinessState.class); return ResponseEntity.ok(new MaintenanceDTO(lastChangeEvent.getState().equals(ReadinessState.REFUSING_TRAFFIC), new Date(lastChangeEvent.getTimestamp()))); } Et la modification ? Grâce à la même API, on peut également modifier ce statut dans via du code: @ApiResponses(value = { @ApiResponse(responseCode = \"204\", description = \"Put the app under maitenance\")}) @PutMapping public ResponseEntity\u003cVoid\u003e initInMaintenance(@NotNull @RequestBody String inMaintenance) { AvailabilityChangeEvent.publish(eventPublisher, this, Boolean.valueOf(inMaintenance) ? ReadinessState.REFUSING_TRAFFIC : ReadinessState.ACCEPTING_TRAFFIC); return ResponseEntity.noContent().build(); } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:3:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#comment-récupérer-le-statut-des-probes"},{"categories":null,"content":" Filtre les appels et indiquer que l’application est en maintenanceMaintenant qu’on a codé les mécanismes de récupération du statut de l’application et de la mise en maintenance, on peut ajouter le mécanisme permettant de traiter ou non les appels entrants. Pour ça on va utiliser un bon vieux filtre servlet. Ce dernier aura la tâche de laisser passer les requêtes entrantes si l’application n’est pas en maintenance et de déclencher une MaintenanceException le cas échéant qui sera traité par la gestion d’erreur globale de l’application ( traité via un @RestControllerAdvice). Pour que l’exception soit bien traitée par ce mécanisme, il faut le déclencher via le HandlerExceptionResolver. @Component public class CheckMaintenanceFilter implements Filter { private final static Logger LOGGER = LoggerFactory.getLogger(CheckMaintenanceFilter.class); @Autowired private ApplicationAvailability availability; @Autowired @Qualifier(\"handlerExceptionResolver\") private HandlerExceptionResolver exceptionHandler; /** * Checks if the application is under maintenance. If it is and if the requested URI is not '/api/maintenance', it throws a \u003ccode\u003eMaintenanceException\u003c/code\u003e * * @param request * @param response * @param chain * @throws IOException * @throws ServletException * @throws info.touret.spring.maintenancemode.exception.MaintenanceException the application is under maintenance */ @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { if (availability.getReadinessState().equals(ReadinessState.REFUSING_TRAFFIC) \u0026\u0026 !((HttpServletRequest) request).getRequestURI().equals(API_MAINTENANCE_URI)) { LOGGER.warn(\"Message handled during maintenance [{}]\", ((HttpServletRequest) request).getRequestURI()); exceptionHandler.resolveException((HttpServletRequest) request, (HttpServletResponse) response, null, new MaintenanceException(\"Service currently in maintenance\")); } else { chain.doFilter(request, response); } } } Enfin, voici la gestion des erreurs de l’API: @RestControllerAdvice public class GlobalExceptionHandler { /** * Indicates that the application is on maintenance */ @ResponseStatus(HttpStatus.I_AM_A_TEAPOT) @ExceptionHandler(MaintenanceException.class) public APIError maintenance() { return new APIError(HttpStatus.I_AM_A_TEAPOT.value(),\"Service currently in maintenance\"); } /** * Any other exception */ @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) @ExceptionHandler({RuntimeException.class, Exception.class}) public APIError anyException() { return new APIError(HttpStatus.INTERNAL_SERVER_ERROR.value(),\"An unexpected server error occured\"); } } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:4:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#filtre-les-appels-et-indiquer-que-lapplication-est-en-maintenance"},{"categories":null,"content":" ConclusionOn a pu voir comment intéragir simplement avec les APIS SPRING pour gérer le statut de l’application pour répondre à cette question :Est-elle disponible ou non? Bien évidemment, selon le contexte, il conviendra d’ajouter un peu de sécurité pour que cette API ne soit pas disponible à tout le monde 🙂 Le code exposé ici est disponible sur Github. Le Readme est suffisamment détaillé pour que vous puissiez tester et réutiliser le code. ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:5:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#conclusion"},{"categories":null,"content":" Pourquoi utiliser GPG ? Par exemple pour signer les commits GIT. Maintenant comment faire quand on est sous Windows 10 et qu’on souhaite utiliser le sous système Linux (WSL2)? Sous GNU/Linux, l’installation et l’utilisation avec git est très simple. Avec WSL2,… il faut un peu d’huile de coude 🙂 Je vais tâcher de décrire dans cet article les différentes manipulations nécessaires pour: Importer une clé GPG existante Utiliser GPG pour signer mes commits dans WSL2 ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:0:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#"},{"categories":null,"content":" Importer une clé GPG existante","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#importer-une-clé-gpg-existante"},{"categories":null,"content":" Export de la clé GPG Identifier l’ ID de la cléLancez la commande suivante: gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#export-de-la-clé-gpg"},{"categories":null,"content":" Export de la clé GPG Identifier l’ ID de la cléLancez la commande suivante: gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#identifier-l-id-de-la-clé"},{"categories":null,"content":" Import gpg --import public.key gpg --import private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:2","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#import"},{"categories":null,"content":" VérificationPour vérifier que la clé est bien configurée, vous pouvez lancer la commande suivante: gpg --list-secret-keys --keyid-format LONG alexandre@.... sec rsa4096/CLE_ID 2019-12-20 [SC] ******************** uid [ ultime ] Alexandre \u003calexandre@....\u003e ssb rsa4096/SUB 2019-12-20 [E] Si la clé n’est pas reconnue comme ultime ou comme de confiance, il faudra l’éditer: gpg --edit-key CLE_ID Please decide how far you trust this user to correctly verify other users' keys (by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menu Your decision? Si vous ne voulez pas trop vous compliquer, je vous conseille de répondre 5. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:3","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#vérification"},{"categories":null,"content":" Configuration GPG pour WSL2Avant de configurer l’agent GPG, vous pouvez vous référer à cet article pour configurer GIT et GPG. La configuration est équivalente. Ensuite, créez le fichier ~/.gnupg/gpg.conf avec le contenu suivant: # Uncomment within config (or add this line) # This tells gpg to use the gpg-agent use-agent # Set the default key default-key CLE_ID Puis créez le fichier ~/.gnupg/gpg-agent.conf avec le contenu ci-dessous: default-cache-ttl 34560000 max-cache-ttl 34560000 pinentry-program /usr/bin/pinentry-curses Le cache ici est défini en secondes. Il est mis ici à 400 jours. Ce dernier fichier fait référence au programme pinentry. Vous pouvez vérifier sa présence grâce à la commande: ls /usr/bin/pinentry-curses Si vous ne l’avez pas, vous pouvez l’installer grâce à la commande suivante: sudo apt install pinentry-curses Maintenant, on peut configurer l’environnement BASH en modifiant le fichier ~/.bashrc # enable GPG signing export GPG_TTY=$(tty) if [ ! -f ~/.gnupg/S.gpg-agent ]; then eval $( gpg-agent --daemon --options ~/.gnupg/gpg-agent.conf ) fi export GPG_AGENT_INFO=${HOME}/.gnupg/S.gpg-agent:0:1 Redémarrez ensuite WSL2 pour que ça soit pris en compte. A la première utilisation de GPG ( par ex. lors d’un commit, vous aurez une interface Ncurses qui apparaîtra dans votre prompt WSL2. Vous aurez à renseigner le mot de passe de votre clé. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:2:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#configuration-gpg-pour-wsl2"},{"categories":null,"content":"Les confinements se suivent et se ressemblent. Me voilà à installer Ubuntu sur un nouvel ordinateur. A l’instar de l’ancien laptop que j’ai acheté pour mon aînée, j’ai acheté un DELL pour ma deuxième fille.J’ai opté pour un DELL Inspiron 5301. A l’instar de mon autre laptop, je j’ai pas pris de risques. J’ai opté pour un DELL qui est pleinement compatible avec Ubuntu. Oui j’aurai pu installer un ordinateur avec Ubuntu pré-installé, mais je n’ai pas eu le temps de faire un choix « serein ». ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:0:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#"},{"categories":null,"content":" Configuration du BIOSVoila les paramètres que j’ai appliqué: Dans le menu \"Storage\" puis \"SATA Operation\": vous devez sélectionner AHCI au lieu de RAID. Dans le menu \"Change boot mode settings \u003eUEFI Boot Mode\" , vous devez désactiver le Secure Boot. Une fois réalisé, vous pouvez redémarrer en appuyant sur la touche F12. Si vous n’arrivez pas à revenir sur le BIOS pour indiquer de booter sur votre clé USB, vous obtiendrez un écran d’erreur Windows dû à la configuration AHCI. Personnellement, en redémarrant une ou deux fois, j’ai obtenu un écran de démarrage avancé qui m’a permis de sélectionner le périphérique (ma clé USB) sur lequel démarrer. Maintenant vous pouvez accéder à l’installeur Ubuntu et profiter. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:1:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#configuration-du-bios"},{"categories":null,"content":" InstallationJ’ai eu plusieurs fois des popup « erreur rencontré ». Ce n’était pas bloquant. J’ai continué. Tout s’est déroulé sans encombre. Le matériel est très bien reconnu. Les seuls logiciels que j’ai installé sont pour l’instant : VLC, Minecraft ( obligatoire dans la famille ) et Chromium. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:2:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#installation"},{"categories":null,"content":"Dès qu’on veut déployer des environnements Kubernetes, helm devient une des solutions à considérer. Le déploiement des objets standards tels que deployment, autoscaler et autres se fait aisément car ces derniers ne changent pas d’un environnement à l’autre. Généralement on déploie la même infrastructure sur tous les environnements du développement à la production. Bien évidemment on pourra limiter la taille des replicas sur l’environnement de développement par exemple mais au fond, le contenu des charts sera identique. Une des difficultés que l’on pourra rencontrer c’est dans la gestion des fichiers de configuration. Je vais essayer d’exposer dans cet article comment j’ai réussi à gérer +/- efficacement (en tout cas pour moi) les fichiers de configuration dans les charts HELM. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:0:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#"},{"categories":null,"content":" Les config maps et secretsLogiquement dans ce type d’architecture, les configmaps et secrets permettent le chargement des variables d’environnement et autres mots de passe. Cependant si vous utilisez certains frameworks qui nécessitent des fichiers de configuration, vous devrez charger les fichiers dans des volumes. Pour ces derniers, les volumes n’ont pas besoin d’être persistents. Par exemple dans la configuration de votre deployment, vous pourrez configurer le montage d’un volume de la manière suivante: volumeMounts: - mountPath: /config name: configuration-volume readOnly: true - mountPath: /secrets name: secret-volume readOnly: true [...] volumes: - configMap: defaultMode: 420 name: configuration name: configuration-volume - name: secret-volume secret: defaultMode: 420 secretName: secrets Pour intégrer un fichier binaire, on pourra le faire de la manière suivante dans le template HELM: apiVersion: v1 # Definition of secrets kind: Secret [...] type: Opaque # Inclusion of binary configuration files data: my_keystore_jks {% raw %} .Files.Get \"secrets/my_keystore.jks\" | b64enc }} {% endraw %} Vous pouvez définir les fichiers directement dans vos configmaps. Cependant, si vos fichiers sont volumineux, vous aurez du mal à les maintenir. Personnellement, j’opte pour mettre les fichiers de configuration à coté et les charger dans le configmap. On pourra procéder de la manière suivante: apiVersion: v1 kind: ConfigMap [...] data: my_conf: {% raw %}{{- (.Files.Glob \"conf/*\").AsConfig | nindent 2 }} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:1:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#les-config-maps-et-secrets"},{"categories":null,"content":" Livrables agnostiquesUne bonne pratique de développement logiciel est d’externaliser la configuration de vos environnements (ex. l’URL JDBC de la base de données) des livrables. Les charts HELM n’échappent à la règle. On peut stocker la configuration de chaque environnement dans le chart, mais dans ce cas, on perdra beaucoup de souplesse lors des mises à jour des propriétés et cela nous imposera une nouvelle version. On a plusieurs niveaux d’externalisation. Le premier est dans le chart. Vous pouvez externaliser les différentes valeurs dans le fichier values.yml. Ci dessous un exemple avec un autoscaler: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: labels: [...] spec: maxReplicas: {% raw %} {{.Values.myapp.maxReplicaCount }}{% endraw %} minReplicas: {% raw %} .Values.myapp.minReplicaCount }}{% endraw %} scaleTargetRef: apiVersion: apps/v1 kind: Deployment [...] targetCPUUtilizationPercentage: {% raw %} .Values.myapp.replicationThreesold }}{% endraw %} Les valeurs sont décrites comme suit: myapp: minReplicaCount: \"2\" maxReplicaCount: \"6\" replicationThreesold: 80 Pour externaliser les valeurs d’environnement, vous pourrez donc externaliser un autre fichier values.yml qui sera appliqué au déploiement. Les valeurs de ce dernier surchargeront les valeurs définies dans le chart. Il est important de noter également que les données présentes dans les fichiers de configuration (ex. fichier application.properties) peuvent être « variabilisées » et surchargées par le même mécanisme. Vous aurez à utiliser la commande tpl. apiVersion: v1 kind: ConfigMap metadata: name: configuration labels: [...] data: application.properties: |- {% raw %}{{ tpl (.Files.Get \"conf/application.properties\") . | nindent 4}} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:2:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#livrables-agnostiques"},{"categories":null,"content":" ConclusionVous l’aurez compris, les charts HELM n’échappent pas aux règles déjà connues de gestion des environnements et des livrables. Même si il y a quelques subtilités à connaître pour intégrer des fichiers de configuration par exemple, les grands principes restent les mêmes. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:3:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#conclusion"},{"categories":null,"content":"Depuis quelques années, Kubernetes (K8S) et son écosystème deviennent l’environnement d’ exécution à la mode. Certaines personnes veulent déployer sur cet environnement en mettant en avant ses capacités de scalabilité. D’autres font du bashing (souvent) justifié sur la complexité et le coût de mise en œuvre d’une telle plateforme. Vous l’aurez compris, cette technologie n’échappe pas au cycle du hype et à la fameuse courbe du Gartner. Après quelques expériences sur cette plateforme ( et beaucoup sur d’autres 😀 ) je vais essayer de peser le pour et le contre qui m’apparaissent importants. Bien évidemment, ce n’est que mon avis, j’ai sans doute omis certaines informations qui pourraient être indispensables pour d’ autres. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:0:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#"},{"categories":null,"content":" Pourquoi et dans quelles conditions il ne faut pas utiliser K8S ?Avant de présenter les avantages des applications cloud, je vais essayer de réaliser l’anti thèse de mon propos. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-et-dans-quelles-conditions-il-ne-faut-pas-utiliser-k8s-"},{"categories":null,"content":" En avez vous (vraiment) besoin ?Vaste sujet et question délicate pour la population informaticienne qui a tendance à suivre les tendances du marché. Cycle Hype Avant de foncer tête baissée dans cette technologie qui est très intéressante au demeurant, il est important de se poser ces quelques questions: Est-ce que mes SLO sont contraignantes? Quel le cycle de déploiement de mes applications? Qui gère les environnements ? Bref, il faut savoir si le jeu en vaut la chandelle. Si vous avez une application qui doit scaler dynamiquement, encaisser les pics, et avoir du zero downtime durant les mises à jour, Kubernetes est fait pour vous. Si vous avez une application de gestion qui n’a pas d’exigences fortes si ce n’est de répondre aux besoins fonctionnels, l’utilisation de Kubernetes est discutable. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#en-avez-vous-vraiment-besoin-"},{"categories":null,"content":" Êtes vous taillé pour ?Kubernetes et son écosystème peuvent s’avérer complexes à appréhender. Si votre entreprise opte pour une utilisation « on premise« , c’est pire. Vous devrez avoir une équipe dédiée qui gérera cette plateforme et offrir une expertise aux équipes de développement. Ne vous trompez pas. Si votre rôle est de développer des applications métier, il vous sera très difficile d’avoir également une expertise sur l’administration de cette plateforme. Vous pourrez l’utiliser et être à l’aise, mais l’administration d’une telle technologie est très compliquée. Le seul conseil que je pourrais vous donner, c’est de ne partir sur Kubernetes que si vous avez une équipe support à disposition. C’est vrai si vous utilisez des services du Cloud tels que Google Cloud ou AWS. Ça l’est encore plus si vous utilisez des services « on premise » tels qu’ Openshift. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#êtes-vous-taillé-pour-"},{"categories":null,"content":" Est-ce que vos développements sont « cloud native » ?Au delà de la plateforme, vous devrez monter en compétence sur le développement et la conception de vos applications. Il vous faudra prendre en considération les 12 facteurs clés dans vos applications. Il n’est pas forcément la peine de passer sur des microservices. Il est également possible de faire des monolithes modulaires qui peuvent être légers et stateless. Beaucoup de ces facteurs sont communément admis comme des bonnes pratiques de développement logiciel (ex. Il faut une intégration continue). Aussi, cela va sans dire, il faut également monter (réellement) en compétence sur les conteneurs et leurs contraintes. Si vous n’avez pas l’habitude de travailler avec des conteneurs ( construction, déploiement, disponibilité d’une registry). Il est préférable de définir une trajectoire avec des étapes intermédiaires. Bref, tous ces sujets doivent être adressés et compris pour toutes les parties prenantes de vos équipes que ça soit les développeurs, les chefs de projet et les équipes métiers à une moindre mesure. Cette technologie représente réellement un grand pas à franchir. Si vous ne vous sentez pas de le faire, ou si vous devez gagner en maturité sur ces sujets, attendez avant de vous lancer sur Kubernetes. On ne pourra jamais vous reprocher de ne pas opter sur Kubernetes si vous ne remplissez pas tous les pré-requis. Pour ce qui est du contraire… ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#est-ce-que-vos-développements-sont-cloud-nativehttpswwwredhatcomfrtopicscloud-native-apps-"},{"categories":null,"content":" Avez vous des interactions avec des services tiers qui sont compatible avec Kubernetes ?Quand vous restez dans votre cluster Kubernetes, généralement, tout va bien. Dès que vous avez des interactions avec des services tiers, ça peut se compliquer. En effet, généralement vous devrez vous connecter à des services tiers qui ne sont pas orienté cloud : des boitiers crypto, des passerelles de transfert, … Il se peut que certains protocoles soient également incompatibles avec Kubernetes. Il vous faudra vous assurer que tout la galaxie de logiciels et systèmes gravitant autour de votre application sera compatible avec une telle architecture. Ceci n’est pas une mince affaire. L’aide d’une équipe support (voir ci-dessus) vous sera d’une grande utilité. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:4","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#avez-vous-des-interactions-avec-des-services-tiers-qui-sont-compatible-avec-kubernetes-"},{"categories":null,"content":" Pourquoi sauter le pas ?","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-sauter-le-pas-"},{"categories":null,"content":" La scalabilité et la résistance à la pannePersonnellement, la première fonctionnalité qui m’a intéressé c’est la gestion de la scalabilité. Si vous avez des objectifs de 99.9% de disponibilité. Kubernetes sera une plus value indéniable dans votre architecture. Après quelques jours heures à batailler avec les fichiers YAML, vous pourrez gérer automatiquement la scalabilité en fonction de plusieurs indicateurs qu’ils soient techniques (ce sont les plus faciles à gérer) ou un peu plus métier en utilisant Prometheus – et oui encore une technologie supplémentaire à connaître. En effet, au lieu de vous en soucier une fois arrivé en production, vous aurez lors du développement l’obligation de prendre en considération l’observabilité de votre application. Par exemple, vous aurez à renseigner si votre application est prête et/ou disponible pour traiter les requêtes. Ces indicateurs vous permettront de scaler automatiquement et de re-créer si nécessaire un POD en cas de panne. J’ai trouvé que cette pratique était vertueuse. Bien évidemment, pas besoin d’être sur Kubernetes pour avoir de l’observabilité dans des applications. Par contre, ici, c’est obligatoire et implémenté dès le développement. La scalabilité automatique est aussi très intéressante. On a souvent vu des serveurs en production qui n’étaient pas suffisamment utilisés. Ici vous n’aurez que les instances nécessaires pour votre cas d’utilisation. La contrainte que l’on peut voir à cette fonctionnalité et qu’on ne maitrise pas complètement le nombre d’instances disponibles. C’est Kubernetes qui s’en charge en prenant en compte le paramétrage que vous aurez renseigné dans vos templates HELM. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#la-scalabilité-et-la-résistance-à-la-panne"},{"categories":null,"content":" Le déploiementAvant de déployer (dans la vraie vie), vous aurez à mettre en place un pipeline CI/CD qui orchestre les différents déploiements sur tous vos environnements. Attention, ce n’est pas une mince affaire 🙂 ! Une fois réalisé, vous verrez automatiquement le gain. Vos déploiements seront réellement fluides. Bon OK, on peut le faire sur des VMS standards. Mais on peut améliorer la procédure de déploiement pour mettre en place du zero downtime pour ne pas interrompre le service lors d’un déploiement. strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#le-déploiement"},{"categories":null,"content":" L’Infrastructure As Code Quand on pense à Kubernetes, et au cloud, on ne pense pas trop à l’Infrastructure As Code au début. Cependant, cette pratique est pour moi l’une des plus utiles. En effet, avoir votre système décrit dans des fichiers, versionnés vous permet de le tester dès le développement. Ça évite ( dans la majorité des cas ) les erreurs lors des installations d’environnement. La mise à jour des logiciels est largement accélérée. Bien évidemment, il existe Terraform et Ansible pour le provisionning des environnements. Ici je trouve qu’on pousse le concept encore plus loin. L’automatisation est à mon avis poussé à paroxysme. Prenons par exemple la gestion des systèmes d’exploitation. La mise à jour sur des serveurs physiques ou virtuels peut prendre énormément de temps et générer des erreurs. Avec de l’infra as code, ceci est testé et validé automatiquement via des tests unitaires dès l’environnement de développement. On peut suivre la gestion des environnements via un gestionnaire de sources et la promotion vers les autres environnements (recette[1-n], pré-production, production) est grandement accélérée. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#linfrastructure-as-code"},{"categories":null,"content":" ConclusionBon, vous l’aurez peut être compris, cette galaxie de technologies est intéressante et peut vous aider dans vos projets. Avant d’arriver à l’utiliser sereinement, il vous faudra sans doute définir une trajectoire et appréhender plusieurs sujets avant d’arriver à déployer vos applications sur un cloud interne ou externe. J’espère que cet article vous aura permis de mettre en évidence les pour et contre d’une telle technologie et le cas échéant vous donnera envie de franchir le pas. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:3:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#conclusion"},{"categories":null,"content":"Derrière ce nom pompeux qui peut effrayer, je vais essayer d’expliquer dans cet article comment on peut versionner facilement ses partitions et les publier sur le web. En cherchant comment mettre de la documentation technique avec des diagrammes PlantUml dans des repos GITLAB et générés avec des pipelines, je me suis mis dans la tête de faire la même chose avec des partitions 🙂 Depuis plusieurs années, j’utilise lilypond pour créer mes partitions. C’est un peu difficile de s’y mettre, mais une fois la syntaxe assimilée, la saisie d’une partition est beaucoup plus efficace. Le rendu des partitions est vraiment optimisé. Si vous voulez plus de détails sur le pourquoi du comment je vous conseille cette page. Vous trouverez des exemples sur le site. J’ai donc eu l’idée de: Stocker ces partitions sur un repo github (jusque là rien d’exceptionnel) Générer automatiquement les partitions au format PDF, PNG et MIDI via une github action (ça commence à devenir intéressant…) Les publier avec les github pages (tant qu’à faire 🙂) ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:0:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#"},{"categories":null,"content":" StockagePourquoi stocker dans un référentiel de sources tel que Github ? Pour les non informaticiens : les partitions sont stockées au format texte. \\version \"2.12.1\" \\header { title=\"Try a little tenderness\" composer=\"Harry Woods, Jimmy Campbell \u0026 Reg Connely\" subtitle = \"Commitments Version\" %poet = \"Poete\" instrument = \"Piano\" editor = \"L'éditeur\" %meter=\\markup {\\bold {\"Remarque sur le rhythme\"}} style = \"Soul\" maintainer = \"Alexandre Touret\" maintainerEmail = \"alexandre.touret@free.fr\" maintainerWeb = \"http://blog.touret.info\" lastupdated = \"\" source = \"Music room\" footer = \"Footer\" copyright =\\markup {\\fontsize #-1.5 \"Delivered by A TOURET\"} } upper= \\relative c'{ \\clef treble \\time 4/4 \\tempo 4=176 \\key g \\major d'2 (b4 e d2 b4 a g2 g2 \u003ce g,\u003e2) \u003cfis, c' d\u003e \\bar \"||\" GIT et GITHUB permettent de versionner facilement et pouvoir faire facilement un retour arrière en cas d’erreur. Aussi, GITHUB offre des fonctionnalités « sociales » et collaboratives qui facilitent la revue des modifications ( en cas de travail à plusieurs ). Bref, ça offre la sécurité d’une sauvegarde et la possibilité d’un retour arrière en cas d’erreur. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:1:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#stockage"},{"categories":null,"content":" Générer les partitions avec une github actionLes github actions sont des outils permettant: GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. J’ai donc décidé de créer un workflow qui permet de générer les partitions au format lilypond. J’ai mis à disposition le code sur github sous licence GNU GPLv3. Elle est utilisable telle quelle. Pour créer l’action, il faut créer un fichier action.yml à la racine du repo. Voici le contenu name: 'Lilypond Generator' description: 'Run Lilypond tool with the given set of arguments to generate music sheets' author: '@alexandre-touret' inputs: args: description: 'Arguments for Lilyponid' required: true default: '-h' runs: using: 'docker' image: 'Dockerfile' args: - ${{ inputs.args }} branding: icon: 'underline' color: 'blue' Vous aurez compris que ce fichier fait référence à une image Docker. Cette dernière n’est ni plus ni moins qu’une Debian avec lilypond d’installé. Pour l’utiliser dans un repo github, on peut créer une action qui l’utilise. Voici un exemple: jobs: build_sheets: runs-on: ubuntu-latest env: LILYPOND_FILES: \"*.ly\" steps: - name: Checkout Source uses: actions/checkout@v1 - name: Get changed files id: getfile run: | echo \"::set-output name=files::$(find ${{github.workspace}} -name \"${{ env.LILYPOND_FILES }}\" -printf \"%P\\n\" | xargs)\" - name: LILYPOND files considered echo output run: | echo ${{ steps.getfile.outputs.files }} - name: Generate PDF music sheets uses: alexandre-touret/lilypond-github-action@master with: args: -V -f --pdf ${{ steps.getfile.outputs.files }} A la dernière ligne on peut passer les arguments nécessaires à lilypond. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:2:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#générer-les-partitions-avec-une-github-action"},{"categories":null,"content":" PublicationLa c’est l’étape la plus facile :). Il suffit d’activer les github pages et de commiter et pusher les partitions générées - name: Push Local Changes run: | git config --local user.email \"${{ secrets.GIT_USERNAME }}\" git config --local user.name \"${{ secrets.GIT_EMAIL }}\" git add . git commit -m \"Add changes\" -a - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} Il suffit de créer une page index.md à la racine et d’ajouter des liens vers les partitions générées ( dans mon cas, ça se passe dans le répertoire /docs ). Vous pouvez trouver un exemple ici. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:3:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#publication"},{"categories":null,"content":" ConclusionVoila comment on peut générer un site avec des partitions crées avec Lilypond. Vous trouverez les différents liens ci-dessous. Peut être que je publierai cette action sur le marketplace une fois que j’aurai publié une documentation digne de ce nom :). Action Exemple d’utilisation Exemple de site généré ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:4:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#conclusion"},{"categories":null,"content":"A mes heures perdues, je travaille sur un « POC/side project qui n’aboutira pas et je m’en fiche » basé sur Quarkus. J’ ai choisi d’utiliser les langages et composants suivants : Kotlin Quarkus Gradle Kubernetes pour le déploiement Oui, tant qu’à faire, autant aller dans la hype … Mon projet est sur GITHUB. Pour automatiser certaines actions et, disons-le, par fierté personnelle, j’ai choisi d’automatiser certaines actions par la mise en œuvre de pipelines CI/CD. Depuis peu, GITHUB a intégré un mécanisme de pipeline : GITHUB Actions. Ça permet, entre autres, de lancer des processus automatisé sur un push ou sur une action pour un commit GIT. La force de l’outil est, selon moi, de facilement s’intégrer avec beaucoup de services du cloud ( sonarcloud, google cloud, heroku,…). On aime ou on n’aime pas, mais chez Microsoft, l’intégration ils savent faire. Par exemple, si on veut lancer une compilation lors d’un push, on peut placer un fichier .github/workflows/build.xml avec le contenu : name: CI on: [push] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build with Gradle without testing run: ./gradlew build -x test Coté GITHUB, vous verrez l’exécution sur un écran dédié Vous pouvez créer autant de workflows que vous souhaitez (si votre projet est en libre accès). Pour chaque workflow, on peut définir et utiliser des jobs. Les logs d’exécution sont disponibles dans ce même écran: ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:0:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#"},{"categories":null,"content":" Worflows implémentésJ’ai choisi d’implémenter les workflows suivants: CI: Build sur la feature branch CD: Build sur master branch et déploiement On obtient donc dans mon cas: Ce n’est pas parfait. Loin de là. Dans la « vraie vie », pour une équipe de dev, je l’améliorerai sans doute par un build docker dans les features branches, une validation formelle et bloquante de l’analyse sonar, etc. Pour un dev perso ça suffit largement. Le contenu de la branche master est compilé et une image docker est crée pour être déployée automatiquement dans GKE. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:1:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#worflows-implémentés"},{"categories":null,"content":" Analyse SONARJ’ai choisi d’utiliser sonarcloud pour analyser mon code. C’est gratuit pour les projets opensource. L’analyse se fait simplement: sonarCloudTrigger: name: SonarCloud Trigger runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: SonarCloud Scan env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }} run: ./gradlew jacocoTestReport sonarqube Dans ce job j’utilise deux secrets. Ce sont des tokens qui permettent de ne pas stocker en dur les données dans les repos GITHUB. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:2:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#analyse-sonar"},{"categories":null,"content":" Création d’une image Docker et déploiement dans le registry GITHUBIci aussi, ça se fait simplement. La preuve : jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build in JVM Mode with Gradle without testing run: ./gradlew quarkusBuild [1] - name: Branch name run: echo running on branch ${GITHUB_REF##*/} - name: Build the Docker image Quarkus JVM run: docker build -f src/main/docker/Dockerfile.jvm -t docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest . [2] - name: Login against github docker repository env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: docker login -u ${GITHUB_ACTOR} -p ${GITHUB_TOKEN} docker.pkg.github.com [3] - name: Publish the Docker image Quarkus JVM run: docker push docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest [4] Création du binaire Création de l’image docker en utilisant la commande docker et le Dockerfile fourni par Quarkus Identification sur la registry Docker de GITHUB Déploiement de l’image Pour plus de détails sur la variable GITHUB_TOKEN, vous pouvez lire cet article de la documentation. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:3:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#création-dune-image-docker-et-déploiement-dans-le-registry-github"},{"categories":null,"content":" Déploiement dans Google Kubernetes EngineMon application est pour l’instant architecturée comme suit (attention c’est compliqué): Pour la déployer dans Google Kubernetes Engine, j’ai besoin d’ implémenter cette « architecture » par les objets Kubernetes suivants: J’utilise les objets suivants: Des services pour exposer la base de données ainsi que l’application Un deployment pour l’application Des pods car à un moment, il en faut… Un statefulset pour la base de données Vous pourrez trouver la définition de tous ces objets au format yaml via ce lien. J’ai fait très simple. Logiquement j’aurai du créer un volume pour les bases de données ou utiliser une base de données en mode PAAS. Pour lancer le déploiement, il faut au préalable créer un secret ( fait manuellement pour ne pas stocker d’objet yaml dans le repository GITHUB) pour se connecter au repo GITHUB via la commande suivante: kubectl create secret docker-registry github-registry --docker-server=docker.pkg.github.com --docker-username=USER--docker-password=PASSWORD --docker-email=EMAIL On peut faire pareil pour les connexions base de données. J’ai mis dans un configmap pour ne pas trop me prendre la tête… Après le déploiement via le pipeline se fait assez simplement: [...] - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master with: version: '286.0.0' service_account_email: ${{ secrets.GKE_SA_EMAIL }} service_account_key: ${{ secrets.GKE_SA_KEY }} project_id: ${{ secrets.GKE_PROJECT }} # Get the GKE credentials so we can deploy to the cluster - run: |- gcloud container clusters get-credentials \"${{ secrets.GKE_CLUSTER }}\" --zone \"${{ secrets.GKE_ZONE }}\" # Deploy the Docker image to the GKE cluster - name: Deploy run: |- kubectl apply -f ./k8s J’utilise les « actions » fournies par Google. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:4:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#déploiement-dans-google-kubernetes-engine"},{"categories":null,"content":" ConclusionPour que ça marche il y a pas mal d’étapes préalables ( des tokens à générer, un utilisateur technique, …). J’ai essayé de les référencer dans le README du projet. Si vous voulez tester l’intégration Kubernetes dans le cloud google, sachez que vous pouvez disposer d’un crédit de 300€ valable un an. Attention, avec ce genre d’architecture, ça part vite… ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:5:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#conclusion"},{"categories":null,"content":"Mon PC Lenovo a un SSD. Le temps de démarrage est actuellement de 11 sec. Ça commence à faire pas mal… J’ai eu donc envie de me pencher sur l’optimisation du démarrage ( encore une fois) . Voici comment gagner (facilement) quelques secondes au démarrage. Boot time Tout d’abord, vous devez analyser les services qui prennent du temps au démarrage. Vous pouvez le faire avec cette commande: systemd-analyze plot \u003e plot.svg J’ai obtenu le graphique suivant: Boot initial ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:0:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#"},{"categories":null,"content":" Configuration GRUBLa première manipulation à réaliser est de désactiver le timeout de GRUB. Pour celà, vous pouvez modifier la variable GRUB_TIMEOUT dans le fichier /etc/default/grub: GRUB_TIMEOUT=0 Ensuite, vous devez mettre à jour la configuration GRUB en exécutant cette commande: sudo update-grub2 Au prochain reboot, vous ne verrez plus le menu GRUB. ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:1:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-grub"},{"categories":null,"content":" Configuration NetworkManagerDans mon cas, le service NetworkManager-wait-online.service prenait près de 9 secondes. Après avoir lu plusieurs billets et rapports de bug, je me suis aperçu que je pouvais le désactiver au boot. Vous pouvez le faire en lançant la commande suivante sudo systemctl disable NetworkManager-wait-online.service ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:2:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-networkmanager"},{"categories":null,"content":" Configuration AptUn autre service qui prenait pas mal de temps était apt-daily.timer qui vérifiait au boot qu’il y avait des mises à jour de l’OS. Après quelques recherches, j’ ai vu qu’on pouvait soit le désactiver ( ce qui n’est pas recommandé pour les mises à jour de sécurité ) soit décaler la recherche. J’ai choisi cette solution. Vous devez donc exécuter la commande suivante: sudo systemctl edit apt-daily.timer Et renseigner le contenu suivant: [Timer] OnBootSec=15min OnUnitActiveSec=1d AccuracySec=1h RandomizedDelaySec=30min Ce service sera donc lancé 15 minutes après le boot. Ce qui est largement suffisant. [EDIT] Vous pouvez appliquer la même configuration pour le service apt-daily-upgrade en exécutant la commande: sudo systemctl edit apt-daily-upgrade.timer Ensuite, vous pouvez recharger la configuration en exécutant cette commande: sudo systemctl daemon-reload ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:3:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-apt"},{"categories":null,"content":" RésultatsAprès ces quelques manipulations qui peuvent prendre 5 minutes grand maximum, j’ai réussi à optimiser le boot en réduisant le démarrage à 5 secondes! Boot (après) Vous pourrez trouver le détail ci-dessous: Détail du Boot (après) ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:4:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#résultats"},{"categories":null,"content":"Avec les contraintes liées au confinement, les répétitions se font de plus en plus rares. Pour ne pas perdre la main, il y a quelques logiciels qui permettent de jouer d’un instrument et d’ improviser tout en ayant une bande son en fond musical. Il y a plusieurs logiciels payants/propriétaires sur différentes plateformes: Band in a box irealpro Garage band Jjazzlabs J’ai découvert ce dernier récemment en naviguant sur le site Linux Mao. Il a l’avantage d’être gratuit (le moteur est sous licence LGPL 3.0, pas le logiciel en tant que tel), de fonctionner sous GNU/LINUX, d’ offrir un son pas mal du tout et de permettre la configuration de la dynamique au fur et à mesure du morceau. Je vais expliquer comment l’installer sur Debian. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:0:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#"},{"categories":null,"content":" Configuration Midi","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration-midi"},{"categories":null,"content":" Activation des périphériques virtuels MIDICréer un fichier /etc/modules.load.d/midi.conf avec le contenu suivant: snd-virmidi Ensuite créer le fichier /etc/modprobe.d/midi.conf avec le contenu suivant: options snd-virmidi midi_devs=1 Logiquement à ce stade, lors du prochain reboot, vous aurez un périphérique virtuel MIDI activé. En attendant vous pouvez lancer la commande suivante $ sudo modprobe snd-virmidi midi_devs=1 ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:1","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#activation-des-périphériques-virtuels-midi"},{"categories":null,"content":" Synthétiser du MIDIPour faire fonctionner ce logiciel, il faut installer une banque de son ( au format SF2) et un logiciel permettant de l’utiliser pour synthétiser du MIDI. La banque de son recommandée est disponible via ce lien. Téléchargez là et copiez la dans un répertoire accessible. Pour le second, il vous faudra installer fluidsynth. Voic les quelques commandes à lancer: $ sudo apt install fluid-synth qsynth ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:2","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#synthétiser-du-midi"},{"categories":null,"content":" Petite vérification…Avant d’ aller plus loin dans la configuration de fluidsynth, vous pouvez vous assurer que tout est OK en récupérant un fichier MIDI et en lançant la commande suivante: $ fluidsynth -a pulseaudio -m alsa_seq -l -i /opt/JJazzLab-2.0-Linux/JJazzLab-SoundFont.sf2 MIDI_sample.mid Normalement vous devriez avoir du son. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:3","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#petite-vérification8230"},{"categories":null,"content":" Configurer fluidsynthLancez qsynth et cliquez sur le bouton « configuration » Vous trouverez ci-dessous la configuration que j’ai appliqué. Elle diffère légèrement de celle présentée dans la documentation. Pensez à redémarrer fluidsynth après application de ces nouveaux paramètres. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:4","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-fluidsynth"},{"categories":null,"content":" Configurer aconnectDisclaimer: La c’est la partie la plus obscure… Il faut maintenant « brancher » la sortie du synthétiseur virtuel MIDI à fluidsynth pour que le son MIDI soit interprété par ce dernier à travers sa banque de son. Ce n’est pas intuitif, je vous avais prévenu … Je ne vous parle pas de la pseudo interface graphique à aconnect. La ligne de console est plus parlante ( c’est pour dire ) . Exécutez la commande suivante: $ aconnect -lo client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Dans mon cas, je vais avoir à connecter le client 24:0 au synthétiseur 128:0 grâce à la commande : $ aconnect 24:0 128:0 Maintenant, si on relance la commande aconnect -lo on obtient le résultat suivant: client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' Connexion À: 128:0 client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Connecté Depuis: 24:0 Attention, cette commande devra être lancée ( ainsi que fluidsynth) avant chaque démarrage de jjazzlab. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:5","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-aconnect"},{"categories":null,"content":" Installation de JjazzlabTéléchargez les binaires sur ce site, puis décompressez l’archive dans le répertoire /opt par ex. Vous devez également installer java $ sudo apt install openjdk-11-jdk Ensuite, vous devez créer le fichier ~/.local/share/applications/jjazzlab.desktop avec le contenu suivant: [Desktop Entry] Type=Application Name=JJazzLab GenericName=JJazzLab Icon= Exec=\"/opt/JJazzLab-2.0-Linux/bin/jjazzlab\" Terminal=false Categories=Audio;Music;Player;AudioVideo; Maintenant vous pouvez directement démarrer JJazzlab via le menu. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:2:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#installation-de-jjazzlab"},{"categories":null,"content":" ConfigurationUne fois jjazzlab démarré, vous devez aller dans le menu « Tools\u003eOptions » et sélectionnez les valeurs suivantes: Ouvrez un fichier example (ex. sunny ) Cliquez sur le menu décrit par un clavier Puis configurez comme suit: Maintenant vous pouvez télécharger les standards fournis sur le site et improviser dessus 🙂 ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:3:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration"},{"categories":null,"content":"Voici un rapide article sur un problème rencontré récemment. Lors de l’exécution d’un container docker, j’ai eu une erreur SIGSEGV 139. Un crash avec aucune log. Bref que du bonheur 🙂 Avant d’aller plus loin voici mon environnement: Debian 10 Docker CE 19.03.8 Après quelques recherches, je me suis rendu compte qu’on pouvait reproduire ce comportement en exécutant cette commande: docker run -it gcc:4.8.5 Une des raisons trouvées serait un problème de compatibilité avec le noyau 4.8.5 (oui ça remonte…). Une solution est d’activer l’émulation vsyscall. Voici la configuration à effectuer: Dans le fichier /etc/default/grub, ajouter la ligne suivante: GRUB_CMDLINE_LINUX_DEFAULT=\"quiet vsyscall=emulate\" Puis lancer les commandes suivantes: $ sudo update-grub $ sudo reboot Maintenant le container devrait pouvoir s’exécuter correctement. ","date":"2020-04-01","objectID":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Erreur 139 à l'exécution d'un container docker","uri":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/#"},{"categories":null,"content":"Suite aux premières annonces de distanciation sociale ( avant que le confinement soit effectif ) j’ai acheté en catastrophe un PC portable. Les critères étaient : 8Go de RAM, un disque SSD … et la compatibilité GNU/LINUX :). N’ayant pas trop de temps pour chercher la bonne affaire ( technologique et financière ), j’ ai acheté un Dell Inspiron 14-3493. Je n’ai pas pris trop de risques. Bien que livré avec Windows 10, ce modèle est déjà certifié compatible Ubuntu. L’installation d’Ubuntu se passe très bien. C’est plié en moins de 30mn. Du coup, je ne la détaillerai pas dans cet article – si vous êtes intéressé, vous pouvez consulter cet article. Pour les pré-requis, c’est une autre paire de manches … Voilà les différentes actions que j’ai réalisé au préalable ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:0:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#"},{"categories":null,"content":" Redémarrer l’ordinateur et accéder au BIOSLà, j’ai un peu galéré pour accéder au BIOS. La seule manipulation que j’ai trouvé et de lancer le menu « Démarrage avancé » puis sélectionner « Utiliser un périphérique ». Vous pouvez donc sélectionner le disque dur. Au boot en appuyant sur la touche F12 et/ou F2, vous pouvez accéder au BIOS. ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:1:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#redémarrer-lordinateur-et-accéder-au-bios"},{"categories":null,"content":" Configuration du BIOSVoila les paramètres que j’ai appliqué: Dans le menu “SATA Operation”: vous devez sélectionner AHCI au lieu de RAID. Dans le menu “Change boot mode settings \u003eUEFI Boot Mode” , vous devez désactiver le Secure Boot. Une fois réalisé, vous pouvez redémarrer en appuyant sur la touche F2 et/ou F12. Si vous n’arrivez pas à revenir sur le BIOS pour indiquer de booter sur votre clé USB, vous obtiendrez un écran d’erreur Windows dû à la configuration AHCI. Personnellement, en redémarrant une ou deux fois, j’ai obtenu un écran de démarrage avancé qui m’a permis de sélectionner le périphérique (ma clé USB) sur lequel démarrer. Maintenant vous pouvez accéder à l’installeur Ubuntu et profiter 🙂 ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:2:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#configuration-du-bios"},{"categories":null,"content":" Après l’installationJe n’ai rien fait de particulier si ce n’est configurer le trackpad. Pour cela, j’ai installé gnome-tweaks. Mis à part ça, tout fonctionne très bien! ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:3:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#après-linstallation"},{"categories":null,"content":"Java 8 est encore largement utilisé dans les entreprises aujourd’hui. Il y a même certains frameworks qui n’ont pas encore sauté le pas. Je vais essayer d’exposer dans cette article les étapes à réaliser pour migrer (simplement) votre application JAVA8 en JAVA 11. Dans cet article, je prendrai comme postulat que l’application se construit avec Maven. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:0:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#"},{"categories":null,"content":" Pré-requisTout d’abord vérifiez votre environnement d’exécution cible! Faites un tour du coté de la documentation et regardez le support de JAVA. Si vous utilisez des FRAMEWORKS qui utilisent des FAT JARS, faites de même (ex. pour spring boot, utilisez au moins la version 2.1.X). Ensuite, vous aurez sans doute à mettre à jour maven ou gradle. Préférez les dernières versions. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:1:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#pré-requis"},{"categories":null,"content":" Configuration mavenLes trois plugins à mettre à jour obligatoirement sont : maven-compiler-plugin maven-surefire-plugin maven-failsafe-plugin ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#configuration-maven"},{"categories":null,"content":" Maven compiler plugin \u003cplugin\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cversion\u003e3.8.1\u003c/version\u003e \u003cconfiguration\u003e \u003crelease\u003e11\u003c/release\u003e \u003cencoding\u003eUTF-8\u003c/encoding\u003e \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:1","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-compiler-plugin"},{"categories":null,"content":" maven surefire / failsafe pluginPour ces deux plugins, ajouter la configuration suivante: \u003cplugin\u003e \u003cartifactId\u003emaven-surefire-plugin\u003c/artifactId\u003e \u003cversion\u003e2.22.2\u003c/version\u003e \u003cconfiguration\u003e [...] \u003cargLine\u003e--illegal-access=permit\u003c/argLine\u003e [...] \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:3:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-surefire--failsafe-plugin"},{"categories":null,"content":" Mise à jour des librairiesBon,la il n’y a pas de magie. Vous devez mettre à jour toutes vos librairies. Mis à part si vous utilisez des librairies exotiques, la plupart supportent JAVA 11 maintenant. C’est une bonne opportunité de faire le ménage dans vos fichiers pom.xml 🙂 ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:4:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#mise-à-jour-des-librairies"},{"categories":null,"content":" APIS supprimées du JDKSi vous faites du XML, SOAP ou que vous utilisiez l’API activation, vous devez désormais embarquer ces librairies. Le JDK ne les inclut plus par défaut. Par exemple: \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-core\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-impl\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ejavax.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-api\u003c/artifactId\u003e \u003cversion\u003e2.3.1\u003c/version\u003e \u003c/dependency\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:5:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#apis-supprimées-du-jdk"},{"categories":null,"content":" Modularisation avec JIGSAWBon là … je vous déconseille de partir directement sur la modularisation, surtout si vous migrez une application existante. Bien que la modularité puisse aider à réduire vos images docker en construisant vos propres JRE et d’améliorer la sécurité, elle apporte son lot de complexité. Bref pour la majorité des applications, je vous déconseille de l’intégrer. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:6:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#modularisation-avec-jigsaw"},{"categories":null,"content":" ConclusionAvec toutes ces manipulations, vous devriez pouvoir porter vos applications sur JAVA11. Il y aura sans doute quelques bugs. Personnellement, j’en ai eu avec CGLIB vs Spring AOP sur une classe instrumentée avec un constructeur privé. Sur ce coup j’ai contourné ce problème ( je vous laisse deviner comment 🙂 ). ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:7:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#conclusion"},{"categories":null,"content":"Depuis quelques temps je me mets à Gradle. Après de (trop?) nombreuses années à utiliser Maven (depuis la version 0.9…), je me risque à modifier mon environnement de build. Du moins sur des projets démo. Quand on a fait pas mal de Maven, on est un peu dérouté au début. On a d’un coté, la plupart des actions qui sont configurées de manière implicite et de l’autre on peut tout coder/étendre ou presque. Je ne vais pas me risquer à faire un comparatif des deux outils. Gradle ( donc fortement orienté ) en a fait un. Je vais plutôt décrire avec cet article comment on peut démarrer rapidement en configurant son environnement pour être utilisé en entreprise. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:0:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#"},{"categories":null,"content":" InstallationLe plus simple est d’utiliser SDKMAN. Voici la manipulation pour l’installer: $ curl -s \"https://get.sdkman.io\" | bash $ source \"$HOME/.sdkman/bin/sdkman-init.sh\" $ sdk install gradle 6.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:1:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#installation"},{"categories":null,"content":" Configuration d’un proxyEt oui comment souvent, passer le proxy d’entreprise est la moitié du boulot :). Pour le configurer de manière globale (c.-à-d. pour tous vos projets) sur votre poste de travail, vous devez créer un fichier gradle.properties dans le répertoire $HOME/.gradle : systemProp.http.proxyHost=proxy systemProp.http.proxyPort=8888 systemProp.http.nonProxyHosts=localhost|127.0.0.1 systemProp.https.proxyHost=proxy systemProp.https.proxyPort=8888 systemProp.https.nonProxyHosts=localhost|127.0.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:2:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-proxy"},{"categories":null,"content":" Configuration d’un miroir Nexus ou ArtifactoryA l’instar du proxy, on va essayer de mettre en place une configuration globale. Pour ce faire, on va utiliser les init scripts. Cette fonctionnalité est très intéressante. Elle permet de centraliser des actions et configurations. Pour créer un script, il faut tout d’abord créer un fichier .gradle dans le répertoire $HOME/.gradle/init.d. Voici un exemple pour Nexus: allprojects { buildscript { repositories { mavenLocal() maven {url \"https://url-nexus\"} } } repositories { mavenLocal() maven { url \"https://url-nexus\"} } } ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:3:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-miroir-nexus-ou-artifactory"},{"categories":null,"content":" Configuration du déploiement dans Nexus / ArtifactoryLe déploiement dans Nexus est possible via le plugin maven publish. La configuration fournie dans la documentation est tellement bien faite ( comme le reste d’ailleurs ) que je ne vais que mettre un lien vers celle-là: Voici le lien. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:4:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-du-déploiement-dans-nexus--artifactory"},{"categories":null,"content":" ConclusionAprès ces quelques actions vous pourrez démarrer des builds avec gradle tout en étant compatible avec un environnement « Maven ». Enjoy 🙂 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:5:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#conclusion"},{"categories":null,"content":"Je suis en train de mettre en œuvre des tests de performance avec Gatling. Un des principaux outils libres de tests de performance. J’ai eu récemment à résoudre un « petit » soucis : je souhaitai partager des variables entre plusieurs scénarios. Il existe pas mal de solutions sur stackoverflow. J’ai condensé certaines d’entre elles pour les adapter à mon besoin. Ces variables sont issues de exécution d’une seule requête et sont automatiquement injectées dans les scénarios suivants. Ce mécanisme permet par exemple de récupérer un jeton d’un serveur d’identification et de l’injecter pour le scénario que l’on souhaite tester. Pour ce faire, il faut ajouter une variable de type LinkedBlockingDeque et injecter le contenu choisi via la session val holder = new LinkedBlockingDeque[String]() ... val firstScenario = scenario(\"First Simulation\") .exec(http(\"first scenario\") .post(\"/base/url1\") .check(jsonPath(\"$.my_variable\").find.saveAs(\"variable\"))) .exec(session =\u003e { holder.offerLast(session(\"variable\").as[String]) session} ); Maintenant on peut l’utiliser dans un autre scénario comme feeder: val secondScenario = scenario(\"Second Simulation\") .feed(sharedDataFeeder) Voici l’exemple complet En espérant que cela puisse aider à certain.e.s d’entre vous 🙂 ","date":"2019-11-21","objectID":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/:0:0","series":null,"tags":["gatling","planetlibre","scala"],"title":"Partager des variables entre scénarios gatling","uri":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/#"},{"categories":null,"content":"Une fois n’est pas coutume, voici un article qui reprend des basiques de la programmation. J’aborde une stack JAVA, mais c’est applicable à d’autres langages. Il existe une fonctionnalité très intéressante dans Spring (et dans J(akarta)EE) que l’on oublie assez souvent : l’AOP ou encore la programmation par aspect. Cette manière de programmer permet notamment de séparer le code fonctionnel et technique. Si vous faites du JAVA, vous utilisez déjà l’AOP. En effet, quand vous faites une insertion en base via JPA dans un EJB ou un bean annoté @Transactional, une transaction est initiée au début de la méthode et fermée à la fin. Avec Spring et notamment dans Spring boot, voici comment initier l’AOP. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:0:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#"},{"categories":null,"content":" Configuration mavenAjouter le starter AOP: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-aop\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:1:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#configuration-maven"},{"categories":null,"content":" Activation des aspectsDans la configuration ci-dessous, je prendrai comme exemple le logging des méthodes ( un log en début de méthode et un log en fin ). La définition des aspects se fait dans des classes annotées par @Configuration. @Configuration @Aspect @ConditionalOnProperty(name = \"debug.enabled\", havingValue = \"true\") public class DebuggingConfiguration { private static final Logger LOGGER = LoggerFactory.getLogger(DebuggingConfiguration.class); private static final String WITHIN_MY_PACKAGE = \"within(my.package..*)\"; /** * Log before execution * * @param joinPoint the current method */ @Before(WITHIN_MY_PACKAGE) public void logBeforeExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"Beginning of method : [{}]\", joinPoint.getSignature().getName()); } } /** * Log after execution * * @param joinPoint the current method */ @After(WITHIN_MY_PACKAGE) public void logAfterExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"End of method : [{}]\", joinPoint.getSignature().getName()); } } } L’utilisation de l’ annotation @ConditionalOnProperty me permet d’activer cette classe de configuration seulement si la propriété debug.enabled est initialisée à true. Les annotations @Before et @After indiquent à Spring AOP quand exécuter ces méthodes ou sur quelles méthodes. Dans mon cas, quand les méthodes appelées sont définies dans les classes d’un package défini. Pour plus de détails sur la syntaxe et les possibilités, vous pouvez vous référer à la documentation. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:2:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#activation-des-aspects"},{"categories":null,"content":"Après avoir soumis mon article sur le coaching des développeurs, je me suis rendu compte que j’ai oublié pas mal de points qui, à bien y réfléchir, me paraissent essentiels. Dans mon précédent article ( the first blood pour le coup ) je me suis attardé sur le « quoi » : toutes les actions que j’ai testé dans l’encadrement des jeunes développeurs et des développeurs en général. Maintenant, je vais essayer de m’attarder sur le « comment » : ma démarche, la posture que l’on doit adopter ( ce n’est que mon ressenti ) etc. Je vais commencer par ce dernier point. Quand on est architecte, développeur sénior ou bien encore tech lead, on est amené à encadrer techniquement des développeurs. Vous pouvez adopter plusieurs postures: A ce stade de lecture de cet article, vous vous dites, quelle est la bonne photo et donc la posture à adopter ? A mon avis, elles sont à proscrire individuellement. Je pense qu’il faut les panacher. Tout d’abord, il faut se souvenir de notre début de carrière et se rappeler du code que l’on a réalisé. J’ai par exemple gardé les premiers programmes réalisés en entreprise ( Servlet, JSP, JAVA 1.2, des méthodes de 3km de long, de la duplication de code en veux tu en voila, …) . Ça me permet de relativiser, d’être assez compréhensif et d’éviter de prendre les gens de haut. Cependant, cette prise de conscience ne doit pas vous empêcher de faire progresser votre entourage et surtout de leur faire éviter les écueils que vous avez vécu. Les ateliers et documentation que vous pourrez leur transmettre sont donc primordiaux. Par exemple, faire lire « Clean Code » ou « Effective Java » aux développeurs – je ne l’oblige pas mais incite fortement – est un moyen de leur faire gagner du temps dans leur apprentissage du code. Ensuite, même si vos padawans vous voient soit comme Pascal le grand frère ou maître Yoda (pour flatter mon égo), il ne faut pas oublier les exigences que vous avez fixé. L’industrie logicielle a gagnée en maturité en favorisant par exemple l’industrialisation via les outils de CI/CD ou bien encore en facilitant l’application de principes de qualité via des outils d’analyse des dépendances (dependency track) et du code (sonarqube). Vous devez vous adapter, favoriser l’adoption de ces pratiques et imposer quelques étapes qualité de préférence automatisée via de la CI. Pour favoriser l’adoption de toutes vos exigences, je conseille d’y aller progressivement. Il ne faut pas oublier que votre objectif est de faire « grandir » vos collègues. Pour cela essayez de les adapter et les faire évoluer dans le temps. Par exemple, pour les tests unitaires, commencez pas mettre en place les différents indicateurs qui vous permettront de mesurer la couverture de code. Ensuite, exigez un niveau de couverture de code (ex. 30%). Suivez le, via les quality gates SonarQube et enfin augmentez le progressivement : 30% , 40%,… Si vous commencez dès le début par un objectif trop haut, ce dernier paraîtra inatteignable et découragera tout le monde. Mieux vaut commencer volontairement très bas pour favoriser l’adoption. Dans un autre domaine, pour vos workflows GIT, vous pouvez commencer dans un premier temps par le workflow de feature branch. Ce dernier posera les bases des pipelines CI, des merge requests et des bonnes pratiques liées à la gestion de configuration. Une fois tout le cérémonial lié à GIT assimilé par votre équipe, passer à GITFLOW sera beaucoup simple. Bref, cette démarche revient à parler de conduite du changement. Il faut identifier vos exigences minimales. Celles-ci doivent être acceptées par votre hiérarchie ET par vos collègues. Sans ça vous échouerez! Si ils vous soumettent quelques idées ou adaptations, n’hésitez pas à les incorporer. Ça peut faciliter l’adoption! Ensuite, planifiez une progression sur 1 ou 2 ans. Cela donnera à vos collègues dans un premier temps des premiers objectifs atteignables puis une marge de progression leur permettant de s’améliorer. Enfin, n’hésitez pas à faire un bilan ( par ex. a","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs \u0026#8211; The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#"},{"categories":null,"content":" ConclusionA mon avis le management et l’encadrement de personnes n’est pas à prendre à la légère. Votre attitude ainsi que la démarche que vous voulez mettre en œuvre feront autant voir plus que toute la documentation et formations que vous mettrez en place. ","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs \u0026#8211; The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#conclusion"},{"categories":null,"content":"Auparavant, dans nos tests, quand on voulait mocker des méthodes « final » ou statiques, on devait passer par PowerMock. Depuis peu, si on utilise Mockito ( \u003e2.1) , on n’a plus besoin d’ajouter PowerMock pour mocker des méthodes « final ». Bon il reste toujours la gestion des méthodes statiques à gérer autrement qu’avec Mockito, mais cela va dans le bon sens. Voici comment activer en quelques commandes le mocking des méthodes « final ». Dans le répertoire src/test/resources, il faut créer un répertoire mockito-extensions avec un fichier nommé org.mockito.plugins.MockMaker. src/test/resources └── mockito-extensions └── org.mockito.plugins.MockMaker A l’intérieur de ce fichier, vous devrez ajouter le contenu suivant : mock-maker-inline Avec cette configuration, vous pourrez dorénavant mocker des méthodes « final » 🙂 Enjoy ","date":"2019-08-16","objectID":"/2019/08/16/mocker-des-methodes-final-avec-mockito/:0:0","series":null,"tags":["java","mockito","planetlibre","tests-unitaires"],"title":"Mocker des méthodes « final » avec Mockito","uri":"/2019/08/16/mocker-des-methodes-final-avec-mockito/#"},{"categories":null,"content":"Juste pour un pense bête, voici comment paramétrer GIT et GITHUB/GITLAB pour signer les commits avec GPG. ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:0:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#"},{"categories":null,"content":" Configuration GPGExécutez la commande suivante : gpg --full-generate-key Sélectionnez une clé RSA (question 1) de 4096 bits (question 2). Une fois cette commande effectuée, vous pouvez récupérer votre clé GPG avec cette commande: gpg --list-secret-keys --keyid-format LONG alexandre@.... /home/alexandre/.gnupg/pubring.kbx ---------------------------------- sec rsa4096/XXXXXXXXXX 2019-08-09 [SC] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX uid [ ultime ] Alexandre Touret \u003cmon.mail.github.ou.gitlab@monprovider.fr\u003e ssb rsa4096/XXXXXXXXXX 2019-08-09 [E] Ensuite, il faut exécuter cette commande gpg --armor --export XXXXXXXXXX ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:1:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-gpg"},{"categories":null,"content":" Configuration GITIndiquez la clé GPG à GIT git config --local user.signingkey XXXXXXXXXXXX Et indiquez que vous voulez signer tous vos commits git config --local commit.gpgsign true Si vous ne faites pas cette dernière commande, vous devrez ajouter l’option -S à chaque exécution de la commande git commit. Exemple: git -a -S -m \"Ajout javadoc\" ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:2:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-git"},{"categories":null,"content":" Configuration GITHUBSur Github ( il y a la même chose sur gitlab), vous pouvez dans vos paramètres ajouter cette clé . De cette manière, vos prochains commits envoyés seront vérifiés. En espérant que ça serve à d’autres 🙂 ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:3:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-github"},{"categories":null,"content":"En changeant de société l’année dernière j’ai eu l’impression de monter d’un cran dans la pyramide des ages. Pour faire plus simple, je me suis senti un peu plus vieux. Si vous avez quelques années d’expérience dans le développement ou tout simplement dans la technique, vous avez déjà eu l’occasion de coacher ou d’encadrer techniquement des jeunes diplômés. Et oui, c’est un signe ! Maintenant vous avez assez de recul ( pour ne pas dire que vous êtes vieux/vieille) pour encadrer techniquement des jeunes ingénieur.e.s Certes vous n’avez pas fait le choix de partir vers la gestion de projet ou le management. Cependant l’encadrement technique ( vous pouvez l’appeler mentorat, tutorat, apprentissage,… ) est nécessaire pour faire monter en compétence les nouveaux arrivants et les rendre autonomes. Je vais essayer de mettre en lumière quelques pratiques que je mets en œuvre et que j’ai pu remettre au goût du jour depuis un an. Si vous avez des idées, avis, n’hésitez pas à les mettre en commentaire. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#"},{"categories":null,"content":" DocumentationIl y a plusieurs types de documentation que je partage. Tout d’abord, j’ai partagé quelques sites et ouvrages qui me paraissent indispensables. Clean Code arrive en premier. Effective Java en second. A mon avis, ça ne sert pas à grand chose d’aller plus loin dans le développement si on n’a pas acquis les notions décrites dans ces livres! Puis vient le refactoring puis les design patterns. Ensuite, j’essaye de partager via notre chat interne les quelques solutions trouvées dans les projets. Enfin, j’ essaye de m’ astreindre à mettre à jour la documentation. Oui c’est un combat de tous les jours 😀 Ça commence par les exemples de code. J’essaye d’ avoir des repos git assez lisibles (c.-à-d. avec un README intelligible) et un code à jour correspondant aux normes en vigueur. Un exemple, j’ai crée un projet permettant d’ illustrer la mise en œuvre des tests unitaires et d’intégration dans un projet standard (spring, tomcat, docker,…). Ces éléments nécessitent un travail important, que ça soit à la création ou pour tenir à jour la documentation. Cependant, ça me permet de ne pas me répéter, et d’ illustrer via un cas pratique ce que j’attends dans les Merge Requests. En effet, chaque développement est assujetti à une Definition of Done ( tests, qualité, …) . Il faut donc que la qualité de la documentation soit en rendez vous ! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#documentation"},{"categories":null,"content":" VeilleAu delà de la documentation, je « pousse » aux différents dev, les articles que je trouve pertinent pendant ma veille technologique. J’invite également tout le monde à en faire. Je ne peux pas les obliger. Maintenant comme je peux le dire régulièrement. Si on souhaite rester dans la technique, il faut se tenir à jour. La veille (sites web, confs, livres,…) en est le meilleur moyen. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:2:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#veille"},{"categories":null,"content":" Ateliers / WorkshopsOrganiser un workshop ou atelier d’une heure ou deux max est un bon moyen de fédérer les troupes. J’essaye d’organiser deux types d’atelier. Le premier est uni directionnel : Une personne présente un sujet technique et les autres en profitent. Ça permet tout d’abord de diffuser plus simplement certains messages. Par exemple, j’ai organisé une présentation de 30 mn sur l’utilisation de NULL dans le code et l’utilisation des Optional. Le deuxième est plus long à préparer. C’est un atelier organisé à la manière d’un hands on sur un sujet très précis. Pendant 1H ou 2H, l’équipe planche sur un sujet. La session est organisé et animé idéalement par un ou plusieurs membres de l’équipe ( ça ne vous empêche pas d’avoir votre mot à dire lors de la préparation 😀 ). Récemment j’ai co-organisé un hands on « Clean Code » en illustrant quelques notions qui nous paraissaient essentielles. Ces évènements sont évidemment chronophages mais offrent un certains retour sur investissement. Outre la présentation technique des différents sujets, les membres de l’équipe se forment et apprennent. Ils peuvent voir en situation les différentes notions que vous évoquez (en fait je les rabâche) lors des MR ou pendant les revues de code. Aussi, je pense que ça contribue à une certaine émulation technologique. Ça prend du (beaucoup de) temps, mais ça en vaut la peine! L’idéal dans ce genre d’exercice est quand tout le monde propose des sujets. Pas seulement l’architecte ou le lead dev. Les développeurs peuvent prendre le lead dans cet exercice. Ca permet d’une part de les valoriser, de les faire monter en compétence. Quoi de mieux pour approfondir un sujet que de monter un talk et/ou hands on dessus ? ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:3:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#ateliers--workshops"},{"categories":null,"content":" Revues de codeJe ne vais pas aborder dans ce chapitre les revues de code que l’on peut faire dans le cadre des projets, lors des MR par exemple. Pour certaines personnes, surtout les juniors, je fais régulièrement une revue de code alternative. Je passe une 1/2 heure, une heure max sur un bout de code que le dev m’aura sélectionné. Je lis le code avec le développeur et je donne quelques axes d’amélioration: design patterns, tests unitaires, refactoring,… Tout y va. Ça permet de se poser et d’aborder quelques sujets: la programmation fonctionnelle, les IO en java,… ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:4:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#revues-de-code"},{"categories":null,"content":" Pour aller plus loinBien évidemment, beaucoup d’autres actions peuvent être mises en place. La plupart de l’accompagnement que je peux réaliser se fait quotidiennement, dans les projets. Pour aller un peu plus loin, un collègue a mis en place un système de mentorat pour accompagner les jeunes développeurs et accélérer leur montée en compétence. Cette idée est très intéressante et peut être appliquée dans beaucoup de contextes. Si vous avez des idées, questions, remarques, pratiques que vous développez chez vous, n’hésitez pas à les partager! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:5:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#pour-aller-plus-loin"},{"categories":null,"content":"Si vous provisionnez vos VM VirtualBox avec Vagrant, vous avez sans doute eu l’idée d’automatiser le provisionning des machines virtuelles. Dans mon cas une VM GNU/Linux basée sur Debian 9. Pour cela, soit vous faite tout manuellement et après les mises à jour deviennent fastidieuses, soit vous appliquez un script shell au démarrage de vagrant, soit vous utilisez Ansible. Ansible est un outil opensource permettant d’automatiser le provisionning et la mise à jour des environnements à distance (via SSH). L’avantage par rapport à des outils tels que Puppet, est qu’il ne nécessite pas l’installation d’agent. Je vais essayer de vous montrer comment mettre en place le provisionning via Ansible pour VirtualBox. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:0:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#"},{"categories":null,"content":" Configuration de VagrantDans le fichier Vagrantfile, on active le provisionning via Ansible: config.vm.provision \"ansible_local\" do |ansible| ansible.playbook = \"site.yml\" ansible.install_mode = \"pip\" ansible.version = \"2.7.10\" end Cette configuration fait référence à un fichier « playbook » site.yml. C’est la configuration qui sera appliqué lors du provisionning . Que ça soit à la création ou pour les mises à jour. Voici un exemple de contenu: - name: VirtualBox hosts: all become: yes become_user: \"root\" become_method: \"sudo\" roles: - common: vars_files: - vars/environment.yml Ce fichier est la racine de notre configuration Ansible. On y référence les rôles appliqués et les fichiers d’ environnement. Voici un exemple de rôle: - name: \"Remove useless packages from the cache\" apt: autoclean: yes force_apt_get: yes - name: \"Remove dependencies that are no longer required\" apt: autoremove: yes force_apt_get: yes - name: \"Update and upgrade apt packages (may take a while)\" become: true apt: upgrade: dist update_cache: yes force_apt_get: yes - name: \"Install useful packages\" become: true apt: name: - gcc - g++ - ... - zsh - firewalld state: present update_cache: no - name: ansible create directory example file: path: \"{{ home }}/.m2\" state: directory owner: \"{{ username }}\" group: \"{{ username }}\" - name: Install Maven settings.xml copy: src: settings.xml dest: \"{{ home }}/.m2/settings.xml\" owner: \"{{ username }}\" group: \"{{ username }}\" - name: \"Install Maven\" raw: \"curl -sL \\\"http://mirror.ibcp.fr/pub/apache/maven/maven-3/{{ maven_version }}/binaries/apache-maven-{{ maven_version }}-bin.tar.gz\\\" -o /opt/apache-maven.tar.gz \u0026\u0026 tar -zxf /opt/apache-maven.tar.gz -C /opt\" become: true become_user: root become_method: sudo - name: \"Change Maven Rights\" file: path: /opt/* state: touch modification_time: \"preserve\" access_time: \"preserve\" owner: \"{{ username }}\" group: \"{{ username }}\" Les variables d’environnement permettent de variabiliser certains champs de vos rôles. On peut trouver par exemple les versions de certains outils déployés maven_version: 3.5.4 username: vagrant home: /home/vagrant docker_compose_version: 1.22.0 Il y a une quantité impressionnante de modules Ansible que l’on peut utiliser. Que ça soit pour lancer des commandes shell ou lancer des services. Contrairement à la création d’un script shell qui pourrait faire les mêmes actions à la création, on peut facilement gérer la mise à jour de la VM car Ansible détecte les modifications lors de son exécution. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-de-vagrant"},{"categories":null,"content":" Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: ansible.cfg [defaults] hostfile = hosts hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-spécifique-pour-virtualbox"},{"categories":null,"content":" Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: ansible.cfg [defaults] hostfile = hosts hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#ansiblecfg"},{"categories":null,"content":" Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: ansible.cfg [defaults] hostfile = hosts hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#hosts"},{"categories":null,"content":" Provisionning","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#provisionning"},{"categories":null,"content":" A la créationle provisionning peut se faire au lancement de vagrant via la commande: vagrant up Pour faire une mise à jour Directement dans la box, vous pouvez lancer les commandes suivantes : sudo mount -t vboxsf vagrant /vagrant Puis, vous pouvez lancer les commandes suivantes dans la box: su - cd /vagrant export ANSIBLE_CONFIG=/vagrant ansible-playbook site.yml ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#a-la-création"},{"categories":null,"content":"En attendant de prendre mon train, j’essaye de me remettre de cette nouvelle édition. Cette année JAVA est revenu au premier plan. Que ça soit via la spécification microprofile, quarkus , graalvm ou encore par les problématiques de migration JDK 8 -\u003e 11. On a pas mal vu des architectures micro services à base de service mesh (istio) et kubernetes. A coté des sujets techniques, un des sujets majeurs était le bien être et la bienveillance au travail. Les vidéos des conférences seront bientôt retransmises sur le channel Youtube de DevoxxFR. D’une manière générale, le niveau des conférences est toujours très bon. J’ai particulièrement apprécié les confs suivantes. N’hésitez pas à les visionnez une fois qu’elles seront disponibles sur Youtube. Cycle de vie des applications dans k8s Créer facilement des microservices avec Eclipse microprofile Back to Basics, ne perdez plus de temps avec les dates Hexagonal at scale Comment concevoir une API REST D’Architecte à MetaArchitecte: Une évolution nécessaire Il y a aussi certaines conférences ou j’ai eu un bon écho : Oubliez JavaEE, voilà JakartaEE Back to Basics, ne perdez plus de temps avec les dates Comprendre les GC à faible latence Je pense qu’il y a encore bien d’autres conférences qui ont été très intéressantes. J’ ai quelques heures de visionnage à prévoir dans mon agenda 🙂 . Quoi qu’il en soit, merci aux organisateurs pour cette édition. C’était top! Rendez vous l’année prochaine ! ","date":"2019-04-20","objectID":"/2019/04/20/devoxx-2019/:0:0","series":null,"tags":["conférence","devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2019","uri":"/2019/04/20/devoxx-2019/#"},{"categories":null,"content":"Après avoir mis à jour mon mot de passe Spotify ( oui, il faut modifier régulièrement ses mots de passe ) , j’ai eu un petit soucis sur MoodeAudio ( version 4.4) et notamment sur la connexion avec Spotify. Après quelques recherches sur le forum de moodeaudio, j’ai trouvé la correction qui allait bien. Voici comment faire : D’abord on se connecte via SSH sur le raspberry pi $ ssh pi@192.168.0.xx Puis on lance la commande: $ sudo mv /var/local/www/spotify_cache/credentials.json /home/pi/ $ sudo reboot Normalement, Spotify Connect devrait fonctionner après le redémarrage 🙂 ","date":"2019-03-15","objectID":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/:0:0","series":null,"tags":["moodeaudio","planetlibre","spotify"],"title":"Au secours! Spotify Connect ne fonctionne plus sur MoodeAudio","uri":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/#"},{"categories":null,"content":"Dans la série j’équipe ma maison en Raspberry PI, j’ai décidé de me doter d’une station radio connectée qui me permettrait de « moderniser » un peu ma chaîne HI-FI. Mes besoins sont: Connexion en analogique à une chaîne HI-FI Jouer des MP3/FLAC stockés dans un NAS Jouer des web radios (ex. FIP, TSF JAZZ) Connexion SPOTIFY Une interface web sympa Après quelques recherches, j’ai donc opté pour une solution basée sur un DAC JustBoom, un Raspberry PI et la distribution MoodeAudio. Voici le DAC que l’on branche directement sur le port GPIO du Raspberry PI: L’installation et la configuration du DAC se sont très bien passées. L’installation se fait comme avec des LEGOs. Pour la configuration, j’ai testé dans un premier temps Volumio puis MoodeAudio. Pour l’instant, je reste sur cette dernière. Toutes les fonctionnalités que je souhaite sont en standard. Pas besoin de plugins tiers. Toutes les étapes d’ installation et de configuration pour que le DAC soit reconnu sont décrites ici. Les gens de chez JustBoom ont bien documenté la configuration pour les principales distributions. Le seul reproche que je trouve à MoodeAudio est l’ergonomie. Sur un téléphone, ce n’est pas top. Surtout sur l’accès aux menus d’administration. J’ai du également ajouter des radios manuellement alors que dans Volumio, avec le plugin TuneIn, ça pouvait se faire automatiquement. Je me suis basé sur les informations fournies par ce site. Quoi qu’il en soit, tout ce que je souhaitais fonctionne super bien! Spotify Connect, l’écoute de TSF JAZZ, la lecture des morceaux de ma bibliothèque fonctionnent nickel ! ","date":"2019-03-07","objectID":"/2019/03/07/une-radio-connectee-diy/:0:0","series":null,"tags":["planetlibre","raspberry-pi"],"title":"Une radio connectée DIY","uri":"/2019/03/07/une-radio-connectee-diy/#"},{"categories":null,"content":"J’ai fini l’édition 2019 du Touraine Tech. Tout d’abord, merci aux organisateurs pour l’accueil et l’organisation. C’était vraiment top! Cette année, je n’ai pas pu faire beaucoup de conférences. Mon Hands on m’ayant retenu une bonne partie de l’après midi, que ça soit durant le talk ou après pour décompresser 🙂 Mon hands on portait sur l’architecture, j’ai eu une vingtaine de personnes qui l’on suivi et ont pratiqué sur différents sujets. {% include gallery caption=“Un kata d’architecture à TNT” layout=“half” %} La cave à vin connecté a remporté un franc succès, du moins pendant la présentation des sujets 🙂 Voici le feedback que j’ai eu sur ma présentation: RDV l’année prochaine ! ","date":"2019-02-02","objectID":"/2019/02/02/touraine-tech-2019-2/:0:0","series":null,"tags":["architecture","conférence","handson","TNT19"],"title":"Touraine Tech 2019","uri":"/2019/02/02/touraine-tech-2019-2/#"},{"categories":null,"content":"Voici ma deuxième contribution pour une série d’articles sur l’opensource pour le blog de mon entreprise. Les articles précédents traitaient de l’histoire de l’opensource puis des différentes formes que peut prendre l’open source. Cette fois j’aborde les business models du monde opensource. Bonne lecture 🙂 ","date":"2019-01-23","objectID":"/2019/01/23/deuxieme-crossover-opensource-business-models/:0:0","series":null,"tags":["planetlibre"],"title":"Deuxième crossover : Opensource business models","uri":"/2019/01/23/deuxieme-crossover-opensource-business-models/#"},{"categories":null,"content":"Voila la description de la conférence/ hands que j’animerai au Touraine Tech est en ligne 🙂 Vous trouverez le descriptif sur cette page. ","date":"2019-01-14","objectID":"/2019/01/14/objectif-top-architecte/:0:0","series":null,"tags":["tourainetech"],"title":"Objectif Top Architecte !","uri":"/2019/01/14/objectif-top-architecte/#"},{"categories":null,"content":"Mon sujet de talk « Objectif Top Architecte » a été retenu pour l’édition 2019 de Touraine Tech. Réservez le 1 février 2019 dans votre agenda ! Tout d’abord merci aux organisateurs pour leur confiance. Je suis vraiment honoré d’être sélectionné une deuxième année consécutive. Cette année, j’animerai un hands on sur l’architecture. Je vais tâcher de vulgariser quelques principes qui me paraissent importants et animer un « coding dojo de l’architecture ». Pas besoin d’être architecte ou (vraiment, … mais vraiment pas besoin) d’avoir une certification TOGAF pour y participer 🙂 ","date":"2018-12-25","objectID":"/2018/12/25/touraine-tech-2019/:0:0","series":null,"tags":["architecture","conférence","tourainetech"],"title":"Touraine Tech 2019","uri":"/2018/12/25/touraine-tech-2019/#"},{"categories":null,"content":"Il y a quelques jours, je cherchais comment tracer rapidement et simplement les entrées sorties d’une API REST en appliquant quelques formatages, des filtres, et des insertions en base si besoin. Travaillant sur une stack SpringBoot, vous allez me dire : oui tu peux faire des filtres. Pour être franc, j’ai essayé d’ appliquer des interceptor et filtres mais dans mon contexte, ça ne collait pas. Me voilà donc à la recherche d’une solution faisant le taff et qui soit peu intrusive dans mon contexte. J’ai trouvé par hasard au fil de mes lectures sur Stackoverflow le framework logbook réalisé par … Zalando ( et oui, ils ne font pas que des chaussures) en licence MIT. Ce composant ne fait qu’une seule chose, mais il le fait bien ! Il permet entre autres de s’intégrer dans une stack JAVA ( JAX-RS ou SpringMVC), de filtrer, récupérer les différentes informations des requêtes et réponses et enfin de formatter selon l’envie (ex. JSON). Voici un exemple de mise en œuvre dans un projet SpringBoot: Dans le fichier pom.xml, ajouter cette dépendance: \u003cdependency\u003e \u003cgroupId\u003eorg.zalando\u003c/groupId\u003e \u003cartifactId\u003elogbook-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.11.2\u003c/version\u003e \u003c/dependency\u003e Dans une de vos classes Configuration, définir la factory de Logbook @Bean public Logbook createLogBook() { // too easy : return Logbook.create(); return Logbook.builder() .condition(Conditions.requestTo(\"/helloworld\")) .formatter(new JsonHttpLogFormatter()).build(); } Dans mon cas j’ai fait un filtre en n’incluant que l’ API /helloworld et j’ai formatté en JSON. On peut également modifier le processus d’écriture pour ne pas écrire dans un fichier mais en base par ex. Ensuite, j’ai ajouté la configuration du logger dans le fichier application.properties logging.level.org.zalando.logbook:TRACE Et voila ! Dans la console, lors d’un appel ou d’une réponse à mon API, j’ai le message suivant : 018-12-01 15:14:18.373 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"remote\",\"type\":\"request\",\"correlation\":\"c6b345013835273f\",\"protocol\":\"HTTP/1.1\",\"remote\":\"127.0.0.1\",\"method\":\"GET\",\"uri\":\"http://127.0.0.1:8080/helloworld\",\"headers\":{\"accept\":[\"/\"],\"host\":[\"127.0.0.1:8080\"],\"user-agent\":[\"curl/7.52.1\"]}} 2018-12-01 15:14:18.418 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"local\",\"type\":\"response\",\"correlation\":\"c6b345013835273f\",\"duration\":48,\"protocol\":\"HTTP/1.1\",\"status\":200,\"headers\":{\"Content-Length\":[\"11\"],\"Content-Type\":[\"text/plain;charset=UTF-8\"],\"Date\":[\"Sat, 01 Dec 2018 14:14:18 GMT\"]},\"body\":\"Hello world\"} Vous remarquerez que les requêtes / réponses peuvent désormais être associés grâce à un identifiant de corrélation. On peut facilement déterminer le temps de traitement d’une requête ou encore faciliter les recherches. Vous trouverez tout le code dans ce repo github. ","date":"2018-12-01","objectID":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/:0:0","series":null,"tags":["logbook","planetlibre","spring","springboot"],"title":"Tracer (facilement) les entrées sorties d'une API REST","uri":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/#"},{"categories":null,"content":"En attendant d’avoir plus d’imagination, voici un rapide tuto pour gérer plusieurs référentiels GIT avec des clés SSH différentes. Imaginons que vous deviez vous connecter sur différents serveurs GIT (ex. github et gitlab) avec des emails différents et donc des clés RSA différentes ( oui je sais ce cas n’arrive pas souvent ). Le tout sous Windows et GNU/LINUX. Sous GNU/LINUX ont peut le gérer différemment via la commande ssh-add. Pour pouvoir gérer ceci de manière simple, j’ai fait la manipulation suivante : Dans le répertoire ~/.ssh, j’ai crée les différentes clés avec la doc fournie par GITHUB. Puis, j’ai crée le fichier ~/.ssh/config avec le contenu suivant: Host monhost1.fr HostName monhost1.fr User git IdentityFile ~/.ssh/id_rsa Host monhost2.fr HostName monhost2.fr User git IdentityFile ~/.ssh/nouvellecle_rsa Et voilà ! Après avoir fait les différentes configurations coté serveur (c.-a-d. ajout des clés publiques), je peux interagir avec les différents serveurs (pull, push). En espérant que ça puisse servir à d’autres. ","date":"2018-11-16","objectID":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/:0:0","series":null,"tags":["git","planetlibre"],"title":"Gérer plusieurs clés et plusieurs repo GIT","uri":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/#"},{"categories":null,"content":"je n’ai pas écrit beaucoup de choses sur mon blog ces derniers temps. C’était en partie dû au fait que j’étais en train d’écrire un article avec R. SEMETEYS pour le blog de mon entreprise. Cet article est disponible ici. Il essaye de synthétiser l’histoire de l’open source. J’espère que vous ne serez pas rebuté par l’anglais ( c’est un exercice 🙂 ) Bonne lecture 🙂 ","date":"2018-10-29","objectID":"/2018/10/29/premier-cross-over/:0:0","series":null,"tags":["planetlibre"],"title":"Premier cross over \u0026#8230;","uri":"/2018/10/29/premier-cross-over/#"},{"categories":null,"content":"Bon, ça fait quelques temps que je n’ai rien posté… Voici un rapide tuto pour installer docker-ce sur une debian9. Oui, je sais, docker est déjà présent sur les dépôts, mais si vous souhaitez avoir une version un peu plus récente, vous pouvez passer par l’installation de la version ce fournie par docker. ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#"},{"categories":null,"content":" Pré-requisSupprimer les éventuelles installations de docker et docker-compose #apt-get remove docker docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:1:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#pré-requis"},{"categories":null,"content":" InstallationLancer les commandes suivantes: # apt-get install apt-transport-https ca-certificates # curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add \u0026#8211; # add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\" Puis lancer # apt update # apt install docker-ce ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation"},{"categories":null,"content":" Installation de docker-composeLancer les commandes suivantes: # curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # chmod a+x /usr/local/bin/docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation-de-docker-compose"},{"categories":null,"content":" Configuration des droitsPour lancer docker depuis un utiliser non root, il faut lancer les commandes suivantes: # groupadd docker # adduser monutilisateur docker # usermod -aG docker monutilisateur Après ceci, vaut mieux redémarrer le pc … ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:3:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-des-droits"},{"categories":null,"content":" Configuration du démonVoici quelques config à appliquer pour que le démon soit accessible par des outils tels que le plugin maven ou encore configurer l’accès à un proxy ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-démon"},{"categories":null,"content":" Configuration du portExécuter la commande: # systemctl edit docker.service Entrer le code suivant: [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock Et l’enregistrer sous /etc/systemd/system/docker.service.d/docker.conf ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-port"},{"categories":null,"content":" Configuration du proxyAvec la même commande # systemctl edit docker.service Entrer la configuration suivante: [Service] Environment=\"HTTP\\_PROXY=http://mon\\_proxy:mon_port/\" Environment=\"NO_PROXY=127.0.0.1\" ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:2","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-proxy"},{"categories":null,"content":" Activation des configurationsLancer les commandes suivantes: # systemctl daemon-reload # systemctl restart docker ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:3","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#activation-des-configurations"},{"categories":null,"content":" ValidationMaintenant, vous pouvez valider votre configuration avec la commande: $ docker run hello-world ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:5:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#validation"},{"categories":null,"content":"L’édition 2018 de DEVOXX touche bientôt à sa fin. Pour ceux qui ne connaissent pas cette conférence, c’est LA conférence sur le développement en France. A titre personnel, je peux plus apprendre en trois jours à cette conférence qu’en formation. Tout d’abord un grand merci aux organisateurs. Ils assurent réellement. Bon, pour l’année prochaine, n’hésitez à retenir ma conférence 😉 Si vous n’avez pas eu la chance d’assister aux trois jours, il faut savoir que vous pourrez voir les rediff sur la chaine youtube. ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:0:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#"},{"categories":null,"content":" Les tendancesVoici les tendances que j’ai retenu : Spring, spring et encore spring Du réactif en veux tu en voila Du DDD sinon rien Du devops et le plus impressionnant pour moi était la conférence de JOSHUA BLOCH(!!!) sur Effective Java. Pas tant dans le contenu, car il reprenait peu ou prou celui du livre, mais de voir une personne de ce calibre (dans le monde JAVA, c’est une rock star) en France, c’est assez impressionnant. Les keynotes ( dont celle sur le smart building) étaient dans l’ensemble très intéressantes. Les conférences étaient également d’un très bon niveau. J’ai pu découvrir par exemple l’avance que peut avoir l’Estonie sur l’IT ( voir le projet x-road ) par rapport à la France qui lance le projet french road. Voici quelques conférences qui m’ont plu et interpelé : Être architecte en 2018 Le smart building Chaos Engineering Effective java Pourquoi vous avez besoin d’une clean architecture Sécurité des web applications Après java 8, java 9 et 10 Je ne vais pas trop les décrire ( voire pas du tout ), elles seront disponibles prochainement sur la chaine youtube . ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:1:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#les-tendances"},{"categories":null,"content":"Depuis quelques jours, je teste Apache Camel pour la mise en œuvre de médiations. Apache Camel est un framework assez ancien. Il est similaire à Spring Intégration et permet l’ implémentation de patterns d’intégration. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:0:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#"},{"categories":null,"content":" Les patterns d’intégrationQu’est-ce qu’un pattern d’intégration allez-vous me dire ? C’est une solution d’architecture ou plus simplement une recette de cuisine permettant d’avoir une solution toute prête à une problématique d’intégration donnée. L’ensemble de ces patterns est décrit sur ce site ( ne vous attardez pas sur le look des années 90 … ). Exemple : Camel permet simplement de gérer l’intégration via un DSL. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#les-patterns-dintégration"},{"categories":null,"content":" Choix d’implémentationsOn peut faire pas mal de choses avec ce FRAMEWORK et de plusieurs manières. J’ai fait les choix d’implémentation suivants : Tout se fera avec SPRING … et pas en XML 🙂 Il faut que toutes les médiations soient testables J’exécute le code dans un FATJAR ( pourquoi avec springboot ) ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:1","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#choix-dimplémentations"},{"categories":null,"content":" Configuration de la routeApache Camel définit les médiations dans des routes. Elles se définissent assez rapidement . Les routes commencent par une instruction from et se terminent par une ou plusieurs instructions to. Pour mon exemple, j’extrais les données d’une table et les stocke dans un fichier. Tout se configure par des URLs. La première permet d’extraire les données via JPA/HIBERNATE. Une entité Address permet le requêtage. La seconde permet le stockage dans un fichier texte JSON. Elles sont externalisées dans des fichiers de configuration pour faciliter les tests et accessibles via SPRING. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:2:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#configuration-de-la-route"},{"categories":null,"content":" Lancement de la routeLe lancement de la route se fait dans une méthode main() : ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:3:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#lancement-de-la-route"},{"categories":null,"content":" TestsCamel fournit une API de test assez bien fournie. Elle permet notamment de mocker des endpoints existants (ex. : le fichier de sortie de mon cas de test). Dans mon cas, j’ai décidé de remplacer la base de données que j’interroge en input par une base HSQLDB chargée en mémoire. Le fichier de sortie est, lui, remplacé dynamiquement par un mock. Pour ce faire, j’ai utilisé les « adviceWith » ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:4:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#tests"},{"categories":null,"content":" Pour aller plus loinIl y a pas mal d’exemples sur le GITHUB de CAMEL. Vous pouvez également acheter le livre « Camel In Action ». Ca ne vaut pas Effective Java 🙂 , mais vu qu’il est écrit par le principal développeur, c’est une très bonne référence. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:5:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#pour-aller-plus-loin"},{"categories":null,"content":"Et oui, il y a un equalizer dans debian….Pulse Audio dispose d’un equalizer. Bon ce n’est encore très user friendly, mais ça fonctionne! ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:0:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#"},{"categories":null,"content":" Installation de l’equalizer apt-get install pulseaudio-equalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:1:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#installation-de-lequalizer"},{"categories":null,"content":" ActivationAjouter les lignes suivantes dans le fichier /etc/pulse/default.pa load-module module-equalizer-sink load-module module-dbus-protocol Relancer le démon pulseaudio \\# pulseaudio -k \u0026\u0026 pulseaudio -D A ce stade, vous devriez avoir dans le panneau de configuration la référence à l’equalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:2:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#activation"},{"categories":null,"content":" LancementEn ligne de commande ( je vous disais que ce n’était pas trop user-friendly), lancer la commande $ qpaeq \u0026 On obtient cette interface: Arrivé à ce niveau, je suis quand même un peu déçu/ Il n’y a pas une vrai intégration dans debian ( pas de lanceur pour l’equalizer ) et il n’y a pas de presets configurés ( #souvienstoiwinamp) J’ai essayé de poster mon soucis sur IRC, mais je n’ai pas encore eu de réponse. Je pense soumettre un bug dans les prochains jours. ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:3:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#lancement"},{"categories":null,"content":"Vagrant est un outil permettant de construire des environnements de travail virtualisés hébergés sur vmware, virtualbox ou encore docker. Il permet par exemple de construire et gérer une VM dans un seul et même workflow et d’éviter les exports et partages de machines virtuelles ( tout est déclaré dans un seul et même fichier ). Voici comment je l’ai installé sur ma debian 9. ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:0:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#"},{"categories":null,"content":" InstallationLe paquet fourni dans la distribution n’est pas compatible avec la version de virtualbox fournie dans le repo virtualbox.org. j’ai donc installé la version disponible sur le site de vagrant. # dpkg -i vagrant\\_2.0.2\\_x86_64.deb ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:1:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation"},{"categories":null,"content":" Configuration","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#configuration"},{"categories":null,"content":" ProxySi vous avez un proxy, il faut effectuer le paramétrage suivant $ export http_proxy= »http://user:password@host:port » $ export https_proxy= »http://user:password@host:port » $ vagrant plugin install vagrant-proxyconf $ export VAGRANT\\_HTTP\\_PROXY= »http://user:password@host:port » $ export VAGRANT\\_NO\\_PROXY= »127.0.0.1\u0026Prime; $vagrant box add \\ precise64 https://files.hashicorp.com/precise64.box `$ export VAGRANT_DEFAULT_PROVIDER`=virtualbox [/code] ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:1","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#proxy"},{"categories":null,"content":" Installation d’une VMVoici un exemple pour une VM virtualbox basée sur ubuntu $ mkdir ~/vagrant $ cd ~/vagrant $ vagrant init pristine ubuntu-budgie-17-x64 $ vagrant up [/code] Avec ces quelques commandes j’obtiens un environnement ubuntu hébergé sur virtualbox sans avoir à installer et configurer la vm. Pour l’instant je ne rentre pas trop dans les détails de la construction des images. Peut-être que je m’y plongerai prochainement ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:3:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation-dune-vm"},{"categories":null,"content":"J’ai eu la chance d’être sélectionné pour la première édition de la conférence TouraineTech. Tout d’abord, je tiens à remercier toute l’équipe du Touraine Tech pour l’accueil et l’organisation de cette conférence. Ma présentation s’intitulait: Jenkins2 le retour (d’expérience). Je faisais un retour d’expérience sur la mise en œuvre de Jenkins 2 et des pipelines. Elle était au format quickie (15mn). J’ai pas mal préparé la présentation car c’était ma première dans ce domaine. Je trouve que ça s’est pas trop mal passé. J’ai fait quelques erreurs dans mes slides ou tout du moins je trouve que je n’ai pas eu l’effet escompté. Quoi qu’il en soit, je suis plutôt content du résultat. Les retours ont été assez satisfaisants. Voici le retour des participants : Pour ceux qui regrettaient de ne pas avoir de démos, j’en suis désolé, mais le format de 15mn ne s’y prêtait pas trop . Si j’avais eu plus de temps, j’aurai fait des démonstrations qui auraient beaucoup mieux illustré mon propos. Pour conclure, je pense rééditer cette expérience. Ça m’a vraiment plu. Je n’ai plus qu’à trouver un sujet pour l’édition 2019 de Touraine Tech 🙂 ","date":"2018-02-26","objectID":"/2018/02/26/ma-presentation-au-touraine-tech/:0:0","series":null,"tags":["planetlibre","tourainetech"],"title":"Ma présentation au Touraine Tech","uri":"/2018/02/26/ma-presentation-au-touraine-tech/#"},{"categories":null,"content":"Dans la série, j’essaye de sauvegarder toutes mes configurations, voici ce que j’ai fait pour configurer correctement cygwin. Pour ceux qui ne connaissent pas ou qui n’ont pas la chance d’utiliser windows au travail, cygwin est un shell avec tous les outils GNU. En attendant d’avoir windows 10 ( au travail ) et un BASH intégré, il n’y a pas mieux. Du moins à mon humble avis. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:0:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#"},{"categories":null,"content":" GIT","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#git"},{"categories":null,"content":" ComplétionOn a besoin des fichiers suivants git-completion.bash git-prompt.sh Je les ai téléchargé et placé dans le répertoire $HOME. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:1","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#complétion"},{"categories":null,"content":" Activation de la configuration et affichage de la branche en cours dans le promptJ’ai activé la configuration git en exécutant les scripts précédemment téléchargés. Voici la personnalisation que j’ai paramétré dans la variable d’environnement PS1: J’ ai également activé des propriétés qui étaient en commentaire dans ce fichier. Je ne les ai pas listée pour ne pas trop surcharger l’article 🙂 ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:2","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#activation-de-la-configuration-et-affichage-de-la-branche-en-cours-dans-le-prompt"},{"categories":null,"content":" Configuration Nom et sécurité ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:3","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#configuration-nom-et-sécurité"},{"categories":null,"content":" VIMQue serait un prompt sans vim ? J’ai installé une suite de plugin : The ultimate vimrc. Il faut cloner le repo GIT et lancer un script git clone --depth=1 https://github.com/amix/vimrc.git ~/.vim_runtime sh ~/.vim_runtime/install_awesome_vimrc.sh ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:2:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#vim"},{"categories":null,"content":"Désolé de remettre ça. Je remets sur mon blog ma configuration Debian. Histoire de ne pas la perdre tant qu’elle est dans mon historique . Voici ce que j’ai réalisé post-installation: ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:0:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#"},{"categories":null,"content":" Ajout dépôts supplémentairesDans le fichier /etc/apt/sources.list, ajouter les repo contrib et non-free . Activer également les mises à jour de sécurité. deb http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb http://security.debian.org/debian-security stretch/updates main non-free contrib deb-src http://security.debian.org/debian-security stretch/updates main non-free contrib # stretch-updates, previously known as 'volatile' deb http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:1:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#ajout-dépôts-supplémentaires"},{"categories":null,"content":" Logiciels tiers","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#logiciels-tiers"},{"categories":null,"content":" Etcher#echo \"deb https://dl.bintray.com/resin-io/debian stable etcher\" | sudo tee /etc/apt/sources.list.d/etcher.list \u003cpre\u003e#apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 379CE192D401AB61 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#etcher"},{"categories":null,"content":" Virtualbox # wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add - Dans le fichier /etc/apt/sources.list.d/virtualbox.list deb https://download.virtualbox.org/virtualbox/debian stretch contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#virtualbox"},{"categories":null,"content":" SpotifyDans le fichier /etc/apt/sources.list.d/spotify.list deb http://repository.spotify.com stable non-free ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:3","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#spotify"},{"categories":null,"content":" Installation paquets supplémentaires # apt-get update # apt-get install firmware-iwlwifi virtualbox-5.2\\ ttf-mscorefonts-installer easytag tuxguitar-jsa htop\\ frescobaldi gparted grsync ntfs-config chromium autofs\\ openjdk-8-jdk openjdk-8-jre gnome-tweak-tool ntfs-config \\ ntfs-3g cifs-utils geogebra-gnome arduino libmediainfo \\ libmediainfo0v5 network-manager-openvpn-gnome dirmngr \\ spotify-client spotify-client-gnome-support \\ etcher apt-transport-https etcher-electron vim \\ fonts-powerline audacity ffmpeg lame unrar rar gdebi \\ sound-juicer traceroute scala net-tools nmap \\ gnome-shell-pomodoro hplip dig dnsutils build-essential \\ linux-headers-amd64 firmware-linux-nonfree lshw ethtool \\ libsane-hpaio xsane autofs vlc ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:3:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-paquets-supplémentaires"},{"categories":null,"content":" Configuration autofsPour ceux qui ne connaissent pas , autofs est un outil permettant de monter directement des partages nfs et cicfs à l’utilisation et non au démarrage de l’ordinateur. Dans le fichier /etc/auto.master /mnt/SERV1/nfs /etc/auto.nfs --ghost, --timeout=60 /mnt/SERV1/cifs /etc/auto.SERV1.cifs --ghost, --timeout=60 /mnt/SERV2 /etc/auto.cifs --ghost, --timeout=60 ensuite insérer la configuration adéquate dans les fichiers référencés : ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#configuration-autofs"},{"categories":null,"content":" auto.cicfs data -fstype=cifs,credentials=/home/USER/.cred-file,user=littlewing,uid=1000,gid=1000 ://192.168.0.XX/REPERTOIRE Les identifiants / mots de passe sont stockés dans un fichier .cred-file stocké à la racine du répertoire utilisateur. Voici un exemple : username=user password=password Le fichier auto.SERV1.cifs reprend la même structure ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autocicfs"},{"categories":null,"content":" auto.nfs REP1 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP1 REP2 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP2 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autonfs"},{"categories":null,"content":" Installation d’atomJ’ai choisi d’installer atom via le package .deb fourni par github. Afin d’automatiser l’installation et la mise à jour, voici le script que j’ai réalisé : #!/bin/sh SETUP_ROOT=/tmp wget -O $SETUP_ROOT/atom.deb \"https://atom.io/download/deb\" echo \"Installation du paquet...\" dpkg -i $SETUP_ROOT/atom.deb echo \"Fini :)\" Ce script est placé dans le répertoire /usr/local/sbin et lancé comme suit : # upgrade-atom.sh ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:5:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-datom"},{"categories":null,"content":" Installation de FirefoxAfin d’avoir la dernière version de firefox, voici le script que j’ai réalisé: #!/bin/sh SETUP_ROOT=/tmp BIN_ROOT=/usr/local/firefox DATE=`date +%Y-%m-%d` OLD_EXE=/usr/lib/firefox-esr/firefox-esr wget -O $SETUP_ROOT/FirefoxSetup.tar.bz2 \"https://download.mozilla.org/?product=firefox-latest\u0026os=linux64\u0026lang=fr\" echo \"Extraction de l'archive...\" tar xjf $SETUP_ROOT/FirefoxSetup.tar.bz2 -C /usr/local echo \"Changement des droits utilisateur\" chown -R :users $BIN_ROOT chmod a+x $BIN_ROOT/firefox echo \"Sauvegarde de l'ancien binaire et Creation des liens symboliques\" if [ -e $OLD_EXE ] then OLD_BINARY=${OLD_EXE}_orig_${DATE} mv $OLD_EXE $OLD_BINARY fi ln -s $BIN_ROOT/firefox $OLD_EXE chmod a+x $OLD_EXE echo \"Fini :)\" ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:6:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-de-firefox"},{"categories":null,"content":" MinecraftVoila l’étape la plus importante, du moins pour mes enfants … J’ai crée le script /usr/local/bin/minecraft.sh #!/bin/bash cd /usr/local/minecraft java -Xmx1G -Xms512M -cp /usr/local/minecraft/Minecraft.jar net.minecraft.bootstrap.Bootstrap J’ai placé le JAR en question dans le répertoire /usr/local/minecraft. Enfin, j’ai crée le fichier « lanceur gnome » /usr/share/applications/minecraft.desktop [Desktop Entry] Name=Minecraft Comment= Categories=Game;BoardGame; Exec=/usr/local/bin/minecraft.sh Icon=Minecraft_Block Terminal=false Type=Application StartupNotify=true J’ai également mis une icone SVG dans le répertoire /usr/share/icons/ ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:7:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#minecraft"},{"categories":null,"content":" Optimisation du bootAprès toutes ces installations, il faut vérifier que les performances, notamment au démarrage ne sont pas trop altérées Pour avoir le détail du boot, il faut utiliser la commande systemd-analyze #systemd-analyze blame 8.113s NetworkManager-wait-online.service 2.549s apt-daily-upgrade.service 803ms networking.service 228ms colord.service 213ms dev-sda1.device 145ms systemd-timesyncd.service 128ms ModemManager.service 102ms autofs.service .... On peut également voir le chemin critique avec cette commande: #systemd-analyze critical-chain The time after the unit is active or started is printed after the \"@\" character. The time the unit takes to start is printed after the \"+\" character. graphical.target @8.944s └─multi-user.target @8.944s └─autofs.service @8.841s +102ms └─network-online.target @8.841s └─NetworkManager-wait-online.service @723ms +8.113s └─NetworkManager.service @642ms +80ms └─dbus.service @612ms └─basic.target @612ms └─paths.target @612ms └─acpid.path @610ms └─sysinit.target @608ms └─systemd-backlight@backlight:acpi_video0.service @1.042s +8ms └─system-systemd\\x2dbacklight.slice @1.042s └─system.slice @119ms └─-.slice @108ms ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#optimisation-du-boot"},{"categories":null,"content":" Désactivation des servicesPar exemple, si vous voulez désactiver le service virtualbox au démarrage # systemctl disable vboxautostart-service.service et ainsi de suite pour tous les services inutiles au démarrage ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#désactivation-des-services"},{"categories":null,"content":" Analyse du démarrage d’un servicePour analyser le démarrage d’un service, on peut utiliser la commande journalctl # journalctl -b -u NetworkManager-wait-online.service ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:9:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#analyse-du-démarrage-dun-service"},{"categories":null,"content":" ConclusionAprès toutes ces étapes, j’ai un système opérationnel. Il manque pas mal d’outils ( ex. maven, npm, intellij,…). Ces outils tiennent plus du poste de développement. ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:10:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#conclusion"},{"categories":null,"content":"Bon, mon site part en carafe. J’ai donc décidé de changer d’hébergeur.Pour faire bref, je quitte l’autre.net et je migre mon blog sur wordpress.com. Bien évidemment, je n’ai aucune sauvegarde…Je repars de zéro A bientôt! ","date":"2018-02-09","objectID":"/2018/02/09/migration-de-mon-blog/:0:0","series":null,"tags":null,"title":"Migration de mon blog","uri":"/2018/02/09/migration-de-mon-blog/#"},{"categories":null,"content":"Ce blog ( oui il y en a encore ), me sert principalement à exposer mes tests, veilles technos et plein d’autres bidouillages de ce genre. Écrire tout ça me permet de ne pas oublier ce que j’ai pu faire quelques mois/années auparavant et d’en faire profiter modestement, la communauté. J’accepte bien évidemment les remarques, les corrections (personne n’est parfait). Cependant je me réserve le droit de publier ou non un commentaire. En règle générale, je le publie sauf si c’est un gros troll. Stop C’ est bien évidemment un blog personnel et non professionnel. Je publie également des articles dans le blog technologique de mon employeur. Enfin, les opinions, si il y en a, que j’expose ici sont les miennes et en aucun cas celles de mon employeur ou d’une quelconque personne. Sur ce, bonne lecture 🙂 ","date":"2018-02-08","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#"},{"categories":null,"content":"Si vous souhaitez me contacter, n’hésitez pas à le faire via Twitter ou LinkedIn. ","date":"2018-02-08","objectID":"/contact/:0:0","series":null,"tags":null,"title":"Contact","uri":"/contact/#"}]