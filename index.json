[{"categories":null,"content":"Photo by Dino Reichmuth on Unsplash After nearly a decade of coding with Spring Boot, I decided to switch to Quarkus (and was quite late to the party) for a workshop about how to embrace the API-First approach in Java. A few years ago, I had already given it a spin. I was not entirely convinced of the value of switching. By the way, I presented in 2022 a talk about that topic with a former colleague of mine Jean-François James. We compared both of the two solutions and concluded the functionalities provided by Spring Boot \u0026 Quarkus were sightly similar. With 2-3 years having passed. I then decided to revisit Quarkus and see how it has evolved. Although there are some still missing features, I was really impressed. I will then try to sum up in this article my journey so far from a developer’s point of view. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:0:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#"},{"categories":null,"content":" 1 The contextInstead of reusing one of my existing workshops, I chose to start a new platform from the ground up. Here is the context diagram: And the container diagram: Basically, it is a simple monolithic application with a database which reaches external services: EBay. A back office called through Kafka. The code repository is available on my GitHub account. This app is not production ready (yet) I drafted and created this application as part of a workshop on API-First. It is not a production-ready. It misses many aspects such as Observability or security. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:1:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#the-context"},{"categories":null,"content":" 2 Developer experienceMy first surprise, was when I started the Quarkus Dev. After generating the project and selecting the different requirements, I ran into two main components which, in my view, significantly improve the Developer Experience (DX) and go far beyond I used to with Spring: The Dev UI The Dev Services The DevUI extensions page Usually many developers look down on Java because it is hard to setup and the integration with external services could be painful. Through these two tools, I think Quarkus found a smart answer to these worries. Once you defined your extensions such as PostgreSQL, you have automatically the corresponding dev services enabled and you can use them either in your integration tests or directly through the dev mode. Last but not least, you can browse all of these through the dev-ui. As mentioned in the Quarkus Guide. It allows you to quickly visualize all the extensions currently loaded view extension statuses and go directly to extension documentation view and change Configuration manage and visualize Continuous Testing view Dev Services information view the Build information view and stream various logs Concretely, what it means for me? I do not have to bother me again on setting up a local Docker compose environment for testing the plateform locally! Usually I had to setup and provide to developers such a tooling to enable local testing. Although Spring also provides Dev Services, I think (it is only my opinion), Quarkus brings it as a end to end solution to developers. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:2:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#developer-experience"},{"categories":null,"content":" 2.1 What about the documentation ?I usually said the documentation belongs to the deliverables. I was well surprised by the Quarkus guides. They are straightforward and help you adopt Quarkus easily. I guess the authors made a special effort on this field. For me the consequence was obvious: I really liked coding with Quarkus, it was easy to move from Spring and find the corresponding features. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:2:1","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#what-about-the-documentation-"},{"categories":null,"content":" 3 Tools \u0026 Framework integrationAs I mentioned earlier, one of the Quarkus’s strengths I pinpointed is to strongly streamline the integration of differents tools and provide a cohesive setup through its extensions. For instance, in my workshop, I sat up the application in this way: xml \u003cdependency\u003e \u003cgroupId\u003eio.github.microcks.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-microcks\u003c/artifactId\u003e \u003cversion\u003e0.2.7\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-hibernate-orm-panache\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-jdbc-postgresql\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-hibernate-orm\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.quarkus\u003c/groupId\u003e \u003cartifactId\u003equarkus-messaging-kafka\u003c/artifactId\u003e \u003c/dependency\u003e With this bunch of dependencies, Quarkus automatically sets up the corresponding dev services and the API to reach these external services (databases, kafka broker,…). ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:3:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#tools--framework-integration"},{"categories":null,"content":" 4 API-First Quarkus developmentThe purpose of my workshop was to delve into API-First. I therefore created an application built using a Code-First approach and put in practice some tools and patterns to make API-First compatible. I then used these tools: ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:4:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#api-first-quarkus-development"},{"categories":null,"content":" 4.1 OpenAPIGeneratorInstead of using the Quarkus OpenAPI Server generator, I prefered using the goold old OpenAPIGenerator Maven plugin. Why? Because it offers more customisation possibilities than the Quarkus extension. Here is how I configured it: xml \u003cplugin\u003e \u003cgroupId\u003eorg.openapitools\u003c/groupId\u003e \u003cartifactId\u003eopenapi-generator-maven-plugin\u003c/artifactId\u003e \u003cversion\u003e7.10.0\u003c/version\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003egenerate-server\u003c/id\u003e \u003cgoals\u003e \u003cgoal\u003egenerate\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003cinputSpec\u003e${project.basedir}/src/main/resources/openapi/guitarheaven-openapi.yaml \u003c/inputSpec\u003e \u003cgeneratorName\u003ejaxrs-spec\u003c/generatorName\u003e \u003cconfigOptions\u003e \u003capiPackage\u003einfo.touret.guitarheaven.application.generated.resource\u003c/apiPackage\u003e \u003cmodelPackage\u003einfo.touret.guitarheaven.application.generated.model\u003c/modelPackage\u003e \u003clibrary\u003equarkus\u003c/library\u003e \u003cdateLibrary\u003ejava8\u003c/dateLibrary\u003e \u003cgenerateBuilders\u003etrue\u003c/generateBuilders\u003e \u003copenApiNullable\u003efalse\u003c/openApiNullable\u003e \u003cuseBeanValidation\u003etrue\u003c/useBeanValidation\u003e \u003cgeneratePom\u003efalse\u003c/generatePom\u003e \u003cinterfaceOnly\u003etrue\u003c/interfaceOnly\u003e \u003clegacyDiscriminatorBehavior\u003efalse\u003c/legacyDiscriminatorBehavior\u003e \u003copenApiSpecFileLocation\u003eopenapi/openapi.yaml\u003c/openApiSpecFileLocation\u003e \u003creturnResponse\u003etrue\u003c/returnResponse\u003e \u003csourceFolder\u003e.\u003c/sourceFolder\u003e \u003cuseJakartaEe\u003etrue\u003c/useJakartaEe\u003e \u003cuseMicroProfileOpenAPIAnnotations\u003etrue\u003c/useMicroProfileOpenAPIAnnotations\u003e \u003cuseSwaggerAnnotations\u003efalse\u003c/useSwaggerAnnotations\u003e \u003cwithXml\u003efalse\u003c/withXml\u003e \u003c/configOptions\u003e \u003coutput\u003e${project.build.directory}/generated-sources/open-api-yaml\u003c/output\u003e \u003cignoreFileOverride\u003e${project.basedir}/.openapi-generator-ignore\u003c/ignoreFileOverride\u003e \u003cmodelNameSuffix\u003eDto\u003c/modelNameSuffix\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e This setup generates at the build time both the model classes and the Server API interfaces. Want to know more about Quarkus API-First development? If you want to dig into API-First development with Quarkus, you can check out my workshop. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:4:1","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#openapigenerator"},{"categories":null,"content":" 4.2 Quarkus OpenAPIGeneratorTo generate the REST API client, I then chose to use the Quarkus OpenAPI Generator extension. It is really easy to implement. Add just this extension: xml \u003cdependency\u003e \u003cgroupId\u003eio.quarkiverse.openapi.generator\u003c/groupId\u003e \u003cartifactId\u003equarkus-openapi-generator\u003c/artifactId\u003e \u003cversion\u003e2.7.1-lts\u003c/version\u003e \u003c/dependency\u003e Define then the following properties in the [src/main/resources/application.properties]: ini quarkus.openapi-generator.codegen.input-base-dir=src/main/resources/openapi-client quarkus.openapi-generator.codegen.spec.ebay_buy_openapi_yaml.base-package=info.touret.guitarheaven.infrastructure.ebay quarkus.openapi-generator.codegen.spec.ebay_buy_openapi_yaml.model-name-suffix=Dto quarkus.openapi-generator.codegen.spec.ebay_buy_openapi_yaml.use-bean-validation=true There is plenty of configuration parameters you can use. If you want to know more, you can browse the documentation. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:4:2","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#quarkus-openapigenerator"},{"categories":null,"content":" 4.3 Small RyeLike other services accessible through the DevUI, the Small Rye Swagger UI is really interesting. Once you add the Rest-Jackson extension enabled, the Swagger UI is automatically plugged to your API when you run the dev mode. Although I would prefer using Redocly instead, the way the SwaggerUI is automatically brought in this setup is enough for me to use it. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:4:3","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#small-rye"},{"categories":null,"content":" 4.4 MicrocksMicrocks offers a facility for mocking external services. With the Quarkus Microcks extension, you can use it similarly to other dev services. I won’t delve deeply into this topic because, aside from integrating it as part of the dev services, the functionalities are similar to those in Spring Boot. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:4:4","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#microcks"},{"categories":null,"content":" 5 PersistenceI must admit. I do like Spring Data and the way it abstracts and generates the JPQL queries through the interface methods naming. At the beginning of my migration journey, I thought moving to Panache would be challenging. I was wrong. Although I missed some functionalities of the Spring Data CRUD Repository, I found my footing easily. Panache offers two modes of usage: the Active Record pattern and the repository pattern. I chose the latter for two reasons: Mocking a repository is easier than mocking static methods with Mockito I prefer to keep data separate from the persistence methods. Anyway, using Panache let you using more JPQL Queries than with Spring Data. Whether is it good or bad, I am not sure. What can I say that shifting from Spring Data to Quarkus is quite straightforward for a Java Developer. What about the Quarkus Spring Data extension? There is a Quarkus Spring Data extension available. I prefered not to use it to work in a Quarkus “standard” way for persistence avoiding dependency on a external framework or compatibility layer. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:5:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#persistence"},{"categories":null,"content":" 6 Rest clientWhether you use the Spring Rest Client or Spring OpenFeign Client, you can switch to Quarkus Rest Client easily. I found the usage straightforward. You can inject it as a field in your code: java @RestClient private EbayClient ebayClient; define the client: java @RegisterRestClient @Path(\"/item_summary/search\") public interface EbayClient { // (1) @GET SearchPagedCollection searchByName(@RestQuery(\"q\") String query); //(2) @ClientExceptionMapper static RuntimeException toException(Response response) { if (response.getStatus() == 400) { return new RuntimeException(\"The remote service responded with HTTP 400\"); } // Disabling some issues with the EBAY Mock return null; } } The client is automatically generated and plugged to the remote endpoint through the URL specified in the application.properties: ini quarkus.rest-client.\"info.touret.guitarheaven.infrastructure.ebay.EbayClient\".url=${quarkus.microcks.default.http}/rest/Browse+API/v1.19.9 quarkus.rest-client.extensions-api.verify-host=false We can also customise the error management with the ClientExceptionHandler. Et voilà Most of the boiler plate code is therefore removed and you can focus on what it worths. By the way, in my workshop on API-First, I then generated the RestClient class from the OpenAPI file. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:6:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#rest-client"},{"categories":null,"content":" 7 Kafka IntegrationThe Kafka integration is also pretty straightforward. Whether you broadcast messages or fetch them, the connection layer is automatically handled by Quarkus: java @Inject @Channel(\"guitar-requests-out\") Emitter\u003cRecord\u003cUUID, GuitarRequest\u003e\u003e guitarRequestEmitter;/** * Sends message to Kafka * * @param guitarRequest : The Guitar to send to Kafka */ public void requestForNewGuitars(GuitarRequest guitarRequest) { LOGGER.info(\"Sending Guitar Request to supplier : {}\", guitarRequest.requestId().toString()); guitarRequestEmitter.send(Record.of(guitarRequest.requestId(), guitarRequest)); }/** * Fetches the kafka topic * \u003cb\u003eThis method is only for testing purpose during the workshop\u003c/b\u003e * * @param guitarRequestRecord: The Kafka record of the Guitar to send */ @Incoming(\"guitar-requests-in\") public void traceRequestsForNewGuitars(Record\u003cUUID, GuitarRequest\u003e guitarRequestRecord) { LOGGER.info(\"Received new Guitar Request: ID: {} - NAME: {} - QTY: {}\", guitarRequestRecord.key(), guitarRequestRecord.value().guitarName(), guitarRequestRecord.value().quantity()); } The configuration of the Kafka Client is then configured in the application.properties file: ini # -------------------------- ## KAFKA Client configuration # -------------------------- quarkus.kafka.devservices.topic-partitions.guitar-requests=1 mp.messaging.outgoing.guitar-requests-out.connector=smallrye-kafka mp.messaging.outgoing.guitar-requests-out.topic=guitar-requests mp.messaging.outgoing.guitar-requests-out.key.serializer=org.apache.kafka.common.serialization.UUIDSerializer mp.messaging.outgoing.guitar-requests-out.value.serializer=info.touret.guitarheaven.infrastructure.kafka.GuitarRequestSerializer mp.messaging.outgoing.guitar-requests-out.auto.offset.reset=earliest mp.messaging.incoming.guitar-requests-in.connector=smallrye-kafka mp.messaging.incoming.guitar-requests-in.topic=guitar-requests mp.messaging.incoming.guitar-requests-in.key.deserializer=org.apache.kafka.common.serialization.UUIDDeserializer mp.messaging.incoming.guitar-requests-in.value.deserializer=info.touret.guitarheaven.infrastructure.kafka.GuitarRequestDeserializer mp.messaging.incoming.guitar-requests-in.auto.offset.reset=earliest It was just a simple integration. For more information, you can check out the guide and the examples. Nevertheless, thanks to the Dev Services, we can use RedPanda in development to pop a Kafka Stack and avoid configuring a Docker compose stack to enable it during integration tests. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:7:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#kafka-integration"},{"categories":null,"content":" 8 Difficulties and some functionalities still missing (from my point of view)","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:8:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#difficulties-and-some-functionalities-still-missing-from-my-point-of-view"},{"categories":null,"content":" 8.1 TestingThe main difficulty I faced was writing and running my integration tests. Quarkus offers the @QuarkusTest facility for creating and running integration tests, and it works well. It is automatically connected to the dev services (e.g., the database), and data is automatically imported at the test startup using the JPA standard method (i.e., using the import.sql file located in the src/test/resources folder). My main concern was that this dataset and the JPA context were shared across all the integration tests. Once I deleted an item during one of my integration tests, I was unable to access it in subsequent tests. While working on my project, this issue slightly annoyed me. I had to troubleshoot why my integration tests failed, even though they had passed before (a common development challenge). I really missed the Spring Test @Sql annotation. Among other benefits, it helps me run SQL scripts whenever needed and reload my data at the beginning of my integration tests. Although it might be considered heavier, it provides more flexibility and ensures data integrity. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:8:1","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#testing"},{"categories":null,"content":" 8.2 Moving from Spring DataAs I exposed earlier, Spring Data offers many functionalities which make the development easier. It was a bit weird recoding JPQL queries for fetching data I used to do without coding with Spring Data. Anyway, I strongly believe it is just a detail. The scope of functionalities is, in my opinion, equivalent. To sum up, Spring Data mostly abstracts the persistence layer, while coding the persistence layer with Quarkus involves more direct use of JPA and Hibernate (even though Panache strongly simplifies the process). ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:8:2","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#moving-from-spring-data"},{"categories":null,"content":" 9 ConclusionI will stop this comparison here. I haven’t explored much so far, but after spending some hours coding, I’m really pleased with the effort the Quarkus community has made to enhance the developer experience. With full support of the Microprofile specifications and its various API or facilities, Quarkus allows you to streamline your development, and write code that is straightforward and more stable over time. One interesting point still missing is about Security. Although there is a Quarkus OpenId Connect integration, I don’t know yet what is the gap between Spring Security and it. Anyway, I may have missed some points in my review. If so, feel free to reach out to me. 2-3 years ago, when people asked about moving to Quarkus, I didn’t see much interest. However, if I had to start a greenfield project today, it would be now my first choice. ","date":"2025-01-22","objectID":"/2025/01/22/moving-from-spring-to-quarkus/:9:0","series":null,"tags":["quarkus","spring","java"],"title":"Moving from Spring Boot to Quarkus","uri":"/2025/01/22/moving-from-spring-to-quarkus/#conclusion"},{"categories":null,"content":" 1 2024 in a NutshellAs 2024 draws to a close, it’s time to cast a professional eye back on this year. Like last year, I have balanced working on a customer project alongside my contributions to the Worldline TechRel1 initiative. I had the opportunity to shift to another team. Although I keep working on the Transport \u0026 Mobility field, I dug into a new product \u0026 met new colleagues. After few months, as part of the agreement with Google, I then dived into the Google Cloud techniology portfolio and passed the Public Cloud Architect certification. In addition, I also started teaching at the University of Tours. I strive to provide my return of experience on the following topics: Microservices Architecture, SQL Basics \u0026 Business Processes. The courses were published under the Creative Commons International Attribution 4.0 International License. Feel free to (re-)use them and submit issues. I am quite sure I made somes mistakes. ","date":"2024-12-19","objectID":"/2024/12/19/2024-wrap-up/:1:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2024","uri":"/2024/12/19/2024-wrap-up/#2024-in-a-nutshell"},{"categories":null,"content":" 2 SpeakingI had several speaking engagements times this year. Here are some highlights of my appearances: 3 international tech conferences : NDC London, Geecon Krakow, Devoxx Morocco 4 French tech conferences: Touraine Tech, Riviera Dev, Volcamp, API Days Paris 2 local meetups: Bordeaux JUG, Poitou JUG A Worldline tech event in Seclin (France) Like last year, I found myself presenting up to three talks, or workshops at the same event. As I said last year, I extended the parterships with other speakers I then co-presented most of my 2024 talks with my Wordline colleagues Raphaël Semeteys, Philippe Duval \u0026 David Pequegnot. I definitevely enjoy sharing the stage with co-speakers. Although the preparation can be tricky due to the organisation (busy agenda versus online preparation meetings), I think the content provided is usually better and I learn a lot new things during the preparation. Here are some topics I shared in 2024: Implementing API-First Approach: Practical Insights for Streamlining Your APIs Intégrons, faisons grandir et progresser les jeunes devs: Bonnes pratiques et retours d’expériences à l’intention des (vieux) devs. L’observabilité dès le développement: Maîtrisez vos applications Java en production Let’s Learn to Identify Technical Requirements for Better Design I also participated in the JChateau Unconference. Interacting again with inspiring individuals like Jean-Michel Doudoux, Andres Almirez, or José Paumard confirmed I really enjoy this format. You learn more in 2-3 days there than in any other conference. I will repeat myself, but I express a new time my gratitude to my employer for the continued opportunities and to the organizers for their unwavering trust and hospitality. The Worldline TechRel initiative has been instrumental in exploring new topics and facilitating collaborations with colleagues. It has enabled me to share, gather feedback on submissions, and conduct live rehearsals (Thanks Yassine Benabbas, Philippe Vincent and the others). ","date":"2024-12-19","objectID":"/2024/12/19/2024-wrap-up/:2:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2024","uri":"/2024/12/19/2024-wrap-up/#speaking"},{"categories":null,"content":" 3 ArticlesDuring the year, I released five articles on my blog and one on the Worldline engineering blog. Additionally, I published an article into the French tech magazine Programmez (#266). I described how to implement observability in a Java application with OpenTelemetry. This last experience was enriching. Writing an article for a magazine is complex and requires more rigor than writing a blog post. Like for my talks, I leant of my colleagues for reviewing and giving feedbacks (Thanks Jean-Michel Girard \u0026 the others). ","date":"2024-12-19","objectID":"/2024/12/19/2024-wrap-up/:3:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2024","uri":"/2024/12/19/2024-wrap-up/#articles"},{"categories":null,"content":" 4 What’ next?I will start 2025 off to a flying start. I will present in late January two talks and will lead two workshops: two at SnowcampIO (Including one with David Pequegnot) and two at Touraine Tech. I don’t know yet whether I will more focus on teaching than in speaking at tech conferences. Time will tell. Anyway, wishing everyone merry christmas, a fantastic New Year’s Eve, and a Happy New Year! It’s akin to Dev Rel but aims to include a broader tech community encompassing OPS, SRE, etc. ↩︎ ","date":"2024-12-19","objectID":"/2024/12/19/2024-wrap-up/:4:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2024","uri":"/2024/12/19/2024-wrap-up/#what-next"},{"categories":null,"content":" Teaching Photo by Element5 Digital on Unsplash From 2024, I have been teaching at the University de Tours and PolyTech Tours. I published the following courses (in French) under the Creative Commons Attribution 4.0 International license: Modélisation des processus métier Introduction aux architectures microservices The sources are available on my GitHub account. Feel free to use and share them! ","date":"2024-11-08","objectID":"/teaching/:0:0","series":null,"tags":null,"title":"Teaching","uri":"/teaching/#"},{"categories":null,"content":" 1 IntroductionOn behalf of the partnership of my current company with Google,I had the opportunity to delve into Google technologies and pass Google’s Professional Cloud Architect certification. Below, you will find a brief overview of my certification journey and some best practices to successfully pass this certification. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:1:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#introduction"},{"categories":null,"content":" 2 PrerequisitesGoogle recommends having at least 3 years of industry experience and at least 1 year designing and managing solutions on GCP. In my view, this last point is worth considering carefully. If you are not used to coding, integrating, and managing cloud platforms, it will be hard to learn and become “fluent” with the entire Google Cloud stack. One of the most important parts of the certification is Kubernetes and GKE (Google Kubernetes Engine). You must be proficient with these technologies before preparing for the certification. For instance, although I hadn’t worked much with Google technologies recently, I had a strong background in development (Java, CI/CD, etc.) and containerized technologies (Docker, Kubernetes, and OpenShift). This background helped me learn faster and focus on other parts such as security or data analysis. Finally, another capability that could be interesting is having a clear overview of the entire software development process and production. The agenda of this certification is not only technical. You will have to learn some basics about security, project management, production management, compliance, and so on. If you want to know what topics you must learn to prepare for this certification, you can refer to this spreadsheet. As you can see, to pass this certification, it’s not necessary to be an expert in all Google technologies, but you must have a clear overview of the whole. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:2:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#prerequisites"},{"categories":null,"content":" 3 How I handled itI took part of a learning program based on Google Skills Boot Cloud Architect learning path. The pace is quite exhausting. I had to watch many videos and practice the hands-on labs. Over the course of 9 weeks, I dedicated at least 2 days each week to learning and practicing (not including the time spent reading the books). They were really well structurised. Besides, there were challenge labs that offered the opportunity to check if we understood all the concepts. No indications were given. Beyond that, for practicing all the command line tools, I chose to use Cloud Shell exclusively. I then bought two books to compensate my lack of knownledge of the Google technologies : Google Cloud Certified Professional Cloud Architect Study Guide, 2nd Edition Visualizing Google Cloud: 101 Illustrated References for Cloud Engineers \u0026 Architects These books are really insightful. They helped me get a glance at the entire Google Cloud Platform. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:3:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#how-i-handled-it"},{"categories":null,"content":" 4 How to practiceBesides the skills boost learning program and the books, it’s crucial to practice the certification exam on a regular basis with quizzes. It will help you know if you missed some points and delve into them if needed. I rehearsed every two days the last days before passing the exam! I used the following quizzes/websites: Professional Cloud Architect Sample Questions Pre-exam quiz PCA - Pre-exam Quiz 2 ExamTopics Whizzlabs The Wiley Quizzes tip The Whizzlabs are far better than the other ones. If it’s affordable for you, I recommend buying the practice exams. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:4:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#how-to-practice"},{"categories":null,"content":" 5 DifficultiesOne of the main interesting point of this certification is it covers more than just the Google Cloud Platform. Through the learning path, you will learn many things about the Software Development LifeCycle, the NFR, the regulation and much more. You could find below the main chapters: Section 1: Designing and planning a cloud solution architecture (~24% of the exam) Section 2: Managing and provisioning a solution infrastructure (~15% of the exam) Section 3: Designing for security and compliance (~18% of the exam) Section 4: Analyzing and optimizing technical and business processes (~18% of the exam) Section 5: Managing implementation (~11% of the exam) Section 6: Ensuring solution and operations reliability (~14% of the exam) As you can see, it covers much more than just the Google products. By the way, it’s really hard to know which technology is the most relevant for which use case (e.g., when to use dataprep or Firestore?). The “Visualizing Google Cloud” book helped me a lot for that purpose. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:5:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#difficulties"},{"categories":null,"content":" 6 TipsMy first advice is to schedule your learning path over months or weeks. It will take a long time. Then, it’s important to repeat quizzes and exams regularly. It helped me a lot to pinpoint the notions and topics I missed. Finally, don’t hesitate to prepare for this certification and share this journey with others. It will undoubtedly accelerate your learning path. I had the opportunity to share a lot with some of my colleagues who passed the same certification. It helped us share tips, tricks, and delve into specific topics I missed. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:6:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#tips"},{"categories":null,"content":" 7 The D DAYAfter this odyssey, when you eventually take the final exam, it’s really important to be “physically” ready. Try to sleep the night before and be in good shape. The exam lasts 2 hours, and you will need the entire time. Personally, I submitted my answers 1 minute before the end. Finally, some of the questions are based on case studies that Google has already published. It is recommended to read these case studies before the exam. If you can study and design these platforms in advance, it will be even better, as it will save you time during the exam. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:7:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#the-d-day"},{"categories":null,"content":" 8 ConclusionThis journey was really interesting (exhausting but insteresting). After passing successfully the exam, I think I learned a lot during this certification preparation and improved my skills in software architecture. If you are eager to pass this certification exam, I hope this article helps you. ","date":"2024-08-02","objectID":"/2024/08/02/gcp_certification/:8:0","series":null,"tags":["GCP"],"title":"My Google Professional Cloud Architect certification journey: insights \u0026 pieces of advice","uri":"/2024/08/02/gcp_certification/#conclusion"},{"categories":null,"content":"Photo by Firdouss Ross ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:0:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#"},{"categories":null,"content":" 1 IntroductionWhile fixing the author and email properties for a bunch of existing commits on different repositories, I realised I often forget configuring the good information for both my professional \u0026 personal GIT repositories. I particularly skip to specify the good email address of the good GPG signature after checking them out. I therefore looked around for an industrialised (~lazy) solution for fixing this issue once for all. I found a solution which fits my needs: the includeIf instruction. Among other things, this functionality helps me centralise my Git configuration and apply the good parameters/instructions (e.g., user.email) dynamically in every repository. You could find below how I did that: ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:1:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#introduction"},{"categories":null,"content":" 2 PrerequisitesHaving Git \u003e=2.36 installed. In case you use the current Ubuntu LTS or Linux Mint release, you can install the latest version of GIT by adding this repository into your APT sources: You can find below the commands to run bash sudo add-apt-repository ppa:git-core/ppa -y sudo apt update \u0026\u0026 sudo apt full-upgrade -y After that, you can check the version of GIT running this command: bash git --version git version 2.43.2 ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:2:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#prerequisites"},{"categories":null,"content":" 3 The configurationIn this article, we will assume to have two different SCMs with two different SSH URLs: The corporate GIT SCM: git@gitlab.INTERNAL_URL Github.com: git@github.com By the way, you can also use HTTPS URls. Now let’s configure our main git configuration (e.g. $HOME/.gitconfig) ini [init] defaultBranch = main [pull] rebase = merges [core] autocrlf = true [includeIf \"hasconfig:remote.*.url:git@gitlab.INTERNAL_URL:*/**\"] path = ~/.gitconfig-corporate [includeIf \"hasconfig:remote.*.url:git@github.com:*/**\"] path = ~/.gitconfig-github As mentioned earlier, I use in this configuration the includeIf functionality for checking the remote URL. Regarding the remote URL (it also works with HTTPS URLs), either the corporate or GitHub configuration will be applied on the fly while applying git commands in my repos. I then configured my corporate git configuration as following: ini [user] name = Alexandre Touret email = alexandre.touret@XXX.com [credential] username = XXXX helper = cache and the GitHub configuration ini [user] email = alexandre-touret@users.noreply.github.com signingkey = XXXXXXXXX name = Alexandre Touret [credential] username = XXXX helper = cache [gpg] program = gpg [commit] gpgsign = true ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:3:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#the-configuration"},{"categories":null,"content":" 4 Gentle reminder: how to use GitHub SSH connection without using Standard 22 portHopefully, GitHub has enabled SSH connections through the standard HTTPS port. You can then set up your SSH client adding this configuration in the $HOME/.ssh/config: ini Host github.com Hostname ssh.github.com Port 443 User git IdentityFile ~/.ssh/id_rsa_github If you want to knwo more about GitHub SSH configuration, check out the documentation ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:4:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#gentle-reminder-how-to-use-github-ssh-connection-without-using-standard-22-port"},{"categories":null,"content":" 5 How to check it?In one Git repository, you can check this configuration typing the command: Here an example for a GitHub repository bash \u003e git config --get user.email alexandre-touret@users.noreply.github.com ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:5:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#how-to-check-it"},{"categories":null,"content":" 6 ConclusionIn this short article I aimed to explain how to smoothly handle two (or more) Git profiles in the same development environment. I chose to use the remote URL to segregate each one. This configuration can finally help you apply automatically the good Git information while committing your work without setting up each project individually. Hope this helps! ","date":"2024-04-02","objectID":"/2024/04/02/git_profiles_at_once/:6:0","series":null,"tags":["Git"],"title":"Smartly Managing Different Git Profiles","uri":"/2024/04/02/git_profiles_at_once/#conclusion"},{"categories":null,"content":"Picture of Tobias Fischer When you code enterprise applications on top of the Java Platform, most of the time, you use ORMs to interface them with relational databases. They bring a lot of simplicity which make you forget SQL queries syntax. Furthermore, most of the time, Java developers don’t really care/know what is under the hood of Spring Data and Java Persistence API (JPA) or such a facility. In my opinion, it’s mainly due to all the features provided by these specifications and frameworks. Unfortunately, when your dataset is coming to grow, querying against your database could be difficult. Among other things, the different queries run by your Java application may potentially break your SLOs. In this article, I have tried to write down a bunch of tips \u0026 tricks to tackle this issue. Even if some are related to Spring Data, I think you can use most of them if you use JPA in a standard way. You will see that even if we can consider using JPA easy at first glance, it can bring a lot of complexity. Acknowledgement I would like to thank my colleagues Max Beckers, David Pequegnot \u0026 Peter Steiner for reviewing my article and giving their advices, useful links \u0026 tips. Thanks also to Alexis Hassler for the tip about Hibernate 6 logging. About the code snippets All the code snippets shown in this article come from this GitHub repository. Feel free to use it! ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:0:0","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#"},{"categories":null,"content":" 1 Observe your application","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:1:0","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#observe-your-application"},{"categories":null,"content":" 1.1 Observe your persistence layerFirst and foremost, you MUST trace and monitor your persistence layer usage. If you use Hibernate (without Spring, Quarkus,…), you can get useful information configuring the logger: xml \u003clogger name=\"org.hibernate.SQL\"\u003e \u003clevel value=\"debug\"/\u003e \u003c/logger\u003e If you use Spring (and JPA, Hibernate), you can also get them adding these configuration properties: ini logging.level.org.hibernate.stat=TRACE logging.level.org.hibernate.SQL=DEBUG logging.level.org.hibernate.orm.jdbc.bind=TRACE logging.level.org.hibernate.SQL_SLOW=TRACE spring.jpa.properties.hibernate.generate_statistics=true spring.jpa.properties.hibernate.format_sql=false What about JPA configuration? If you want to get the same Hibernate configuration, you can read this article. After getting all the queries and operations done by your persistence layer, you will be able to pinpoint which component is responsible for slowing down your queries. To cut long story short, which one is guilty? The database or the ORM. In the case of huge SQL queries, I usually execute them directly in SQL using the database tools to check if I have the same behaviour. Example of such an output: jshelllanguage 2024-03-21T22:14:46.853+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.store_id,b1_0.title,b1_0.year_of_publication from book b1_0 2024-03-21T22:14:46.875+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select s1_0.id,s1_0.name,b1_0.store_id,b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.title,b1_0.year_of_publication from store s1_0 left join book b1_0 on s1_0.id=b1_0.store_id where s1_0.id=? 2024-03-21T22:14:46.897+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.900+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.902+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.904+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.906+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.908+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.909+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.911+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.boo","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:1:1","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#observe-your-persistence-layer"},{"categories":null,"content":" 1.1 Observe your persistence layerFirst and foremost, you MUST trace and monitor your persistence layer usage. If you use Hibernate (without Spring, Quarkus,…), you can get useful information configuring the logger: xml If you use Spring (and JPA, Hibernate), you can also get them adding these configuration properties: ini logging.level.org.hibernate.stat=TRACE logging.level.org.hibernate.SQL=DEBUG logging.level.org.hibernate.orm.jdbc.bind=TRACE logging.level.org.hibernate.SQL_SLOW=TRACE spring.jpa.properties.hibernate.generate_statistics=true spring.jpa.properties.hibernate.format_sql=false What about JPA configuration? If you want to get the same Hibernate configuration, you can read this article. After getting all the queries and operations done by your persistence layer, you will be able to pinpoint which component is responsible for slowing down your queries. To cut long story short, which one is guilty? The database or the ORM. In the case of huge SQL queries, I usually execute them directly in SQL using the database tools to check if I have the same behaviour. Example of such an output: jshelllanguage 2024-03-21T22:14:46.853+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.store_id,b1_0.title,b1_0.year_of_publication from book b1_0 2024-03-21T22:14:46.875+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select s1_0.id,s1_0.name,b1_0.store_id,b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.title,b1_0.year_of_publication from store s1_0 left join book b1_0 on s1_0.id=b1_0.store_id where s1_0.id=? 2024-03-21T22:14:46.897+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.900+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.902+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.904+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.906+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.908+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.909+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-21T22:14:46.911+01:00 DEBUG 39814 --- [optimization-jpa] [nio-8080-exec-1] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.boo","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:1:1","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#dig-into-you-datasource-connection-pool-configuration"},{"categories":null,"content":" 1.2 Observe your databaseAs Java developers, we usually forget that database platforms provide valuable tools to analyse your queries. Once you have pointed out the time/resource consuming queries, you must check if your database query is time-consuming by, for instance, running a full scan of your table. In this purpose, you can check the SQL queries execution plan. If you use PostgreSQL (what else), you can get these insights using the EXPLAIN command. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:1:2","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#observe-your-database"},{"categories":null,"content":" 2 Check your entities associationsLet’s go back to our Java application. One of the main points of attention of any JPA (and SQL) queries is how your entity is joined with others. Every jointure brings costs and complexity. For JPA queries, you must check first if your relationship between two objects should be either EAGER or LAZY. You probably understood: there is no free lunch. You must measure first the JPA queries and mapping time-consumption and check which solution is the best. By default, EAGER relationship are set up for @ManyToOne and @OneToOne. LAZY are for @OneToMany. Most of the time, I keep using the default configuration. However, you must take care of the whole entity graph loaded by your query. Does your entity loaded by a @OneToOne relationship loads also a @OneToMany relationship in a EAGER way? It’s the kind of question you will need to answer. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:0","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#check-your-entities-associations"},{"categories":null,"content":" 2.1 The famous N+1 issueIn this example, we will look into a 1-N relationship: java @Entity public class Store{ [...] @OneToMany(fetch = FetchType.EAGER,mappedBy = \"store\") private List\u003cBook\u003e books; [...] java @Entity public class Book { [...] @ManyToOne(targetEntity = Store.class) private Store store; [...] If you remember well, this relationship is fetched in a EAGER way. When I try to get all the stores using a findAll() method: For example: java public List\u003cStore\u003e findAllStores() { return storeRepository.findStores().stream().toList(); } Hibernate will query the database in this way: 1 query to select the main entity N queries for the entities linked by the jointure In our case, we can see the following queries in the logs: shell Hibernate: select s1_0.id,s1_0.name from store s1_0 Hibernate: select b1_0.store_id,b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.title,b1_0.year_of_publication from book b1_0 where b1_0.store_id=? Hibernate: select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? Hibernate: select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? Hibernate: select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? Hibernate: select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? [...] It’s unfortunately not finished yet. Imagine now, your book entity is related to another one in a EAGER way. java @ManyToMany(fetch = FetchType.EAGER) private List\u003cAuthor\u003e authors; You will execute then another SQL queries. For instance, in this case: jshelllanguage Hibernate: select b1_0.authors_id,b1_1.id,b1_1.description,b1_1.isbn_10,b1_1.isbn_13,b1_1.medium_image_url,b1_1.nb_of_pages,b1_1.price,b1_1.rank,b1_1.small_image_url,s1_0.id,s1_0.name,b1_1.title,b1_1.year_of_publication from book_authors b1_0 join book b1_1 on b1_1.id=b1_0.books_id left join store s1_0 on s1_0.id=b1_1.store_id where b1_0.authors_id=? To sum up At the same way SQL jointures are really time-consuming, the way you can link entities may strongly impact the performance in either memory or while running SQL queries against our database. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:1","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#the-famous-n1-issue"},{"categories":null,"content":" 2.2 Use a dedicated entity graphIf you are still struggling with the way Hibernate loads your Entity graph, you can also try to specify the graph of entities to load by yourself. This feature introduced in JPA 2.1 can help you avoid retrieving specific useless attributes or optimise the loading of the linked entities. Let’s go back to our application. Imagine that in one use case, when we fetch a list of books, we don’t need the list of authors. Using this API we can avoid fetching it in this way java @Entity @NamedEntityGraph(name = \"store[books]\", attributeNodes = @NamedAttributeNode(\"books\") ) public class Store implements Serializable { [...] java @Repository public interface StoreRepository extends JpaRepository\u003cStore,Long\u003e { @EntityGraph(value = \"store[books]\") Optional\u003cStore\u003e findByName(String name); } You will get the following output: jshelllanguage 2024-03-22T14:35:17.515+01:00 DEBUG 74072 --- [optimization-jpa] [nio-8080-exec-3] org.hibernate.SQL : select s1_0.id,b1_0.store_id,b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.title,b1_0.year_of_publication,s1_0.name from store s1_0 left join book b1_0 on s1_0.id=b1_0.store_id where s1_0.name=? 2024-03-22T14:35:17.537+01:00 DEBUG 74072 --- [optimization-jpa] [nio-8080-exec-3] o.h.stat.internal.StatisticsImpl : HHH000117: HQL: [CRITERIA] select s1_0.id,b1_0.store_id,b1_0.id,b1_0.description,b1_0.isbn_10,b1_0.isbn_13,b1_0.medium_image_url,b1_0.nb_of_pages,b1_0.price,b1_0.rank,b1_0.small_image_url,b1_0.title,b1_0.year_of_publication,s1_0.name from store s1_0 left join book b1_0 on s1_0.id=b1_0.store_id where s1_0.name=?, time: 21ms, rows: 1 2024-03-22T14:35:17.559+01:00 DEBUG 74072 --- [optimization-jpa] [nio-8080-exec-3] org.hibernate.SQL : select a1_0.books_id,a1_1.id,a1_1.firstname,a1_1.lastname,a1_1.public_id from book_authors a1_0 join author a1_1 on a1_1.id=a1_0.authors_id where a1_0.books_id=? 2024-03-22T14:35:17.565+01:00 DEBUG 74072 --- [optimization-jpa] [nio-8080-exec-3] org.hibernate.SQL : select b1_0.authors_id,b1_1.id,b1_1.description,b1_1.isbn_10,b1_1.isbn_13,b1_1.medium_image_url,b1_1.nb_of_pages,b1_1.price,b1_1.rank,b1_1.small_image_url,s1_0.id,s1_0.name,b1_1.title,b1_1.year_of_publication from book_authors b1_0 join book b1_1 on b1_1.id=b1_0.books_id left join store s1_0 on s1_0.id=b1_1.store_id where b1_0.authors_id=? 2024-03-22T14:35:17.582+01:00 WARN 74072 --- [optimization-jpa] [nio-8080-exec-3] .w.s.m.s.DefaultHandlerExceptionResolver : Ignoring exception, response committed already: org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Infinite recursion (StackOverflowError) 2024-03-22T14:35:17.583+01:00 WARN 74072 --- [optimization-jpa] [nio-8080-exec-3] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Infinite recursion (StackOverflowError)] 2024-03-22T14:35:17.583+01:00 INFO 74072 --- [optimization-jpa] [nio-8080-exec-3] i.StatisticalLoggingSessionEventListener : Session Metrics { 571600 nanoseconds spent acquiring 1 JDBC connections; 0 nanoseconds spent releasing 0 JDBC connections; 954800 nanoseconds spent preparing 3 JDBC statements; 2433201 nanoseconds spent executing 3 JDBC statements; 0 nanoseconds spent executing 0 JDBC batches; 0 nanoseconds spent performing 0 L2C puts; 0 nanoseconds spent performing 0 L2C hits; 0 nanoseconds spent performing 0 L2C misses; 0 nanoseconds spent executing 0 flushes (flushing a total of 0 entities and 0 collections); 0 nanoseconds spent executing 0 partial-flushes (flushing a total of 0 entities and 0 collections) } ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:2","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#use-a-dedicated-entity-graph"},{"categories":null,"content":" 2.3 Create a dedicated entity to reduce the number of attributesOne another misconception about JPA is to always fully map all the properties of table to its corresponding entity! A gentle reminder: we don’t need to map all the columns in an entity! For instance, if your table has 30 columns, and you only need 10 in your use case, why querying, fetching and storing in memory all of these data? That’s why I usually recommend to have, when it’s relevant, a dedicated entity for specific use cases. It could be lighter than the regular one and enhance the performances of your application. For instance, if we have a regular Book entity: java @Entity public class Book implements Serializable { @NotNull private String title; @Column(name = \"isbn_13\") private String isbn13; @Column(name = \"isbn_10\") private String isbn10; @ManyToMany(fetch = FetchType.EAGER, cascade = {CascadeType.PERSIST, CascadeType.MERGE, CascadeType.REFRESH, CascadeType.DETACH}) private List\u003cAuthor\u003e authors; @Column(name = \"year_of_publication\") private Integer yearOfPublication; @Column(name = \"nb_of_pages\") private Integer nbOfPages; @Min(1) @Max(10) private Integer rank; private BigDecimal price; @Column(name = \"small_image_url\") private URL smallImageUrl; @Column(name = \"medium_image_url\") private URL mediumImageUrl; @Column(length = 10000) private String description; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @ManyToOne(targetEntity = Store.class) private Store store; @Transient private transient String excerpt; [...] We can shrink it with only the required attributes: java @Entity public class Book implements Serializable { @NotNull private String title; @Column(name = \"isbn_13\") private String isbn13; @Column(name = \"isbn_10\") private String isbn10; @Column(name = \"year_of_publication\") private Integer yearOfPublication; @Column(name = \"nb_of_pages\") private Integer nbOfPages; @Min(1) @Max(10) private Integer rank; private BigDecimal price; private String description; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; @Transient private transient String excerpt; Think about data consistency Think about the whole data consistency or your data stored in the database! Be aware about it when you omit specific jointures or columns. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:3","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#create-a-dedicated-entity-to-reduce-the-number-of-attributes"},{"categories":null,"content":" 2.4 Use JOIN FETCH in your queriesNow, one another strategy is to manually control/declare the jointures and how different entities would be fetched by your queries. To do that, you can use the JOIN FETCH instruction: For example: java @Query(value = \"from Store store JOIN FETCH store.books books\") Set\u003cStore\u003e findStores(); In this manner, you can shrink the number of queries done from N+1 to only one. However, you MUST check and measure if it’s worth it. Sometimes, this kind of query can be more time-consuming in either database or in the JVM than several small ones. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:4","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#use-join-fetch-in-your-queries"},{"categories":null,"content":" 2.5 Use a DTO or a tupleImagine we have a screen with of list of data coming from several entities. Instead of fetching all of these, and struggling with fetching strategies, we can also run DTO (or tuple) projections. You can therefore select all (and only) the data you need with only ONE query. To get your code even clearer, you can also use records to make your data immutable. java public record BookDto (Long id, String description) { } You can get a set of this record writing the query: java @Query(value = \"select new info.touret.query.optimizationjpa.BookDto(b.id, b.description) from Book b\") Set\u003cBookDto\u003e findAllDto(); ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:2:5","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#dto"},{"categories":null,"content":" 3 Other optimisation tracks","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:3:0","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#other-optimisation-tracks"},{"categories":null,"content":" 3.1 Avoid transactions while reading our database with the annotation @Transactional(readonly=true)One thing we often (again) remember: read-only database operations don’t need transactions! In the good old days, it was already a good practice to set up two different datasources for the persistence context: one read-only avoiding database transactions and one which allowed it. Anyway, you can now declare your service only reads data and doesn’t need to open a database transaction using the @Transactional(readonly=true) annotation. By the way, this feature goes well with using dedicated entities as mentioned above. For a specific search/query use case, you can use both of them to make your code even more straightforward. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:3:1","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#avoid-transactions-while-reading-our-database-with-the-annotation-transactionalreadonlytrue"},{"categories":null,"content":" 3.2 Pagination w/ Spring DataWhen you browse a large dataset, it’s usually a good practice to paginate results. The good news when you use Spring Data, is you have all the features included by default. The bad news is you may have time/cpu-consuming queries run for calculating the number of elements, pages and the position of the current result’s page. If getting the number of pages is useless for you, you can switch to Slices instead of Pages. When using this feature, you will only know if there is another slice available onwards or backwards through the methods hasNext() and hasPrevious(). You will find below good links talking about it on StackOverflow: https://stackoverflow.com/questions/49918979/page-vs-slice-when-to-use-which https://stackoverflow.com/questions/12644749/way-to-disable-count-query-from-pagerequest-for-getting-total-pages ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:3:2","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#pagination-w-spring-data"},{"categories":null,"content":" 3.3 Caching specific dataYou may use and query specific which is not daily (or monthly) updated. For instance, the department, country tables. In this case, you may want to cache them in memory (i.e., Second-Level cache. With JPA you can quickly cache specific entities using the @Cacheable annotation. For instance, in a Spring Boot application, you must configure your cache first: java @SpringBootApplication @EnableCaching public class OptimizationJpaApplication { public static void main(String[] args) { SpringApplication.run(OptimizationJpaApplication.class, args); } @Bean public Caffeine caffeineConfig() { return Caffeine.newBuilder().expireAfterWrite(1, TimeUnit.DAYS); } @Bean public CacheManager cacheManager(Caffeine caffeine) { CaffeineCacheManager caffeineCacheManager = new CaffeineCacheManager(); caffeineCacheManager.setCaffeine(caffeine); return caffeineCacheManager; } } And declare your methods which use your cache: java @Query(value = \"from Store store JOIN FETCH store.books books\") @Cacheable(\"stores\") Set\u003cStore\u003e findStores(); You will use your cache then and get the following logs after the second try: jshelllanguage 2024-03-23T22:01:41.299+01:00 WARN 65315 --- [optimization-jpa] [nio-8080-exec-3] .w.s.m.s.DefaultHandlerExceptionResolver : Ignoring exception, response committed already: org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Infinite recursion (StackOverflowError) 2024-03-23T22:01:41.300+01:00 WARN 65315 --- [optimization-jpa] [nio-8080-exec-3] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: Infinite recursion (StackOverflowError)] 2024-03-23T22:01:41.300+01:00 INFO 65315 --- [optimization-jpa] [nio-8080-exec-3] i.StatisticalLoggingSessionEventListener : Session Metrics { 0 nanoseconds spent acquiring 0 JDBC connections; 0 nanoseconds spent releasing 0 JDBC connections; 0 nanoseconds spent preparing 0 JDBC statements; 0 nanoseconds spent executing 0 JDBC statements; 0 nanoseconds spent executing 0 JDBC batches; 0 nanoseconds spent performing 0 L2C puts; 0 nanoseconds spent performing 0 L2C hits; 0 nanoseconds spent performing 0 L2C misses; 0 nanoseconds spent executing 0 flushes (flushing a total of 0 entities and 0 collections); 0 nanoseconds spent executing 0 partial-flushes (flushing a total of 0 entities and 0 collections) } If you want to dig into the differences between Spring cache support \u0026 the JSR 107, you can check out this documentation. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:3:3","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#caching-specific-data"},{"categories":null,"content":" 3.4 In case of emergency: break the glass!OK, none of all the tips exposed in this article has worked? Now, remember that, at the end of the day, you use a database. It comes with many tools which may run your queries at lightning speed. You can use SQL views or SQL materialized views to specify the data you want to fetch. In addition, feel free to use Native queries , Named Native Queries or Stored Procedure Queries (ONLY FOR) for the 10-20% of your most time-consuming queries. At the end of the day, you won’t be faster using an ORM! For instance, when you use a SQL view, you can, with no effort, run either a native query or fetch a DTO or a tuple (see above): Here is a trivial example to illustrate it: java @Query(value=\"select * from Store s where s.name= :name\", nativeQuery=true) Optional\u003cStore\u003e findByName(String name); ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:3:4","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#native"},{"categories":null,"content":" 4 ConclusionIf you reached this last chapter, you would see there are plenty of solutions to fix ORM/JPA performance issues. I aimed at a summary of the most efficient solutions for type of problem. As a matter of fact, I’m pretty sure there are other ones. Anyway, the first thing to put in place, is the whole observability stack: through logging, traces or prometheus metrics you will get deep insights of your application. Check also your database to see if you have a “full table scan” when you run specific SQL queries. It will help you find where is the bottleneck. Last but not least, don’t rush into such optimisations (e.g., native queries)! Observe your application first to figure out if it’s worth it. Don’t forget that any Premature optimisation is the root of all evil! Just in case... After reading this article, if you’ve seen any errors/issues or tip I missed, feel free to submit an issue. ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:4:0","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#conclusion"},{"categories":null,"content":" 4.1 Further reading https://jakarta.ee/learn/docs/jakartaee-tutorial/current/persist/persistence-intro/persistence-intro.html https://spring.io/projects/spring-data https://blog.ippon.tech/boost-the-performance-of-your-spring-data-jpa-application https://thorben-janssen.com https://vladmihalcea.com ","date":"2024-03-25","objectID":"/2024/03/25/jpa_spring_data_query_optimisations/:4:1","series":null,"tags":["Java","JPA","Spring_Data"],"title":"Tips \u0026 tricks for optimising Spring Data \u0026 JPA queries","uri":"/2024/03/25/jpa_spring_data_query_optimisations/#further-reading"},{"categories":null,"content":"Picture of Prateek Katyal Early in my career, when the documentation was not so spread across the Internet, I had to buy and crack open books to learn new things. I stumbled upon of Le Monde en “Tique”, a famous Parisian bookstore getting books of Java EE, XML, or whatever else. I then realised, even with the number of documentations, articles, videos growing on Internet, I learnt far better new topics reading books. All through my career I therefore tried to keep this habit. During the last Touraine Tech edition, I had a chat with one of my ex-colleagues about that. He asked me: “Do you have a list of the best books you read and enjoyed? It would be nice if not” I will therefore try to reply here to this query. You will find below TEN books I found really helpful at different steps of my career. About this list It was quite hard to draw up this list which looks like to “if you had been on a desert island, which book you would have brought”. Please, remember then this list is not exhaustive and there are plenty of interesting books beyond this list. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:0:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#"},{"categories":null,"content":" 1 The stapleHere are few books I strongly recommend to read and read again to every developer in this particular order. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:1:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#the-staple"},{"categories":null,"content":" 1.1 Clean code by R. C. MARTIN Clean Code For few years, when I must mentor a young developer, I ask him/her to read Clean code first and code then. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:1:1","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#clean-code-by-r-c-martin"},{"categories":null,"content":" 1.2 Refactoring by M. FOWLER Refactoring I must admit, I am a big fan of M. FOWLER. Every of his presentations, articles or books are really informative and inspiring. In my opinion, this book belongs to the mandatory reading list for every person who wants to become better in this field. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:1:2","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#refactoring-by-m-fowler"},{"categories":null,"content":" 1.3 Design Patterns by the GoF Design Patterns by the GoF One of the first tech book I bought. It was during the first years of my career. I had a blast reading it because it addressed common problems with simple cooking recipes we could easily put into practice. I think it is still worth reading it. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:1:3","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#design-patterns-by-the-gof"},{"categories":null,"content":" 2 For Java developers","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:2:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#for-java-developers"},{"categories":null,"content":" 2.1 Effective Java by J. BLOCH Effective Java From my perspective, it is the perfect Java developer “Clean Code” book companion. If you are a Java developer beginner (or not!), you can also refer to Head First Java by K. SIERA \u0026 B. BATES. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:2:1","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#effective-java-by-j-bloch"},{"categories":null,"content":" 3 For designers","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:3:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#for-designers"},{"categories":null,"content":" 3.1 Fundamentals of Software Architecture by M. RICHARDS \u0026 N. FORD Fundamentals of Software Architecture It is definitively the book I would have wished to read 10+ years ago! In my opinion, reading this book is a mandatory step for anyone who wants to dig into architecture. It addresses both the hard skills with all the main architecture design patterns and the soft skills (i.e., how to work as an architect?). ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:3:1","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#fundamentals-of-software-architecture-by-m-richards--n-ford"},{"categories":null,"content":" 3.2 Software architecture for developers: Technical Leadership and the balance with agility by S. BROWN Software architecture for developers: Technical Leadership and the balance with agility This is another terrific book talking about software architecture. I also recommend reading it. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:3:2","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#software-architecture-for-developers-technical-leadership-and-the-balance-with-agility-by-s-brown"},{"categories":null,"content":" 3.3 Building Microservices by S. NEWMAN Building Microservices If you want to dig into how to design a microservice platform, this book is a good start. It will provide you a good overview of all the main points to tackle such as integration, deployment, or the famous Conway’s Law. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:3:3","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#building-microservices-by-s-newman"},{"categories":null,"content":" 3.4 Building Evolutionary Architectures by N. FORD, R. PARSONS \u0026 P. KUA Building Evolutionary Architectures This book comforted me a platform must be on the one hand designed and sat up as simply as possible and on the other hand supports changes over the time. Among other things, the authors bring guidance to achieve that. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:3:4","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#building-evolutionary-architectures-by-n-ford-r-parsons--p-kua"},{"categories":null,"content":" 4 For speakers","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:4:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#for-speakers"},{"categories":null,"content":" 4.1 Resonate by N. DUARTE Resonate It is considered as a reference for people who talk in public. Even if I read a bunch on this field, it is still the best I read so far. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:4:1","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#resonate-by-n-duarte"},{"categories":null,"content":" 5 For people who want to get rid of stress and being (mostly) organised","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:5:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#for-people-who-want-to-get-rid-of-stress-and-being-mostly-organised"},{"categories":null,"content":" 5.1 Getting Things Done by D. ALLEN Getting Things Done Last but not least, this book changed 15 years ago the way I organised both my personal and professional life. This kind of personal life methodology does not work for everyone. For me, it was a game changer. ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:5:1","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#getting-things-done-by-d-allen"},{"categories":null,"content":" 6 ConclusionUnfortunately, I haven’t read plenty of fascinating books yet. Regarding software architecture or design topics, I usually choose a book taking a look on the Gregor Hohpe bookshelf first. You will find many of the books I already mentioned in this article and many more! ","date":"2024-02-14","objectID":"/2024/02/14/my-tech-library/:6:0","series":null,"tags":["Learning"],"title":"My (not only) Tech Library","uri":"/2024/02/14/my-tech-library/#conclusion"},{"categories":null,"content":" 1 The sad realityPicture this: it’s Friday afternoon, and you’re eagerly looking forward to unwinding for the weekend. Suddenly, an Ops engineer alerts you about a critical issue—a stubborn HTTP 500 error that’s causing a major roadblock. Despite the dedicated efforts of the Ops engineers, the root cause remains elusive due to a lack of contextual information. Hours pass by, but you take it upon yourself to delve into the problem. Eventually, after reproducing and debugging the issue on your computer, you uncover the issue. Does this sound like science fiction? If you’ve experienced a similar scenario, you’re likely familiar with the challenges posed by unidentified end users and their unique usage patterns—enter Ops and observability! I’ve previously delved into the topic of observability. Here are a bunch of articles I wrote on this blog or on the Worldline Tech Blog: Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry Observabilité et Circuit Breaker avec Spring Enabling distributed tracing on your microservices Spring app using Jaeger and OpenTracing In this article, I aim to highlight the importance of putting in place observability during the earliest stages of a project. I will then outline how to merge logs and traces from a good old Spring Boot application on the Grafana Stack to gain clearer insights into your platform’s workings. By doing so, you can transform your relationship with Ops teams, making them your best friends. What about the code? The examples provided in this article come from this project hosted on Github. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:1:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#the-sad-reality"},{"categories":null,"content":" 2 A definition of ObservabilityWe can shortly define it as this: Observability is the ability to understand the internal state of a complex system. When a system is observable, a user can identify the root cause of a performance problem by examining the data it produces, without additional testing or coding. This is one of the ways in which quality of service issues can be addressed. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:2:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#a-definition-of-observability"},{"categories":null,"content":" 3 A short presentation of the Grafana stackThe Grafana stack aims at a tool which allows you to query, visualise, alert and explore all of your metrics. You can aggregate them through a wide range of data sources. With regard to the topic of this article,it will provide us all you need to collect logs, metrics and traces (and beyond) to monitor and understand the behaviour of your platforms. I will therefore particularly focus on: Grafana: The dashboard engine Loki: The log storage engine Tempo: The trace storage engine By the way, I also configured in this project a Prometheus TSDB to store metrics. To get it started easily, I just created a Docker Compose stack to run it on your desktop. You can run it with these commands: bash cd docker docker compose up ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:3:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#a-short-presentation-of-the-grafana-stack"},{"categories":null,"content":" 4 Logs, Traces \u0026 MonitoringLet’s go back to the basics: To make a system fully observable, the following abilities must be implemented: Logs Traces Metrics They can be defined as follows: ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:4:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#logs-traces--monitoring"},{"categories":null,"content":" 5 LogsWhen a program fails, OPS usually tries to identify the underlying error analyzing log files. It could be either reading the application log files or using a log aggregator such as Elastic Kibana or Splunk. In my opinion, most of the time developers don’t really care about this matter. It is mainly due to they did not experience such a trouble. For two years, I had to administrate a proprietary customer relationship management solution. The only way to analyse errors was navigating through the logs, using the most appropriate error levels to get the root cause. We didn’t have access to the source code (Long live to open source programs). Hopefully the log management system was really efficient. It helped us get into this product and administrate it efficiently. Furthermore, I strongly think we should systematise such experiences for developers. It could help them (us) know what is behind the curtain and make more observable and better programs. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:5:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#logs"},{"categories":null,"content":" 5.1 Key principlesYou must first dissociate the logs you make while you code (e.g., for debugging) from the production logs. The first should normally remove the first. For the latter, you should apply some of these principles: Identify and use the most appropriate level (DEBUG, INFO, WARN, ERROR,…) Provide a clear and useful message for OPS (yes you make this log for him/her) Provide business context (e.g., the creation of the contract 123456 failed) Logs must be read by an external tool (e.g., using a log aggregator) Logs must not expose sensitive data: You must think about GDPR, PCI DSS standards If you want to dig into log levels and the importance to indicate contextual information into your logs, I suggest you reading this article from my colleague Nicolas Carlier. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:5:1","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#key-principles"},{"categories":null,"content":" 6 What about Grafana LokiFor this test, I chose to use loki-logback-appender to send the logs to Loki. About this appender I chose to use this appender for testing purpose. If you deploy your application on top of Kubernetes, you would probably opt for a more suitable solution such as FluentD. The configuration for a Spring Boot application is pretty straightforward: You must add first the appender to your classpath: groovy implementation 'com.github.loki4j:loki-logback-appender:1.4.2' and create a logback-spring.xml to configure it: xml \u003cappender name=\"LOKI\" class=\"com.github.loki4j.logback.Loki4jAppender\"\u003e \u003chttp\u003e \u003curl\u003ehttp://localhost:3100/loki/api/v1/push\u003c/url\u003e \u003c/http\u003e \u003cformat\u003e \u003clabel\u003e \u003cpattern\u003eapp=${name},host=${HOSTNAME},level=%level\u003c/pattern\u003e \u003creadMarkers\u003etrue\u003c/readMarkers\u003e \u003c/label\u003e \u003cmessage\u003e \u003cpattern\u003e {\"level\":\"%level\",\"class\":\"%logger{36}\",\"thread\":\"%thread\",\"message\": \"%message\",\"requestId\": \"%X{X-Request-ID}\"} \u003c/pattern\u003e \u003c/message\u003e \u003c/format\u003e \u003c/appender\u003e Et voilà! About the format It is just my 2 cents: more and more I tend to produce structured logs using JSON for instance. It is usually easier to manipulate them all along the log ingestion tools chain (e.g, with LogStash. After restarting your application: bash gradle bootRun After running some API calls with the following command: bash http :8080/api/events You can now get logs browsing Grafana ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:6:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#what-about-grafana-loki"},{"categories":null,"content":" 7 TracesUpon initial inspection, one might consider the existing setup sufficient. However, I highly recommend delving into the realm of Distributed Tracing, a technology I have previously introduced (refer to the aforementioned discussion). Not only it will be first really useful when you deploy distributed architectures but also for the other kind of platforms. The true value of distributed tracing becomes evident not only in the deployment of distributed architectures but across various platforms. In the complex landscape of production issues, identifying the root cause or understanding why a specific SQL query failed or took an extended duration can be challenging. Traditionally, attempts to replicate such issues in alternative environments often fall short due to the inherent complexities of data, server configurations, and benchmarking. This technology empowers you to gain valuable insights that were previously elusive. When grappling with production issues, you no longer need to rely solely on replication efforts; distributed tracing provides a clear and comprehensive perspective on what might be amiss. To sum up: Try it, you’ll like it! ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#traces"},{"categories":null,"content":" 7.1 The setupThere is several ways to set it up. Nowadays, OpenTelemetry is the de facto standard. Most of the solutions are compatible with it. Nevertheless, after challenging some APMs, I found some missing features which are really useful in real life projects. For instance, you can not easily ignore URLs, for instance the actuator endpoints, from the traces you will manage. You can do that in just one property with the Elastic APM agent. There is an issue about this feature. I suggest using the agents. It is less intrusive than other solutions. For instance if you use the spring boot gradle plugin you can configure it as following: groovy plugins { id 'java' id 'org.springframework.boot' version '3.2.1' id 'io.spring.dependency-management' version '1.1.4' } ext { opentelemetryAgentVersion = '1.32.0' // Mettez la version appropriée } group = 'info.touret.observability' version = '0.0.1-SNAPSHOT' java { sourceCompatibility = '21' } repositories { mavenCentral() } dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'io.micrometer:micrometer-registry-prometheus' testImplementation 'org.springframework.boot:spring-boot-starter-test' implementation 'com.github.loki4j:loki-logback-appender:1.4.2' implementation \"io.opentelemetry.javaagent:opentelemetry-javaagent:${opentelemetryAgentVersion}\" } task copyJavaAgent(type: Copy) { from configurations.detachedConfiguration( dependencies.create(\"io.opentelemetry.javaagent:opentelemetry-javaagent:${opentelemetryAgentVersion}\") ) into \"${project.getLayout().getBuildDirectory()}/javaagents\" rename { 'javaagent.jar' } } processResources.dependsOn copyJavaAgent bootRun { doFirst { jvmArgs = [\"-javaagent:${project.getLayout().getBuildDirectory()}/javaagents/javaagent.jar\"] } // // systemProperties = [ // 'otel.traces.sampler': 'parentbased_traceidratio', // 'otel.traces.sampler.arg': '0.2' // ] } tasks.named('test') { useJUnitPlatform() } After restarting your application, you can reach the API with this command: bash http :8080/api/events This API is really simple. To illustrate how to handle errors using both the Spring stack and the Grafana stack, an error is always thrown using the Problem Detail RFC 7807 while reaching it. Here the service: java @Service public class ObservabilityService { public void breakMethod() { throw new IllegalStateException(\"Breaking method issue\"); } } And the controller which returns the error: java @GetMapping(\"/api/event\") public ResponseEntity\u003cObservabilityEventDto\u003e getEvent() throws ErrorResponseException { try { observabilityService.breakMethod(); var observabilityEventDto = new ObservabilityEventDto(UUID.randomUUID().toString(), \"OK\"); return ResponseEntity.ok(observabilityEventDto); } catch (Exception e) { var observabilityEventDto = new ObservabilityEventDto(UUID.randomUUID().toString(), \"Error\"); LOGGER.error(e.getMessage()); throw new ErrorResponseException(HttpStatus.INTERNAL_SERVER_ERROR, ProblemDetail.forStatus(HttpStatus.INTERNAL_SERVER_ERROR), e); } } Using Problem Detail responses, you will get such a response when an error occurs: bash http :8080/api/events HTTP/1.1 500 Connection: close Content-Type: application/problem+json Date: Wed, 17 Jan 2024 08:09:20 GMT Transfer-Encoding: chunked { \"instance\": \"/api/events\", \"status\": 500, \"title\": \"Internal Server Error\", \"type\": \"about:blank\" } After testing this service a few times, you can now see the traces on your Grafana dashboard. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:1","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#the-setup"},{"categories":null,"content":" 7.2 Head or Tail sampling?One significant drawback of implementing this technology lies in the potential performance overhead it introduces to the instrumented application. In cases where high-pressure APIs generate or broadcast SPANs for every transaction, there’s a substantial risk of significantly impacting the Service Level Objectives (SLOs) of your platform. A viable solution to mitigate this challenge involves sampling the traces, such as retaining only 20% of the transactions. There are two primary approaches: Head Sampling: In this method, SPANs are sampled and filtered directly from the producer (e.g., a backend). This is essential for heavily utilized platforms and proves to be the most efficient, as it produces only the necessary spans, thereby avoiding the dissemination of unnecessary SPANs. However, it comes with the trade-off of potentially losing critical traces involving failures. The sampling rate is purely statistical (e.g., 10 or 20% of SPANs sampled and broadcast). Tail Sampling: Alternatively, SPANs are sampled retrospectively, often through tools like the Open Telemetry Collector. While this method allows for filtering SPANs based on various criteria, such as the transaction status, it does not address the overhead issue. All SPANs are initially broadcast and then filtered, making it less suitable for heavily used scenarios. Both approaches have their pros and cons, and the choice depends on the specific requirements of the platform. For an in-depth exploration of this issue, you can refer to this article. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:2","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#head-or-tail-sampling"},{"categories":null,"content":" 8 Correlating Logs \u0026 TracesNow, you have on one side the logs of your applications, and on the other the traces. To dig into errors and see what is behind the curtain of any error logged, it is really import to correlate both. For that, you must specify in your logs the traceID and spanID of the corresponding trace. Hopefully, logback and the Loki appender can help you on this! We therefore will modify the pattern of the logs in the logback-spring.xml file: xml \u003cpattern\u003e {\"level\":\"%level\",\"TraceID\":\"%mdc{trace_id:-none}\",\"spanId\":\"%mdc{span_id:-none}\",\"class\":\"%logger{36}\",\"thread\":\"%thread\",\"message\": \"%message\",\"requestId\": \"%X{X-Request-ID}\"} \u003c/pattern\u003e As a developer point of view, the job is done :) Now, it is time for the OPS/SRE to configure Grafana to link Loki and Tempo through the TraceID field. For that, you can create a derived field directly in the datasource configuration: yaml datasources: - name: Loki type: loki access: proxy uid: loki url: http://loki:3100 jsonData: maxLines: 1000 derivedFields: - datasourceUid: tempo matcherRegex: '\\\"TraceID\\\": \\\"(\\w+).*\\\"' name: TraceID # url will be interpreted as query for the datasource url: '$${__value.raw}' # optional for URL Label to set a custom display label for the link. urlDisplayLabel: 'View Trace' - name: Tempo type: tempo access: proxy uid: tempo url: http://tempo:3200 jsonData: nodeGraph: enabled: true serviceMap: datasourceUid: 'mimir' tracesToLogs: datasourceUid: loki filterByTraceID: true filterBySpanID: false mapTagNamesEnabled: false Now you will be able to browse directly to the corresponding trace from your log event and the other way around. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:8:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#correlating-logs--traces"},{"categories":null,"content":" 9 MetricsNow, let us deep dive into the metrics of our application! We can do that through Prometheus. We can configure now Prometheus to grab the metrics exposed by our application. To do that, we need first to activate the Prometheus endpoint: We need to add this dependency first: groovy implementation 'io.micrometer:micrometer-registry-prometheus' And enable the corresponding endpoint: ini management.endpoints.web.exposure.include=health,info,prometheus After enabling it, as a developer point of view, it is done :-) The prometheus statistics can be scrapped by Prometheus itself using this configuration yaml scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /actuator/prometheus scheme: http static_configs: - targets: - host.docker.internal:8080 Finally, you can directly browse it through Grafana to integrate all of these metrics into your dashboards 🎉. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:9:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#metrics"},{"categories":null,"content":" 10 ConclusionI endeavored to provide you with a comprehensive overview of what an OPS professional could anticipate while investigating an issue and the corresponding topics that require attention. As you probably figured out, we only applied just a bunch of configuration sets. One of the key merits of these tools lies in their non-intrusiveness within the code itself. To cut long story short: it is not a big deal! Integrating these configurations can be a significant stride forward, providing invaluable assistance to the entire IT team, from development to operations, as they navigate and troubleshoot issues—whether in production or elsewhere. I will finish this article by my opinion on such topics: regardless of the targeted tools, this set of configuration must be considered as the first feature to implement for every cloud native application. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:10:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#conclusion"},{"categories":null,"content":" 1 2023 in a NutshellAs we approach 2024, it’s time to cast a professional eye back on 2023. Throughout the year, I’ve balanced work on a customer project alongside my contributions to the Worldline TechRel1 initiative. Each involvement has fueled the other, offering a reciprocal flow of inspiration. Involvement in real-life projects has often sparked new ideas for talks and topics to delve into. In turn, my experiences in Dev Rel have offered fresh perspectives and external feedback, enriching the implementation process. Highlights of my speaking engagements this year include: 6 tech conferences 4 meetups A Worldline tech event in Barcelona An online presentation (i.e., BBL) for ABBEAL This year, unlike the previous one, I found myself presenting two talks or a talk and a workshop at the same conference—a challenging but exciting experience! Consequently, I delivered a similar number of talks compared to 2022. My primary topics this year included: Rest API Versioning The Hitchhiker’s Guide to Software Architecture Design The Architecture Katas Notably, I co-presented the second talk with my colleague Raphaël Semeteys, extending my collaborative speaking initiatives that commenced in 2022 with Jean-François James (I shared the stage with him at LyonJUG). This collaborative process has been immensely rewarding. It helps me push me beyond my comfort zone, enabling me to approach various topics from a fresh perspective. A prime example was our exploration of discussing software architecture in a more light-hearted and unconventional manner, which initially seemed improbable to me. In fact, I intend to explore more partnerships in the future. Moreover, I participated in the recent JChateau Edition, my initiation into the world of “unconferences” (JunConf)—an incredibly enriching experience! Interacting with inspiring individuals like Jean-Michel Doudoux, Andres Almirez, and José Paumard was truly inspiring. During the year, I released six articles on my blog and one on the Worldline engineering blog. I express my gratitude to my employer for the continued opportunities and to the organizers for their unwavering trust and hospitality. Your support has been instrumental in my journey. The Worldline TechRel initiative has been instrumental in exploring new topics and facilitating collaborations with colleagues. It has enabled me to share, gather feedback on submissions, and conduct live rehearsals (Thanks Marie-Alice Blete \u0026 Philippe Vincent and the others). Heartfelt thanks to all who have supported me along this journey! ","date":"2023-12-20","objectID":"/2023/12/20/2023-wrap-up/:1:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2023","uri":"/2023/12/20/2023-wrap-up/#2023-in-a-nutshell"},{"categories":null,"content":" 2 What Lies Ahead?Through the 2Worldline TechRel initiative, I’ve had the privilege of meeting exceptional individuals and advancing in various technical domains. Crafting and presenting talks on technical subjects demand in-depth knowledge, and this has already sparked ideas for the upcoming year. Kicking off the new year with a bang, I am thrilled to announce a workshop at NDC London in late January (details here). Additionally, I will be co-presenting a new talk with my colleague Philippe Duval at Touraine Tech. Wishing everyone a fantastic New Year’s Eve, and if you come across this article in 2024: Happy New Year!! It’s akin to Dev Rel but aims to include a broader tech community encompassing OPS, SRE, etc. ↩︎ Similar to Dev Rel, but aspires to involve all tech crew members (e.g., OPS \u0026 SRE). ↩︎ ","date":"2023-12-20","objectID":"/2023/12/20/2023-wrap-up/:2:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2023","uri":"/2023/12/20/2023-wrap-up/#what-lies-ahead"},{"categories":null,"content":"Just out of curiosity, I downloaded and sat up Rancher Desktop on my laptop. I daily use Docker and Docker compose on top of WSL2 using home made mechanism/tooling I would then see if Rancher Desktop fits well in this case and could help me. In this (very short) article, we’ll go over the necessary steps to configure WSL2 Ubuntu virtual machines and Docker with Rancher Desktop. If you want to get into Rancher Desktop in another way and discover how to install Skaffold, you can read this article. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:0:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#"},{"categories":null,"content":" 1 Install \u0026 configure Rancher Desktop The setup is quite straightforward. Follow the instructions provided in the official documentation to get Rancher Desktop up and running on your Windows machine. Rancher Desktop allows you to run Docker and Docker Compose seamlessly within a WSL2 environment. During the setup process, I chose to install Moby to use Docker and Docker Compose. After installing Rancher Desktop, you will need to ensure your virtual machine (VM) is connected to expose the Docker daemon and related commands. You can find detailed steps in the Rancher Desktop documentation under WSL Preferences. Don’t forget that in some cases, you may need to restart both WSL2 and Rancher Desktop for the changes to take effect. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:1:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#install--configure-rancher-desktop"},{"categories":null,"content":" 2 Configure Docker Credential StoreWhen you start your Docker compose infrastructure and encounter an error like this: jshelllanguage Error saving credentials: error storing credentials - err: exit status 1, You’ll need to configure Docker’s credential store. To resolve this issue, follow these steps: Inside your WSL2 VM, create or edit the ~/.docker/config.json file. Add the following content to the config.json file: json { \"credsStore\": \"wincred.exe\" } This configuration points to the docker-credential-wincred.exe binary and will resolve the credential storage problem when using Docker. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:2:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#configure-docker-credential-store"},{"categories":null,"content":" 3 Get container’s output in the consoleA common issue with Docker containers in Rancher Desktop is the lack of output in the console when running a container, such as with the command docker run hello-world. This issue is well-documented in this GitHub issue. To view the container’s output in the console, you need to start your commands with the -i option. For example: For instance: jshelllanguage docker run -i hello-world This option tells Docker to attach to the container’s standard input, allowing you to see the output directly in your console. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:3:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#get-containers-output-in-the-console"},{"categories":null,"content":" 4 ConclusionI hope this article has been helpful for you, and you’re now ready to supercharge your development workflow with Rancher Desktop and Docker! ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:4:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#conclusion"},{"categories":null,"content":"In my last article, I dug into Distributed Tracing and exposed how to enable it in Java applications. We didn’t see yet how to deploy an application on Kubernetes and get distributed tracing insights. Several strategies can be considered, but the main point is how to minimize the impact of deploying APM agents on the whole delivery process. In this article, I will expose how to ship APM agents for instrumenting Java applications deployed on top of Kubernetes through Docker containers. To make it clearer, I will illustrate this setup by the following use case: We have an API “My wonderful API” which is instrumented through an Elastic APM agent. The data is then sent to the Elastic APM. Now, if we dive into the “Wonderful System”, we can see the Wonderful Java application and the agent: Elastic APM vs Grafana/OpenTelemetry In this article I delve into how to package an Elastic APM agent and enable Distributed Tracing with the Elastic APM suite. You can do that in the same way with an OpenTelemetry Agent. Furthermore, Elastic APM is compatible with OpenTelemetry. We can basically implement this architecture in two different ways: Deploying the agent in all of our Docker images Deploying the agent asides from the Docker images and using initContainers to bring the agent at the startup of our applications We will then see how to lose couple application docker images to the apm agent one. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:0:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#"},{"categories":null,"content":" 1 Why not bringing APM agents in our Docker images?It could be really tempting to put the APM agents in the application’s Docker image. Why? Because you just have to add the following lines of code in our Docker images definition: dockerfile RUN mkdir /opt/agent COPY ./javaagent.jar /opt/agent/javaagent.jar Nonetheless, if you want to upgrade your agent, you will have to repackage it and redeploy all your Docker images. For regular upgrades, it will not bother you, but, if you encounter a bug or a vulnerability, it will be tricky and annoying to do that. What is why I prefer loose coupling the “business” applications Docker images to technical tools such as APM agents. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:1:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#why-not-bringing-apm-agents-in-our-docker-images"},{"categories":null,"content":" 2 Deploy an APM agent through initContainersWhile looking around how to achieve this, I came across to the Kubernetes initContainers. This kind of container is run only once during the startup of every pod. A bunch of commands is ran then on top of it. For our current use case, it will copy the javaagent into a volume such as an empty directory volume. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#deploy-an-apm-agent-through-initcontainers"},{"categories":null,"content":" 2.1 Impacts in the “Wonderful Java Application Docker imageThe main impact is to declare a volume in your Docker image: dockerfile VOLUME /opt/agent It will be used by both the Docker container and the initContainer. We can consider it as a “bridge” between these two ones. We also have to declare one environment variable: JAVA_OPTS. For instance: dockerfile ENV JAVA_OPTS=$JAVA_OPTS [...] ENTRYPOINT [\"sh\", \"-c\", \"java ${JAVA_OPTS} org.springframework.boot.loader.JarLauncher\"] Il will be used during the deployment to set up our Wonderful Java Application. Now, let’s build our initContainer’s Docker image. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:1","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#impacts-in-the-_wonderful-java-application_-docker-image"},{"categories":null,"content":" 2.2 InitContainer Docker Image creationIt is really straightforward. We can use for example, the following configuration: dockerfile FROM alpine:latest RUN mkdir -p /opt/agent_setup RUN mkdir /opt/agent COPY ./javaagent.jar /opt/agent_setup/javaagent.jar VOLUME /opt/agent ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:2","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#initcontainer-docker-image-creation"},{"categories":null,"content":" 2.3 Kubernetes configurationWe can now set up our Kubernetes Deployment to start the corresponding container and copy the Java agent. yaml kind: Deployment spec: containers: - name: java-app image: repo/my-wonderful-java-app:v1 volumeMounts: - mountPath: /opt/agent name: apm-agent-volume initContainers: - command: - cp - /opt/agent_setup/javaagent.jar - /opt/agent name: apm-agent-init image: repo/apm-agent:v1 volumeMounts: - mountPath: /opt/agent name: appd-agent-volume volumes: - name: appd-agent-volume emptyDir: {} Why not just copying the Java agent directly in the initContainer Docker image execution? The copy must be run with a command specified in the initContainer declaration and cannot be done during the initContainer execution (i.e., specified in its Dockerfile). Why? The volume is mounted just after the initContainer execution and drops the JAR file copied earlier. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:3","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#kubernetes-configuration"},{"categories":null,"content":" 3 Start the Java Application with the agentLast but not least, we can now configure the pods where we run our Java applications. We will use the JAVA_OPTS environment variable to configure the location of the Java agent, and the Elastic APM Java system properties. For instance: jshelllanguage JAVA_OPTS=-javaagent:/opt/agent/javaagent.jar -Delastic.apm.service_name=my-wonderful-application -Delastic.apm.application_packages=org.mywonderfulapp -Delastic.apm.server_url=http://apm:8200 You can then configure your Kubernetes deployment as: yaml spec: containers: - name: java-app env: - name: JAVA_OPTS value: -javaagent:/opt/agent/javaagent.jar -Delastic.apm.service_name=my-wonderful-application -Delastic.apm.application_packages=org.mywonderfulapp -Delastic.apm.server_url=http://apm:8200 Et voila! ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:3:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#start-the-java-application-with-the-agent"},{"categories":null,"content":" 4 ConclusionWe have seen how to pack and deploy Distributed Tracing java agents and Java Applications built on top of Docker images. Obviously, my technical choice of using an InitContainer can be challenged regarding your technical context and how you are confortable with your delivery practices. You probably noticed I use an emptyDir to deploy the Java agent. Normally it will not be a big deal, but I advise you to check this usage with your Kubernetes SRE/Ops/Administrator first. Anyway, I think it is worth it and the tradeoffs are more than acceptable because this approach are, in my opinion, more flexible than the first one. Hope this helps! ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:4:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#conclusion"},{"categories":null,"content":"Picture Credit: Nick FEWINGS ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:0:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#"},{"categories":null,"content":" 1 IntroductionIn today’s dynamic landscape, Distributed Tracing has emerged as an indispensable practice. It helps to understand what is under the hood of distributed transactions, providing answers to pivotal questions: What comprises these diverse requests? What contextual information accompanies them? How extensive is their duration? Since the introduction of Google’s Dapper, a plethora of tracing solutions has flooded the scene. Among them, OpenTelemetry has risen as the frontrunner. Other alternatives such as Elastic APM and DynaTrace are also available. This toolkit seamlessly aligns with APIs and synchronous transactions, catering to a broad spectrum of scenarios. However, what about asynchronous transactions? The necessity for clarity becomes even more pronounced in such cases. Particularly in architectures built upon messaging or event streaming brokers, attaining a holistic view of the entire transaction becomes arduous. Why does this challenge arise? It’s a consequence of functional transactions fragmenting into two loosely coupled subprocesses: Hopefully you can rope OpenTelemetry in it to shed light. What about the main concepts of Distributed Tracing? I will not dig into the concepts of Distributed tracing in this article. If you are interested in it, you can read my article on the Worldline Tech Blog. I will explain in this article how to set up and plug OpenTelementry to gather asynchronous transaction traces using Apache Camel and Artemis. The first part will use Jaeger and the second one, Tempo and Grafana to be more production ready. All the code snippets are part of this project on GitHub. (Normally) you can use and run it locally on your desktop. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:1:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#introduction"},{"categories":null,"content":" 2 Jaeger","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#jaeger"},{"categories":null,"content":" 2.1 ArchitectureThe SPANs are broadcast and gathered through OpenTelemetry Collector. It finally sends them to Jaeger. Here is the architecture of such a platform: ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture"},{"categories":null,"content":" 2.2 OpenTelemetry CollectorThe cornerstone of this architecture is the collector. It can be compared to Elastic LogStash or an ETL. It will help us get, transform and export telemetry data. Source: https://opentelemetry.io/docs/collector/ For our use case, the configuration is quite simple. First, here is the Docker Compose configuration: yaml otel-collector: image: otel/opentelemetry-collector:0.75.0 container_name: otel-collector command: [ \"--config=/etc/otel-collector-config.yaml\" ] volumes: - ./docker/otel-collector-config.yaml:/etc/otel-collector-config.yaml ports: - \"1888:1888\" # pprof extension - \"8888:8888\" # Prometheus metrics exposed by the collector - \"8889:8889\" # Prometheus exporter metrics - \"13133:13133\" # health_check extension - \"4317:4317\" # OTLP gRPC receiver - \"55670:55679\" # zpages extension and the otel-collector-config.yaml: yaml # (1) receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] # (2) exporters: # prometheus: # endpoint: \"0.0.0.0:8889\" # const_labels: # label1: value1 logging: jaeger: endpoint: jaeger:14250 tls: insecure: true # zipkin: # endpoint: http://zipkin:9411/api/v2/spans # tls: # insecure: true # (3) processors: batch: extensions: health_check: pprof: endpoint: :1888 zpages: endpoint: :55679 # (4) service: extensions: [pprof, zpages, health_check] pipelines: traces: receivers: [otlp] processors: [batch] exporters: [logging, jaeger] metrics: receivers: [otlp] processors: [batch] exporters: [logging] Short explanation If you want further information about this configuration, you can browse the documentation. For those who are impatient, here are a short explanation of this configuration file: Where to pull data? Where to store data? What to do with it? What are the workloads to activate? ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#opentelemetry-collector"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: xml \u003cdependency\u003e \u003cgroupId\u003eorg.apache.camel.springboot\u003c/groupId\u003e \u003cartifactId\u003ecamel-opentelemetry-starter\u003c/artifactId\u003e \u003c/dependency\u003e In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. java @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration xml \u003cprofile\u003e \u003cid\u003eopentelemetry\u003c/id\u003e \u003cactivation\u003e \u003cproperty\u003e \u003cname\u003eapm\u003c/name\u003e \u003cvalue\u003eotel\u003c/value\u003e \u003c/property\u003e \u003c/activation\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003ecopy-javaagent\u003c/id\u003e \u003cphase\u003eprocess-resources\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003ecopy\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003cartifactItems\u003e \u003cartifactItem\u003e \u003cgroupId\u003eio.opentelemetry.javaagent\u003c/groupId\u003e \u003cartifactId\u003eopentelemetry-javaagent\u003c/artifactId\u003e \u003cversion\u003e${opentelemetry-agent.version}\u003c/version\u003e \u003coverWrite\u003etrue\u003c/overWrite\u003e \u003coutputDirectory\u003e${project.build.directory}/javaagents\u003c/outputDirectory\u003e \u003cdestFileName\u003ejavaagent.jar\u003c/destFileName\u003e \u003c/artifactItem\u003e \u003c/artifactItems\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cagents\u003e \u003cagent\u003e${project.build.directory}/javaagents/javaagent.jar\u003c/agent\u003e \u003c/agents\u003e \u003c!-- \u003csystemPropertyVariables\u003e--\u003e \u003c!-- \u003cotel.traces.sampler\u003eparentbased_traceidratio\u003c/otel.traces.sampler\u003e--\u003e \u003c!-- \u003cotel.traces.sampler.arg\u003e0.2\u003c/otel.traces.sampler.arg\u003e--\u003e \u003c!-- \u003c/systemPropertyVariables\u003e--\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/profile\u003e The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. jshelllanguage cd containers docker compose up You can now start both the producer and the consumer: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the m","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: xml org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. java @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration xml opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. jshelllanguage cd containers docker compose up You can now start both the producer and the consumer: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the m","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#libraries-to-add"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: xml org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. java @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration xml opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. jshelllanguage cd containers docker compose up You can now start both the producer and the consumer: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the m","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code-1"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: xml org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. java @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration xml opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. jshelllanguage cd containers docker compose up You can now start both the producer and the consumer: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the m","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#the-java-agent"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: xml org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. java @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don’t know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration xml opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. jshelllanguage cd containers docker compose up You can now start both the producer and the consumer: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml jshelllanguage mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: jshelllanguage mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the m","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#how-is-made-the-glue-between-the-two-applications"},{"categories":null,"content":" 2.4 DashboardTo get traces, I ran this dumb command to inject traces into Jaeger: jshelllanguage while true ; http :9080/camel/test; end Now, you can browse Jaeger (http://localhost:16686) and query it to find trace insights: Number of different apps If you dig into one transaction, you will see the whole transaction: One transaction And now, you can correlate two sub transactions: Two sub transactions ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard"},{"categories":null,"content":" 3 Tempo \u0026 GrafanaThis solution is pretty similar to the previous one. Instead of pushing all the data to Jaeger, we will use Tempo to store data and Grafana to render them. We don’t need to modify the configuration made in the existing Java applications. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo--grafana"},{"categories":null,"content":" 3.1 ArchitectureAs mentioned above, the architecture is quite the same. Now, we have the collector which broadcast data to Tempo. We will then configure Grafana to query to it to get traces. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture-1"},{"categories":null,"content":" 3.2 Collector configurationThe modification of the Collector is easy (for this example). We only have to specify the tempo URL. yaml receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] exporters: otlp: endpoint: tempo:4317 tls: insecure: true service: pipelines: traces: receivers: [otlp] exporters: [otlp] ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#collector-configuration"},{"categories":null,"content":" 3.3 Tempo configurationI used here the standard configuration provided in the documentation: yaml server: http_listen_port: 3200 distributor: receivers: # this configuration will listen on all ports and protocols that tempo is capable of. jaeger: # the receives all come from the OpenTelemetry collector. more configuration information can protocols: # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver thrift_http: # grpc: # for a production deployment you should only enable the receivers you need! thrift_binary: thrift_compact: zipkin: otlp: protocols: http: grpc: opencensus: ingester: max_block_duration: 5m # cut the headblock when this much time passes. this is being set for demo purposes and should probably be left alone normally compactor: compaction: block_retention: 1h # overall Tempo trace retention. set for demo purposes metrics_generator: registry: external_labels: source: tempo cluster: docker-compose storage: path: /tmp/tempo/generator/wal remote_write: - url: http://prometheus:9090/api/v1/write send_exemplars: true storage: trace: backend: local # backend configuration to use wal: path: /tmp/tempo/wal # where to store the wal locally local: path: /tmp/tempo/blocks overrides: metrics_generator_processors: [service-graphs, span-metrics] # enables metrics generator search_enabled: true ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo-configuration"},{"categories":null,"content":" 3.4 Grafana configurationNow we must configure Grafana to enable querying into our tempo instance. The configuration is made here using a configuration file provided during the startup The datasource file: yaml apiVersion: 1 datasources: # Prometheus backend where metrics are sent - name: Prometheus type: prometheus uid: prometheus url: http://prometheus:9090 jsonData: httpMethod: GET version: 1 - name: Tempo type: tempo uid: tempo url: http://tempo:3200 jsonData: httpMethod: GET serviceMap: datasourceUid: 'prometheus' version: 1 ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#grafana-configuration"},{"categories":null,"content":" 3.5 DashboardAs we have done before, we must start the infrastructure using Docker Compose: jshelllanguage cd containers docker compose -f docker-compose-grafana.yml up Then, using the same rocket scientist maven commands, we can run the same commands and browse now Grafana (http://localhost:3000) to see our traces: Transactions Deep dive into one transaction ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:5","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard-1"},{"categories":null,"content":" 4 ConclusionWe saw how to highlight asynchronous transactions and correlate them through OpenTelemetry and Jaeger or using Tempo \u0026 Grafana. It was voluntarily simple. If you want to dig into OpenTelemetry Collector configuration, you can read this article from Antik ANAND (Thanks to Nicolas FRANKËL for sharing it) and the official documentation. A noteworthy aspect of OpenTelemetry lies in its evolution into an industry-standard over time. For instance,Elastic APM is compatible with it. I then exposed how to enable this feature on Apache Camel applications. It can be easily reproduced with several stacks. Last but not least, which solution is the best? I have not made any benchmark of Distributed Tracing solutions. However, for a real life production setup, I would dive into Grafana and Tempo and check their features. I am particularly interested in mixing logs, traces to orchestrate efficient alerting mechanisms. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:4:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#conclusion"},{"categories":null,"content":"While chatting with one of my WL colleague, I stumbled upon Fish shell. I immediately liked its autocompletion and extensibility mechanisms. After many years using BASH and ZSH, I therefore decided to move on to this new shell. Unlike the others, it’s not POSIX-compatible. Furthermore, to get (at least) the same functionalities as OhMyZsh, I chose to install StarShip. I will then describe how I moved on and updated my existing tools such as SdkMan. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:0:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#"},{"categories":null,"content":" 1 FISH Installation OS I applied these commands on both Ubuntu20/WSL2 and Linux Mint. To install it, run this command: jshelllanguage sudo apt install fish You must also use a font available on the NerdFonts website. By the way, you can also use the fonts available through your package manager. For instance, I chose using JetBrains Mono After downloading it, you can reload your font cache running this command: jshelllanguage fc-cache -fv ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:1:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#fish-installation"},{"categories":null,"content":" 2 StarShip installationI ran this command: jshelllanguage curl -sS https://starship.rs/install.sh | sh How to update StarShip To update StarShip, you must use the same command. I also added the following command at the end of ~/.config/fish/config.fish: shell starship init fish | source Due to some WSL2 incompatibilities, I also chose to use the plain text presets running this command: jshelllanguage starship preset plain-text-symbols -o ~/.config/starship.toml ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:2:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#starship-installation"},{"categories":null,"content":" 3 SDKMAN updateAt this stage, SdkMan didn’t work at all. To put it alive again, I had to install Fisher and a SdkMan for fish plugin. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#sdkman-update"},{"categories":null,"content":" 3.1 Fisher installRun this command: jshelllanguage curl -sL https://raw.githubusercontent.com/jorgebucaran/fisher/main/functions/fisher.fish | source \u0026\u0026 fisher install jorgebucaran/fisher ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:1","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#fisher-install"},{"categories":null,"content":" 3.2 SdkMan for fish pluginRun this command: jshelllanguage fisher install reitzig/sdkman-for-fish@v2.0.0 ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:2","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#sdkman-for-fish-plugin"},{"categories":null,"content":" 3.3 Run SdkManRun this command: jshelllanguage sdk ug Say yes and restart a shell. Now it should work. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:3","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#run-sdkman"},{"categories":null,"content":" 4 NVMI had the same issue with NVM. I then installed another plugin with Fisher: jshelllanguage fisher install jorgebucaran/nvm.fish ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:4:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#nvm"},{"categories":null,"content":" 5 GnuPGI use GnuPG for signing my GIT commits. Installing Fisher broke my setup. I then added this new configuration file $HOME/.config/fish/conf.d/config_gpgagent.fish with the following content: jshelllanguage set -gx GPG_TTY /dev/pts/0 To activate it, restart your shell (again). ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:5:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#gnupg"},{"categories":null,"content":" 6 ConclusionI can now use FISH for my daily job. As I said first, this article is only a reminder for my next setups (aka when I will broke my GNU/Linux boxes and try to restore them). Hope it will help you! ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:6:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#conclusion"},{"categories":null,"content":" 1 Once upon a time an API … Second Law of Consulting “No matter how it looks at first, it’s always a people problem” - Gerald M. Weinberg Once upon a time, the ACME Corporation was building a brand new IT product. It aimed at a new software to manage bookstores through a web interface and an API. In the first steps, the developers drew up a first roadmap of their API based on the expectations of their first customers. They therefore built and shipped a microservices platform and released their first service contract for their early adopters. Here is the design of this platform: The High level design More in depth To sum up To cut long story short, we have a microservices platform based on the Spring Boot/Cloud Stack exposed through an API Gateway and secured using OpenID Connect. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:1:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#once-upon-a-time-an-api-"},{"categories":null,"content":" 2 The platform and its roadmapAfter shipping it into production, they drew up a roadmap for their existing customers to both improve the existing features and bring new ones. As of now, we could think everything is hunky-dory isn’t it? While engineers worked on improving the existing API, the sales representative have contracted with new customers. They enjoyed this product and its functionalities. However, they also ask for new requirements and concerns. Some of them are easy to apply, some not. For instance, a new customer asked the ACME engineers for getting a summary for every book and additional REST operations. Easy! However, last but not least, this customer would also get a list of authors for every book whereas the existing application only provides ONE author per book. This is a breaking change! What is a breaking change? A breaking change occurs when the backward compatibility is broken between two following versions. For instance, when you completely change the service contract on your API, a client which uses the old API definition is unable to use your new one. A common theoretical approach could be to apply versions on our APIs and adapt it according to the customer. Unfortunately, the devil is in the details. I will describe in this article attention points I struggled with in my last projects. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:2:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#the-platform-and-its-roadmap"},{"categories":null,"content":" 3 What to version? How and where to apply it?After answering to the first question: Do I really need API versioning? you then have to answer to this new one: what should we consider versioning? You only have to version the service contract. In the case of a simple web application based on a GUI and an API Versioning is applied in the service contract of your API. If you change your database without impacting the APIs, why should you waste your time creating and managing a version of your API? It doesn’t make sense. On the other way around, when you evolve your service contract, you usually impact your database (e.g., see the example of breaking change above). Moreover, the version is usually specified on the “middleware” side, where your expose your API. I’ll come back to this point in a later section. If you want to dig into what is a breaking change and what to version, you can read this guide on the GitHub website. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#what-to-version-how-and-where-to-apply-it"},{"categories":null,"content":" 3.1 How many versions must I handle?Tough question! Throughout my different experiences struggling with API versioning, the most acceptable trade-off for both the API provider and customer/client was to only handle two versions: the current and the deprecated one. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#how-many-versions-must-i-handle"},{"categories":null,"content":" 3.2 Where?Now, you have to answer to this question: Where should I handle the version? On the Gateway? On Every Backend? On every service or on every set of services? Directly in the code managed by different packages. Usually, I prefer manage it on the gateway side and don’t bother with URL management on every backend. It could avoid maintenance on both code and tests for every release. However, you can’t have this approach on monolithic applications (see below). ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#where"},{"categories":null,"content":" 3.3 How to define it?Here are three ways to define API versions: In the URL (e.g., /v1/api/books) In a HTTP header (e.g., X-API-VERSION: v1) In the content type (e.g., Accept: application/vnd.myname.v1+json) The last one is now deprecated. The RFC 9110 deprecates now custom usages of the accept HTTP header. I strongly prefer the first one. It is the most straightforward. For instance, if you provide your books API first version, you can declare this URL in your OpenAPI specification:/v1/api/books. The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it’s less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn’t specify the header? Is there any default version? Yes, you can read the documentation to answer these questions, but who (really) does? The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it’s less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn’t specify the header? Is there any default version? Yes, you can read the documentation, but who (really) does? The first solution (i.e., version in the URL) mandatorily conveys the associated version. It is so visible for all the stakeholders and could potentially avoir any mistakes or headaches while debugging. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#how-to-define-it"},{"categories":null,"content":" 4 What about the main software/cloud providers?Before reinventing the wheel, let’s see how the main actors of our industry deal with this topic. I looked around and found three examples: ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#what-about-the-main-softwarecloud-providers"},{"categories":null,"content":" 4.1 Google The version is specified in the URL It only represents the major versions which handle breaking changes ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#google"},{"categories":null,"content":" 4.2 Spotify The version is specified in the URL The API version is still V1 … ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#spotify"},{"categories":null,"content":" 4.3 Apple The version is specified in the URL The API version is still V1 … ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#apple"},{"categories":null,"content":" 5 Appropriate (or not) technologiesIn my opinion, technologies based on the monolith pattern don’t fit handling properly API Versioning. If you are not eager to execute two versions of your monolith, you would have to provide both of the two versions within the same app and runtime. You see the point? You would therefore struggle with: packaging testing both of two releases for every deployment even if a new feature doesn’t impact the deprecated version removing, add new releases in the same source code,… And loosing your mind. In my opinion, best associated technologies are more modular whether during the development or deployment phases. For instance, if you built your app with Container based (Docker, Podman, K8S,..) stack, you would easily switch from one version to another, and sometimes you would be able to ship new features without impacting the oldest version. However, we need to set up our development and integration workflow to do that. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:5:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#appropriate-or-not-technologies"},{"categories":null,"content":" 6 Configuration management \u0026 delivery automationWhen I dug into API versioning, I realised it impacts projects organisation and, by this way, the following items: The source code management: one version per branch or not? The release process: How to create releases properly? Fixes, merges,…: How to apply fixes among branches and versions? The delivery process: How to ship you versions? Yes it IS a big deal Here is the least bad approach I think it works while addressing all of these concerns: ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#configuration-management--delivery-automation"},{"categories":null,"content":" 6.1 Source code configurationWhen you want to have two different versions in production, you must decouple your work in several GIT (what else) branches. For that, I usually put in place GitFlow. source: Atlassian Usually, using this workflow, we consider the develop branch serves as an integration branch. But, now we have two separate versions? Yes, but don’t forget we have a current version and a deprecated one. SemVer I base my versioning naming and numbers on SemVer To handle API versions, we can use release branches. You can easily declare versions regarding your API versions. For instance: release/book-api-1.0.1 release/book-api-2.0.1 We can so have the following workflow: Develop features in feature branches and merge them into the develop branch. Release and use major release numbers (or whatever) to identify breaking changes and your API version number Create binaries (see below) regarding the tags and release branches created Fix existing branches when you want to backport features brought by new features (e.g., when there is an impact on the database mapping), and release them using minor version numbers Apply fixes and create releases ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#source-code-configuration"},{"categories":null,"content":" 6.2 Delivery processAs of now, we saw how to design, create and handle versions. But, how to ship them? If you based your source code management on top of GitFlow, you would be able now to deliver releases available from git tags and release branches. The good point is you can indeed build your binaries on top of these. The bad one, is you must design and automatise this whole process in a CI/CD pipeline. Don’t forget to share it to all the stakeholders, whether developers, integrators or project leaders who are often involved in version definition. Hold on, these programs must be executed against a configuration, aren’t they? Nowadays, if we respect the 12 factors during our design and implementation, the configuration is provided through environment variables. To cut long story short, your API versioning will also impact your configuration. Thus, it becomes mandatory to externalise it and version it. You can do it in different ways. You can, for example, deploy a configuration server. It will provide configuration key/values regarding the version. If you want a live example, you can get an example in a workshop I held this year at SnowcampIO. The configuration is managed by Spring Cloud Config. You can also handle your configuration in your Helm Charts if you deploy your app on top of Kubernetes. Your configuration values will be injected directly during the deployment. Obviously if it’s a monolith, it will be strongly difficult. Why? Because you will lose flexibility on version management and the capacity on deploying several versions of your service. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#delivery-process"},{"categories":null,"content":" 7 Authorisation managementHere is another point to potentially address when we implement API versioning. When you apply an authorisation mechanism on your APIs using OAuthv2 or OpenID Connect, you would potentially have strong differences in your authorisation policies between two major releases. You would then restrict the usage of a version to specific clients or end users. One way to handle this is to use scopes stored in claims. In the use case we have been digging into, we can declare scopes such as: book:v1:write or number:v2:read to specify both the authorised action and the corresponding version. For example, here is a request to get an access_token from the v1 scopes: bash http --form post :8009/oauth2/token grant_type=\"client_credentials\" client_id=\"customer1\" client_secret=\"secret1\" scope=\"openid book:v1:write book:v1:write number:v1:read\" And the response could be: bash { \"access_token\": \"eyJraWQiOiIxNTk4NjZlMC0zNWRjLTQ5MDMtYmQ5MC1hMTM5ZDdjMmYyZjciLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJjdXN0b21lcjIiLCJhdWQiOiJjdXN0b21lcjIiLCJuYmYiOjE2NzI1MDQ0MTQsInNjb3BlIjpbImJvb2t2Mjp3cml0ZSIsIm51bWJlcnYyOnJlYWQiLCJvcGVuaWQiLCJib29rdjI6cmVhZCJdLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgwMDkiLCJleHAiOjE2Nz I1MDQ3MTQsImlhdCI6MTY3MjUwNDQxNH0.gAaDcOaORse0NPIauMVK_rhFATqdKCTvLl41HSr2y80JEj_EHN9bSO5kg2pgkz6KIiauFQ6CT1NJPUlqWO8jc8-e5rMjwWuscRb8flBeQNs4-AkJjbevJeCoQoCi_bewuJy7Y7jqOXiGxglgMBk-0pr5Lt85dkepRaBSSg9vgVnF_X6fyRjXVSXNIDJh7DQcQQ-Li0z5EkeHUIUcXByh19IfiFuw-HmMYXu9EzeewofYj9Gsb_7qI0Ubo2x7y6W2tvzmr2PxkyWbmoioZdY9K0 nP6btskFz2hLjkL_aS9fHJnhS6DS8Sz1J_t95SRUtUrBN8VjA6M-ofbYUi5Pb97Q\", \"expires_in\": 299, \"scope\": \"book:v1:write number:v1:read openid book:v1:read\", \"token_type\": \"Bearer\" } Next, you must validate every API call with the version exposed by your API gateway and the requested scope. When a client tries to reach an API version with inappropriate scopes (e.g., using book:v1:read scope for a client which only uses the v2). You will throw this error: json { \"error\": \"invalid_scope\" } ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:7:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#authorisation-management"},{"categories":null,"content":" 8 And now something completely different: How to avoid versioning while evolving your API?You probably understood that versioning is totally cumbersome. Before putting in place all of these practices, there’s another way to add functionalities on a NON-versioned API without impacting your existing customers. You can add new resources, operations and data without impacting your existing users. {: .notice–warning} With the help of serialization rules, your users would only use the data and operations they know and are confident with. You will therefore bring backward compatibility to your API. Just in case, you can anticipate API versioning by declaring a V1 prefix on your API URL and stick to it while it’s not mandatory to upgrade it. That’s how and why Spotify and Apple (see above) still stick to the V1. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:8:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#and-now-something-completely-different-how-to-avoid-versioning-while-evolving-your-api"},{"categories":null,"content":" 9 Wrap-upYou probably understood when getting into this topic that API versioning is a project management issue with consequences that requires tackling difficult technical ones. To sum up, you need to ask yourself these questions: Do I need it? Can I postpone API versioning by dealing with serialisation rules and just adding new data or operations? Is my architecture design compatible? Are my source code management and delivery practices compatible? After coping with all these points, if you must implement API versioning, you would need to onboard all the different stakeholders, not just developers, to be sure your whole development and delivery process is well aligned with practice. And I forgot: Good luck! ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:9:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#wrap-up"},{"categories":null,"content":"Il y a deux ans déjà, j’ai migré mon site Wordpress sur un site statique hébergé sur Github Pages. Ce dernier était basé sur Ruby, Jekyll et Minimal mistakes. Bien que le projet Minimal Mistakes ne donnait plus trop de signes de vie, le rendu convenait. Cependant, j’étais bloqué sur différents points: La gestion d’articles en anglais et français Le thème dark (inutile donc indispensable) Quelques fonctionnalités manquantes: par ex. MermaidJS J’ai donc décidé de le migrer sur Hugo. Ce générateur de site est basé sur Go et est très rapide d’exécution. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:0:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#"},{"categories":null,"content":" 1 DémarrageJe n’ai pas migré le site comme indiquait la documentation. J’ai préféré créer un nouveau site et copier coller le contenu existant, à savoir les images et les articles. Je vous conseille de lire la documentation qui est bien faite. Ensuite, j’ai choisi le thème LoveIt. Pour l’installer, il suffit de cloner le repo dans le répertoire themes. bashs git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:1:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#démarrage"},{"categories":null,"content":" 2 Reprise de donnéesJ’ai copié les éléments suivants: Les fichiers statiques que j’avais à disposition (CNAME, robots.txt) dans le répertoire static Les images dansle répertoire static/assets/images Les posts et pages statiques ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:2:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#reprise-de-données"},{"categories":null,"content":" 3 Travail sur les posts et images","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#travail-sur-les-posts-et-images"},{"categories":null,"content":" 3.1 Les postsJ’ai ensuite modifié les noms des fichiers en enlevant les dates qui les préfixaient. Ensuite, j’ai modifié les en-têtes de chaque post. J’ai pu le faire en automatisant avec VS CODE. Voici le pattern que j’ai modifié: yml header: teaser: /assets/images/2018/02/2000px-cygwin_logo-svg.png en yml featuredImagePreview: /assets/images/2022/12/review.webp Ca c’était le plus facile… ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-posts"},{"categories":null,"content":" 3.2 Les imagesDans chaque post, j’ai revu les images et leur positionnement. J’ai donc passé chaque article manuellement. C’était réellement fastidieux. Pour être totalement franc, je n’ai paas cherché à automatiser ça. Je pense qu’un script shell, python aurait pu faire l’affaire. Heureusement, je n’en avais pas une centaine… J’ai ajouté quand je pouvais l’en-tête suivant (adaptez le chemin vers l’image ;-) ): yaml featuredImage: /assets/images/2022/12/review.webp images: [\"/assets/images/2022/12/review.webp\"] Le premier attribut permet d’avoir une image d’en-tête pour l’article. Le second permet d’avoir l’image lors d’un partage sur un réseau social (ex. Twitter) j’ai ajouté le code suivant pour centrer les images: markdown {{\u003c style \"text-align:center\" \u003e}} ![dataflow](/assets/images/2022/08/maksym-tymchyk-vHO-yT1BDWk-unsplash.webp) {{\u003c style \u003e}} ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-images"},{"categories":null,"content":" 4 ConfigurationPour garder les mêmes URLs, j’ai choisi de modifier le pattern d’ URL pour inclure la date. C’est un vieux reliquat de mon blog Wordpress. toml [languages.fr.permalinks] posts = '/:year/:month/:day/:filename/' Pour le reste, j’ai copié collé l’exemple fourni par le thème et renseigné les champs en fonction de ce que je voulais. J’ai ensuite adapté le multi langue pour avoir la possibilité de faire des articles en anglais et en français. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#configuration"},{"categories":null,"content":" 4.1 Moteur de rechercheJ’utilise Lunr. Voici la configuration: toml [languages.en.params.search] enable = true type = \"lunr\" contentLength = 4000 placeholder = \"\" maxResultLength = 10 snippetLength = 50 highlightTag = \"em\" absoluteURL = false Il faut également penser à activer la sortie au format JSON: toml # Options to make hugo output files [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] page = [\"HTML\", \"MarkDown\"] section = [\"HTML\", \"RSS\"] taxonomy = [\"HTML\", \"RSS\"] taxonomyTerm = [\"HTML\"] ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#moteur-de-recherche"},{"categories":null,"content":" 4.2 CommentairesA l’instar de mon blog avec Jekyll, j’utilise Utteranc.es. Voici la configuration: toml [params.page.comment.utterances] enable = true # owner/repo repo = \"alexandre-touret/alexandre-touret.github.io\" issueTerm = \"pathname\" label = \"\" lightTheme = \"github-light\" darkTheme = \"github-dark\" ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#commentaires"},{"categories":null,"content":" 5 ConclusionVous aurez bien compris que ma motivation principale derrière cette migration était d’ avoir un support multi langue un peu sympa. Vous avez dans cet article les principales actions que j’ai réalisé. N’hésitez pas à regarder la configuration et les articles pour plus de détails. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:5:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#conclusion"},{"categories":null,"content":"You can read the English version below ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:0:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#"},{"categories":null,"content":" 1 2022 en quelques mots2023 est tout proche. Il est temps de faire un rapide bilan sur cette année 2022 (d’un point pro). Après avoir changé de projet en début d’année, j’ai pu, grâce à mon employeur Worldline, participer en tant que speaker à 8 conférences en français et anglais 2 meetups une présentation en ligne à Malt Academy J’ai également écrit 6 articles sur mon blog et 4 sur le blog d’ingénierie de Worldline. Je tiens à remercier mon employeur pour me permettre de vivre cette expérience ainsi que les organisatrices et organisateurs pour leur confiance et leur accueil (vous vous reconnaitrez). Merci également à toutes celles et ceux qui m’ont aidé également à monter en compétence en tant que speaker. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:1:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-en-quelques-mots"},{"categories":null,"content":" 2 Et maintenant?Grâce à l’initiative 1Tech-Rel de Worldline, j’ai pu faire des super rencontres et progresser techniquement dans certains domaines : Préparer et proposer un sujet technique vous impose de le creuser à fond et de le maîtriser. Pour l’année prochaine, on a quelques idées. J’espère pouvoir communiquer là-dessus rapidement. Je vous souhaite en attendant un bon réveillon et si vous découvrez mon article en 2023 une bonne année. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:2:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#et-maintenant"},{"categories":null,"content":" 3 2022 in few words2023 is coming. It is time to review 2022 in a professional point of view. After moving on to another project early 2022, I had the opportunity on behalf of my company Worldline, to participate as a speaker to: 8 tech conferences 2 meetups 1 online presentation at Malt Academy I also wrote 6 articles on my blog and 4 on the Worldline engineering blog. Thank you to my employer for giving me this opportunity and the organisers for their trust and their hospitality. I’m pretty sure you will recognise yourselves. Thanks then to who helped me in my speaker journey. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:3:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-in-few-words"},{"categories":null,"content":" 4 And now, something completely different?On behalf of the 2Worldline tech rel initiative, I had the chance to meet great people and move forward in some technical domains. Drawing up and submitting a talk about a technical topic requires you to dig into it and be proficient on it. I already have ideas for the next year. I hope communicating about it shortly. Best wishes for the new year eve and if you come across this article in 2023: Happy new year!! c’est comme des dev rel mais on essaye d’inclure au delà des devs (ex. les OPS, SRE). ↩︎ It is just like dev rel, but we would like to include all the tech crew (e.g., OPS \u0026 SRE) ↩︎ ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:4:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#and-now-something-completely-different"},{"categories":null,"content":"Pour ce dernier article de l’année 2022, voici un rapide retour d’expérience. Je suis actuellement en cours de préparation d’un workshop pour l’édition 2023 de SnowcampIO. J’aborderai dans ce dernier le versioning des APIs REST. Pour illustrer ce sujet ô combien épineux, j’ai réalisé une plateforme “microservices” en utilisant différents composants de la stack Spring. Container Tools Comments API Gateway Spring Cloud Gateway 2022.0.0-RC2 Bookstore API JAVA 17,Spring Boot 3.0.X ISBN API JAVA 17,Spring Boot 3.0.X Configuration Server Spring Cloud Config 2022.0.0-RC2 Database PostgreSQL Authorization Server JAVA 17,Spring Boot 3.0.X, Spring Authorization Server 1.0.0 En résumé, j’utilise Spring Boot, Cloud, Security, Authorization Server, Circuit Breaker, Spring Data,… J’ai démarré le développement avant l’annonce officielle de la version 3.0 de Spring Boot. Ce n’était pas réellement obligatoire pour cet atelier, mais j’ai souhaité quand même migrer cette application dans la dernière version de Spring Boot/Framework. Je vais décrire dans cet article comment j’ai réussi à migrer toute cette stack et les choix que j’ai fait pour que ça fonctionne. Bien évidemment, cette application n’est pas une “vraie” application en production. Par exemple, je n’ai qu’une seule entité JPA… Cependant, je la trouve représentative et espère (très modestement) que mon retour d’expérience pourra servir. La Pull Request correspondante est disponible sur GitHub. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:0:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#"},{"categories":null,"content":" 1 Pré-requisUne documentation existe. Vous pouvez la consulter ici. Il existe aussi plusieurs articles sur le blog du projet Spring. Voici un exemple. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:1:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#pré-requis"},{"categories":null,"content":" 2 Dépendances et configuration des plugins","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#dépendances-et-configuration-des-plugins"},{"categories":null,"content":" 2.1 JDKPour Spring Boot 3, il faut impérativement utiliser un JDK \u003e=17. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:1","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#jdk"},{"categories":null,"content":" 2.2 Mises à jourL’une des premières actions à réaliser est de migrer votre application vers la version 2.7. À l’heure où j’écris cet article, la version de Spring Cloud est encore en version RC. J’ai donc dû ajouter le repository “milestone” de Spring: groovy repositories { maven { url 'https://repo.spring.io/milestone' } mavenCentral() } Ensuite, j’ai utilisé les versions suivantes pour les différents composants spring: Spring Boot : 3.0.0 Spring Cloud : 2022.0.0-RC2 Spring Dependency Management : 1.1.0 Dans mon application, j’utilisais certains plugins Gradle pour la génération du code notamment OpenAPIGenerator. Pour ce dernier, j’ai ajouté un paramètre pour le rendre compatible avec spring boot 3: groovy useSpringBoot3 : \"true\" Bref, il faut impérativement tous les mettre à jour et vérifier la compatibilité ! ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:2","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#mises-à-jour"},{"categories":null,"content":" 3 Ajout de nouvelles dépendancesPour vérifier la pertinence de certaines propriétés dans la nouvelle version, Spring a mis à disposition ce plugin: groovy runtimeOnly 'org.springframework.boot:spring-boot-properties-migrator' Il permet de notifier à l’exécution si un paramètre est déprécié ou totalement inutile. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:3:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#ajout-de-nouvelles-dépendances"},{"categories":null,"content":" 4 Migration namespace javax vers jakartaeeSelon votre code, les dépendances que vous pouvez avoir, cette étape pourra aller du renommage des import javax vers jakarta à d’innombrables maux de tête. Si vous utilisez Spring Boot au-dessus d’un Tomcat (c.-à-d. en mode old school), il vous faudra mettre à jour le conteneur de servlet à une version compatible. Dans mon application, je n’ai eu qu’à modifier les imports dans les entités, filtres et méthodes annotées par l’annotation @PostConstruct(). java import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.GeneratedValue; import jakarta.persistence.GenerationType; ... Sur ce sujet, Jetbrains a publié un tutoriel sur la migration vers Jakarta. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:4:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#migration-namespace-javax-vers-jakartaee"},{"categories":null,"content":" 5 Distributed Tracing et observabilitéSpring embarque désormais plusieurs fonctionnalités liées à l’observabilité sous forme de starters. Dans mon cas, j’avais embarqué opentracing (qui était déprécié depuis quelques temps) et me connectait sur Jaeger. J’ai suivi cet article paru sur le blog de Spring. J’ai par conséquent basculé sur Zipkin (pour mon Workshop, l’utilisation du distributed tracing est un peu la cerise sur le gâteau). Voici les starters que j’ai intégrés : groovy implementation 'io.micrometer:micrometer-tracing-bridge-brave' implementation 'io.zipkin.reporter2:zipkin-reporter-brave' implementation 'io.opentelemetry:opentelemetry-exporter-zipkin' implementation 'org.springframework.boot:spring-boot-starter-aop' J’ai par la suite intégré les propriétés suivantes dans la configuration: yaml spring: zipkin: base-url: http://localhost:9411 sender: type: web management: tracing: sampling: probability: 1.0 metrics: distribution: percentiles-histogram: http: server: requests: true Je pense que j’aurai pu faire fonctionner Jaeger. Je n’ai pas voulu perdre de temps (SnowcampIO arrive bientôt…). ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:5:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#distributed-tracing-et-observabilité"},{"categories":null,"content":" 6 SecuritéJ’ai eu quelques soucis après avoir mis à jour Spring Authorization Server et Spring Security. Je pense que la version précédente de Spring était plus permissive sur l’injection et le nom des beans chargés dans les classes Configuration. J’ai donc revu la validation côté gateway et plus particulièrement la validation du jeton JWT. J’ai dû notamment ajouter le paramètre jwk-set-uri qui est obligatoire maintenant : yaml resourceserver: jwt: jwk-set-uri: http://localhost:8009 Je n’ai pas eu de réels problèmes coté Authorization Server car j’avais déjà migré vers la version 0.4.0. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:6:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#securité"},{"categories":null,"content":" 7 ConclusionVous l’aurez compris, si vous faites l’effort de suivre régulièrement les versions de Spring, vous devriez venir à bout facilement de la migration vers la dernière version de Spring. Néanmoins, sur des projets conséquents (et je ne parle pas de ceux où il n’y a de tests automatisés…) ça peut s’avérer coûteux. Certaines actions et contournements peuvent prendre du temps (ex. javax –\u003e jakarta). Enfin, je vous conseille d’attendre la première version mineure et la version définitive de Spring Cloud avant de vous lancer pour “de vrai”. Bien que Spring ait fait un effort de documentation pour la migration, il est plus sage d’attendre que les premiers correctifs soient publiés avant de vous lancer. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:7:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#conclusion"},{"categories":null,"content":"Après trois ans d’inactivité pour des raisons que l’on connait malheureusement toutes et tous, Devoxx Belgium était de retour à Anvers. Je n’avais jamais participé (en vrai) à une conférence internationale. C’était donc une première pour moi. Pour y aller, j’ai eu trois fois de la chance: J’ai eu cette opportunité grâce à Worldline - mon employeur J’ai réussi à avoir un billet pendant les cinq minutes où se sont vendus les billets lors du premier batch Ma présentation au format Quickie a été retenue. J’ai présenté un talk à Devoxx!!!!!!! Voici mon retour d’expérience des trois jours de conférence. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:0:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#"},{"categories":null,"content":" 1 Impressions généralesTout d’abord, j’ai pu assister à de nombreux Devoxx France. J’ai cru naïvement que les deux évènements se ressembleraient. Je me suis trompé. Je ne dirai pas lequel est le meilleur1. Je ne saurais le dire. On est sur un autre type de conférence. Il y a un peu moins de feedback de la communauté, même si il y a eu une présentation de Doctolib et plus de présentations réalisées par des grands acteurs du marché ou par des grands speakers internationaux (ex. Simon Ritter, Simon Brown ou James Gosling). Aussi, alors que la ligne éditoriale de Devoxx France s’est tournée au fil des années sur d’autres langages et plateformes telles que NodeJS, Go, Scala, ici on est dans du Java pur et dur. Les (très) grands speakers de l’écosystème sont présents et on fait des super talks: James Gosling, Simon Ritter, Mario Fusco, Gavin King ou José Paumard. En résumé, le coeur de la communauté Java bat à Anvers pendant une semaine. Une majorité de Java champions sont +/- présents et nous font partager leur expertise. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:1:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#impressions-générales"},{"categories":null,"content":" 2 Les tendancesLes grandes tendances étaient: L’IA et les applications Le projet Loom GraalVM ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:2:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#les-tendances"},{"categories":null,"content":" 3 Quelques conférencesL’ensemble des conférences est déjà publié sur Youtube. N’ hésitez pas à les consulter. Il y a beaucoup de talks de qualité. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#quelques-conférences"},{"categories":null,"content":" 3.1 Artificial Intelligence: You Are Here by Alan D ThompsonLe Dr Alan D. Thompson est un expert en intelligence artificielle. Il nous a donné une présentation pendant la keynote sur ce que l’ IA peut réellement faire de nos jours. C’est de plus en plus utilisé dans notre industrie au travers de Github Copilot, Codegeex,… Après nous avoir rappellé la timeline de l’adoption de l’ IA, il a illustré avec des peintures déssinées par une IA comment un ordinateur peut maintenant comprendre une phrase en langage naturel et la traduire en image. Il a également présenté le langage de modélisation GPT3. Vous pouvez trouver la vidéo ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:1","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#artificial-intelligence-you-are-here-by-alan-d-thompson"},{"categories":null,"content":" 4 Revolutionizing Java-Based Applications with GraalVM by Alina Yurenko and Thomas WuerthingerDans cette présentation, les présentateurs d’Oracle ont abordé une autre grande tendance du marché: le retour au natif qui permet de limiter l’impact sur le démarrage, la mémoire et la taille des packages. Au travers d’un exemple basé sur Micronaut, ils ont expliqué comment GraalVM peut répondre à ces enjeux. Ils ont également démystifié plusieurs mythes liés à GraalVM. Par exemple, GraalVM supporte la réflexion (… et parfois non). On peut utiliser également [Java Flight Recorder](Java Flight Recorder) pendant la compilation. Le support à l’exécution des applications est bientôt prévu. La developer experience était également à l’ordre du jour. Comment offrir une bonne expérience alors que la compilation prend plus de temps? Pour répondre à cette épineuse question, ils ont conseillé de gardé le mode JIT avec une JVM pendant le développement et d’utiliser l’ AOT pour le déploiement final. Ceci permet de disposer d’une machine puissante et de garantir la compatibilité matérielle et OS de la machine de production. La vidéo est disponible ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:4:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#revolutionizing-java-based-applications-with-graalvm-by-alina-yurenko-and-thomas-wuerthinger"},{"categories":null,"content":" 5 The lost art of software design by Simon BrownJ’utilise le modèle C4 depuis plusieurs années. Je l’ai même présenté très brièvement dans mon talk. Aussi, j’ai été très impressionné quand j’ai pu assister à la présentation de Simon Brown sur la conception logicielle. Il a expliqué pourquoi la conception n’était pas conflictuelle avec les méthodes agiles. Ca permet notamment d’ identtifier et de gérer les risques. Au delà du modèle C4, il a montré comment identifier et évaluer les différents risques avec le “Risk Storming”. Enfin, il a répondu à la question à un million d’euros: “Quand arrêter la conception?” Vous trouverez la réponse ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-lost-art-of-software-design-by-simon-brown"},{"categories":null,"content":" 5.1 Ahead Of Time and Native in Spring Boot 3.0 by Stéphane Nicoll \u0026 Brian ClozelUne autre conférence qui met en avant GraalVM! Cette fois on abordait le support de l’ AOT et du mode natif dans la future version de Spring Boot. Les présentateurs ont expliqués comment Spring supportait le mode natif: le processus appliqué, la gestion des métadonnées et l’analyse réalisée. En (très très bref) résumé, l’ AOT génère des sources dont le chargement des Bean Definition. Ils ont également pointé du doigt des changements que je considère bloquants: On ne peut pas utiliser changer les profils au runtime La surcharge des propriétés et variable d’environnement n’est pas possible à l’exécution On ne peut pas utiliser de Java Agent. Pour ce dernier point, cela risque de poser de nombreux soucis que ça soit l’utilisation d’un APM tel que Dynatrace ou le support de l’AOP. A la fin de cette présentation, ils ont donné quelques recommendations. Parmi celles-ci: Exécuter en développement l’application en mode AOT avec une JVM Exécuter les tests en mode natif Vous trouverez la vidéo ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:1","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#ahead-of-time-and-native-in-spring-boot-30-by-stéphane-nicoll--brian-clozel"},{"categories":null,"content":" 5.2 The Art of Java Language Pattern Matching by Simon RitterSimon Ritter a exploré toutes les possibilité du pattern matching en Java. Toutes les fonctionnalités ne sont pas encore disponibles. On peut néanmoins faire beaucoup de choses. Après un rappel sur les nouvelles fonctionnalités depuis le JDK11 (Sealed classes, Records), Simon Ritter a illustré leur utilisation dans ce contexte. Si vous voulez tout connaître sur cette fonctionnalité, je vous conseille fortement de regarder ce talk. Voici la vidéo ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:2","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-art-of-java-language-pattern-matching-by-simon-ritter"},{"categories":null,"content":" 6 ConclusionVoila les quelques talks qui m’ont interpellé. Il y en a beaucoup d’autres tels que ceux de Julien TOPCU ou Marcy ERICKA CHARELLOIS. Cette première participation était très enrichissante. J’ai eu à plusieurs reprises l’impression d’avoir l’information à la source (ex. pour Spring). Si j’ai autant de chance, je pense rééditer l’expérience l’année prochaine. En tant que speaker pour une conférence ou un workshop? Seul l’avenir nous le dira! Vous savez, moi je ne crois pas qu’il y aient de bonnes ou mauvaises conférences. Moi, si je devais résumer ma vie aujourd’hui avec vous, je dirais que c’est d’abord des rencontres, des gens qui m’ont tendu la main… ↩︎ ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:6:0","series":null,"tags":["conference","java"],"title":"Ma première participation à Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#conclusion"},{"categories":null,"content":"Dans mon dernier article, j’ai tenté de faire un état des lieux des solutions possibles pour implémenter des batchs cloud natifs. J’ai par la suite testé plus en détails les jobs et cron jobs Kubernetes en essayant d’avoir une vue OPS sur ce sujet. Le principal inconvénient (qui ne l’est pas dans certains cas) des jobs est qu’on ne peut pas les rejouer. Si ces derniers sont terminés avec succès - Vous allez me dire, il faut bien les coder - mais qu’on souhaite les rejouer pour diverses raisons, on doit les supprimer et relancer. J’ai vu plusieurs posts sur StackOverflow à ce sujet, je n’ai pas trouvé de solutions satisfaisantes relatifs à ce sujet. Attention, je ne dis pas que les jobs et cron jobs ne doivent pas être utilisés. Loin de là. Je pense que si vous avez besoin d’un traitement sans chaînage d’actions, sans rejeu, les jobs et cron jobs sont de bonnes options. Le monitoring et reporting des actions réalisées peut se faire par l’observabilité mise en place dans votre cluster K8S. Après plusieurs recherches, je suis tombé sur Spring Data Flow. L’offre de ce module de Spring Cloud va au delà des batchs. Il permet notamment de gérer le streaming via une interface graphique ou via son API. Dans cet article, je vais implémenter un exemple et le déployer dans Minikube. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:0:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#"},{"categories":null,"content":" 1 Installation et configuration de MinikubeL’installation de minikube est décrite sur le site officiel. Pour l’installer, j’ai exécuté les commandes suivantes: bash curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Au premier démarrage, vous finirez l’installation bash minikube start ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-et-configuration-de-minikube"},{"categories":null,"content":" 1.1 Installation de Spring Cloud Data FlowPour installer Spring Cloud Data Flow directement dans Kubernetes, vous pouvez exécuter les commandes suivantes: bash helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release bitnami/spring-cloud-dataflow Après quelques minutes de téléchargement, vous devriez avoir le retour suivante à l’exécution de la commande kubectl get pods bash kubectl get pods ~ » kubectl get pods NAME READY STATUS RESTARTS AGE dataflow-mariadb-0 1/1 Running 1 (24h ago) 24h dataflow-rabbitmq-0 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-server-75db59d6cb-lrwp8 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-skipper-9db568cf4-rzsqq 1/1 Running 1 (24h ago) 24h ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-de-spring-cloud-data-flow"},{"categories":null,"content":" 1.2 Accès au dashboardPour accéder au dashboard de Spring Cloud Data Flow, vous pouvez lancer les commandes suivantes: bash export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services dataflow-spring-cloud-dataflow-server) kubectl port-forward --namespace default svc/dataflow-spring-cloud-dataflow-server ${SERVICE_PORT}:${SERVICE_PORT} Ensuite, vous pourrez accéder à la console web via l’URL http://localhost:8080/dashboard. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#accès-au-dashboard"},{"categories":null,"content":" 2 Développement d’une TaskJ’ai crée une simple task qui va rechercher la nationalité d’un prénom. Pour ceci, j’utilise l’API https://api.nationalize.io/. On passe un prénom en paramètre et on obtient une liste de nationalités possibles avec leurs probabilités. Vous trouverez les sources de cet exemple sur mon Github. Aussi, la documentation est bien faite, il suffit de la lire. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#développement-dune-task"},{"categories":null,"content":" 2.1 InitialisationJ’ai initié un projet Spring avec les dépendances suivantes: groovy dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.cloud:spring-cloud-starter-task' developmentOnly 'org.springframework.boot:spring-boot-devtools' testImplementation 'org.springframework.boot:spring-boot-starter-test' implementation 'org.springframework.boot:spring-boot-starter-jdbc' runtimeOnly 'org.mariadb.jdbc:mariadb-java-client' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } Attention, les starters et dépendances JDBC/MariaDB sont indispensables pour que votre tâche puisse enregistrer le statut des exécutions. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#initialisation"},{"categories":null,"content":" 2.2 Construction de la tâcheUne tâche se crée facilement en annotation une classe “Configuration” par l’annotation @EnableTask java @Configuration @EnableTask public class TaskConfiguration { ... } Ensuite l’essentiel du job s’effectue dans la construction d’un bean CommandLineRunner : java @Bean public CommandLineRunner createCommandLineRunner(RestTemplate restTemplate) { return args -\u003e { var commandLinePropertySource = new SimpleCommandLinePropertySource(args); var entity = restTemplate.getForEntity(\"https://api.nationalize.io/?name=\" + Optional.ofNullable(commandLinePropertySource.getProperty(\"name\")).orElse(\"BLANK\"), NationalizeResponseDTO.class); LOGGER.info(\"RESPONSE[{}]: {}\", entity.getStatusCode(), entity.getBody()); }; } Dans mon exemple, j’affiche dans la sortie standard le payload de l’API ainsi que le code HTTP de la réponse. Voici un exemple d’exécution : bash 2022-08-12 15:11:07.885 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2022-08-12 15:11:07.894 INFO 1 --- [ main] i.t.b.cloudtask.CloudTaskApplication : Started CloudTaskApplication in 17.704 seconds (JVM running for 19.18) 2022-08-12 15:11:10.722 INFO 1 --- [ main] i.t.batch.cloudtask.TaskConfiguration : RESPONSE[200 OK]: NationalizeResponseDTO{name='Alexandre', countries=[CountryDTO{countryId='BR', pr.... ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#construction-de-la-tâche"},{"categories":null,"content":" 2.3 PackagingIci rien de nouveau, il suffit de lancer la commande: bash ./gradlew build ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#packaging"},{"categories":null,"content":" 3 Déploiement","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#déploiement"},{"categories":null,"content":" 3.1 Création et déploiement de l’image DockerPour déployer notre toute nouvelle tâche, nous allons d’abord créer l’image Docker avec buildpack. Tout d’abord on va se brancher sur minikube pour que notre image soit déployée dans le repository de minikube bash eval $(minikube docker-env) Ensuite, il nous reste à créer l’image Docker bash ./gradlew bootBuildImage --imageName=info.touret/cloud-task:latest Pour vérifier que votre image est bien présente dans minikube, vous pouvez exécuter la commande suivante: bash minikube image ls | grep cloud-task info.touret/cloud-task:latest ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-et-déploiement-de-limage-docker"},{"categories":null,"content":" 3.2 Création de l’applicationAvant de créer la tâche dans l’interface, il faut d’abord référencer l’image Docker en créer une application: Il faut déclarer l’image Docker avec le formalisme présenté dans la capture d’écran. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-de-lapplication"},{"categories":null,"content":" 3.3 Création de la tâcheVoici les différentes actions que j’ai réalisé via l’interface: Vous trouverez plus de détails dans la documentation officielle. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#création-de-la-tâche"},{"categories":null,"content":" 4 ExécutionMaintenant, il nous est possible de lancer notre tâche. Vous trouverez dans les copies d’écran ci-dessous les différentes actions que j’ai réalisé pour exécuter ma toute nouvelle tâche. J’ai pu également accéder aux logs. Il est également important de noter qu’ après l’exécution d’une tâche, le POD est toujours au statut RUNNING afin que Kubernetes ne redémarre pas automatiquement le traitement. bash kubectl get pods | grep cloud-task cloud-task-7mp72gzpwo 1/1 Running 0 57m cloud-task-pymdkr182p 1/1 Running 0 65m A chaque exécution il y aura donc un pod d’alloué. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:4:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#exécution"},{"categories":null,"content":" 5 Aller plus loinParmi les fonctionnalités que j’ai découvert, on peut : relancer un traitement le programmer nettoyer les exécutions les pistes d’audit le chaînage des différentes tâches Gros inconvénient pour le nettoyage: je n’ai pas constaté un impact dans les pods alloués. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:5:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#aller-plus-loin"},{"categories":null,"content":" 6 ConclusionPour résumer, je vais me risquer à comparer les deux solutions jobs/cron jobs Kubernetes et une solution basée sur Spring Cloud Dataflow. Je vais donc utiliser la liste des caractéristiques présentée par M. Richards et N. Ford dans leur livre: Fundamentals of Software Architecture1. Bien évidemment, cette notation est purement personnelle. Vous noterez que selon où on positionne le curseur, l’une des deux solutions peut s’avérer meilleure (ou pas). Bref, tout dépend de vos contraintes et de ce que vous souhaitez en faire. A mon avis, une solution telle que Spring Cloud Dataflow s’inscrit parfaitement pour des traitements mixtes (streaming, batch) et pour des traitements Big Data. N’hésitez pas à me donner votre avis (sans troller svp) en commentaire ou si ça concerne l’exemple, directement dans Github. Architecture characteristic K8s job rating Spring Cloud Dataflow rating Partitioning type Domain \u0026 technical Domain \u0026 technical Number of quanta 2 1 1 to many Deployability ⭐⭐⭐⭐ ⭐⭐⭐⭐ Elasticity ⭐⭐⭐ ⭐⭐⭐⭐ Evolutionary ⭐⭐⭐ ⭐⭐⭐⭐ Fault Tolerance ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ Modularity ⭐⭐⭐ ⭐⭐⭐⭐⭐ Overall cost ⭐⭐⭐⭐ ⭐⭐⭐ Performance ⭐⭐⭐⭐⭐ ⭐⭐⭐ Reliability ⭐⭐⭐⭐ ⭐⭐⭐ Scalability ⭐⭐⭐⭐ ⭐⭐⭐⭐ Simplicity ⭐⭐⭐⭐⭐ ⭐⭐⭐ Testability ⭐⭐⭐ ⭐⭐⭐⭐ A lire absolument! ↩︎ ~ Nombre de livrables indépendants fortement couplés ↩︎ ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:6:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"Déployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#conclusion"},{"categories":null,"content":"I have been speaking in Tech conferences from 2018. Below the conferences I presented so far. If you want to know more about them, you can see the descriptions on the talks page of my blog. ","date":"2022-07-02","objectID":"/speaker-resume/:0:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#"},{"categories":null,"content":" 1 2025 SnowcampIO 🇫🇷 : L’observabilité dès le développement: Maîtrisez vos applications Java en production avec Open Telemetry SnowcampIO 🇫🇷 : Intégrons, faisons grandir et progresser les jeunes devs: Bonnes pratiques et retours d’expériences à l’intention des (vieux) devs ","date":"2022-07-02","objectID":"/speaker-resume/:1:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2025"},{"categories":null,"content":" 2 2024 APIDAYS Paris 2024 🇬🇧 : Implementing API-First Approach: Practical Insights for Streamlining Your APIs VolcampIO 🇫🇷 : Une plateforme à concevoir, deux architectes: trois possibilités? Devoxx Morocco 🇫🇷 : Intégrons, faisons grandir et progresser les jeunes devs: Bonnes pratiques et retours d’expériences à l’intention des (vieux) devs RivieraDev 🇫🇷 : Apprenons à identifier les exigences techniques pour mieux concevoir RivieraDev 🇫🇷 : Intégrons, faisons grandir et progresser les jeunes devs: Bonnes pratiques et retours d’expériences à l’intention des (vieux) devs RivieraDev 🇫🇷 : L’observabilité dès le développement: Maîtrisez vos applications Java en production avec Grafana Worldline Techforum 🇬🇧 Make your Java application be truly observable with OpenTelemetry Geecon Krakow 🇬🇧 : Let’s Learn to Identify Technical Requirements for Better Design Poitou-Charentes JUG 🇫🇷 : Le versionning des APIs REST: dans la vraie vie on fait comment ? Bordeaux JUG 🇫🇷 Le versionning des APIs REST: dans la vraie vie on fait comment ? Touraine Tech 🇫🇷 : Apprenons à identifier les exigences techniques pour mieux concevoir NDC LONDON 🇬🇧 : REST API Versioning: Hands on! ","date":"2022-07-02","objectID":"/speaker-resume/:2:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2024"},{"categories":null,"content":" 3 2023 APIDAYS Paris 2023 🇬🇧 Real-Life REST API Versioning Strategies \u0026 Best Practices AFUP Tours 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Codeurs en Seine 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Codeurs en Seine 🇫🇷 : Le versionning des APIs REST par la pratique RivieraDev 🇫🇷 : Le versionning des APIs REST par la pratique DEVFEST LILLE 🇫🇷 : Le versionning des APIs REST: dans la vraie vie on fait comment ? VOXXED DAYS BRUSSELS 🇬🇧 : The hitchhiker’s guide to software architecture design TADX 🇫🇷 Une plateforme à concevoir, deux architectes: trois possibilités? Lyon JUG 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture SnowcampIO 🇫🇷 : Le versionning des APIs REST par la pratique SnowcampIO 🇫🇷 : Une plateforme à concevoir, deux architectes: trois possibilités? ","date":"2022-07-02","objectID":"/speaker-resume/:3:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2023"},{"categories":null,"content":" 4 2022 Paris JUG 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Devoxx BE 🇬🇧 : [Architecture Katas : Improve your system architecture design skills in a funny way !] Cloud Nord 🇫🇷 : Java dans le Cloud: avec Spring ou Quarkus Jug Summer Camp 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Voxxed Days Luxembourg 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Breizhcamp 🇫🇷 : Java dans le Cloud: avec Spring ou Quarkus Camping des Speakers 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture Nantes JUG 🇫🇷 Améliorer les compétences et les infrastructures avec les katas d’architecture Nantes JUG 🇫🇷 Checklist pour concevoir une application dans le cloud : 10 conseils à l’attention des concepteurs et architectes SnowcampIO 🇫🇷 : Checklist pour concevoir une application dans le cloud Touraine Tech 🇫🇷 : Java dans le Cloud: avec Spring ou Quarkus ","date":"2022-07-02","objectID":"/speaker-resume/:4:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2022"},{"categories":null,"content":" 5 2021 Devoxx UK 🇬🇧 : Kubernetes \u0026 Co, beyond the hype: 10 tips for designers who want to create cloud native apps Devoxx UK 🇬🇧 : Architecture Katas : Improve your system architecture design skills in a funny way ! Volcamp IO 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture JNation 🇬🇧 : Kubernetes \u0026 Co, beyond the hype: 10 tips for designers who want to create cloud native apps The Cloud First 🇫🇷 : Améliorer les compétences et les infrastructures avec les katas d’architecture The Cloud First 🇫🇷 : Checklist pour concevoir une application dans le cloud ","date":"2022-07-02","objectID":"/speaker-resume/:5:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2021"},{"categories":null,"content":" 6 2020 Worldline Techforum Iberia 🇬🇧 : Java Next Steps ","date":"2022-07-02","objectID":"/speaker-resume/:6:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2020"},{"categories":null,"content":" 7 2019 Orleans Tech 🇫🇷 : “Unifiez vos traitements Batch et Streaming avec Apache Beam” Worldline Techforum 🇬🇧 : Java Next Steps Touraine Tech 🇫🇷 Architecture Hands-on ","date":"2022-07-02","objectID":"/speaker-resume/:7:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2019"},{"categories":null,"content":" 8 2018 Touraine Tech 🇫🇷 :Jenkins 2 : Le retour (d’expérience) ","date":"2022-07-02","objectID":"/speaker-resume/:8:0","series":null,"tags":null,"title":"Speaker Resume","uri":"/speaker-resume/#2018"},{"categories":null,"content":" Talks You can find below the slides and videos of my latest talks in French and in English. I held most of them in both two languages. You can also get my slides on SpeakerDeck and videos on this Youtube playlist: ","date":"2022-07-02","objectID":"/talks/:0:0","series":null,"tags":null,"title":"Talks","uri":"/talks/#"},{"categories":null,"content":" 1 TalksHere is a bunch of talks I presented at various tech events (meetups, conferences). You can find them either in French or English. ","date":"2022-07-02","objectID":"/talks/:1:0","series":null,"tags":null,"title":"Talks","uri":"/talks/#talks"},{"categories":null,"content":" 1.1 Implementing API-First Approach: Practical Insights for Streamlining Your APIs Abstract We often oppose Code-first to API-first approaches. Most of the coders prefer the first one because it is more code centric, and the documentation is automatically generated. In fact, the latter is often perceived by them as a “boring-documentation-first” development. However, the API-first design and implementation offer many benefits: streamlining your APIs, tackling the complexity and most of all, boosting the developer experience of your customers. Through practical feedback we will cover how to implement API-first Approaches with the associated practices and tooling. We will then pinpoint the main difficulties to struggle with. We will therefore see how to tackle this huge challenge: improve the quality of your API, increasing the adoption of your APIs by your customers, and most importantly, make your developers love the Documentation Code First approach! Slides ","date":"2022-07-02","objectID":"/talks/:1:1","series":null,"tags":null,"title":"Talks","uri":"/talks/#implementing-api-first-approach-practical-insights-for-streamlining-your-apis"},{"categories":null,"content":" 1.2 Intégrons, faisons grandir et progresser les jeunes devs: Bonnes pratiques et retours d’expériences à l’intention des (vieux) devs. Résumé Êtes-vous confronté au défi d’encadrer et de développer les compétences des jeunes développeurs ? En tant que lead développeur ou développeur expérimenté, vous êtes souvent amené à guider techniquement de nouveaux talents. Mais quelle approche adopter pour assurer leur progression de manière optimale ? Je participe depuis plusieurs années à différentes initiatives de mentorat et d’accompagnement de jeunes développeurs. Je vous propose donc de partager les bonnes pratiques que j’ai pu mettre en œuvre ainsi que les écueils que j’ai rencontré. Nous saurons donc à la fin de cette présentation comment intégrer et rendre autonomes les nouveaux arrivant.es le plus efficacement possible. Slides Video ","date":"2022-07-02","objectID":"/talks/:1:2","series":null,"tags":null,"title":"Talks","uri":"/talks/#intégrons-faisons-grandir-et-progresser-les-jeunes-devs-bonnes-pratiques-et-retours-dexpériences-à-lintention-des-vieux-devs"},{"categories":null,"content":" 1.3 L’observabilité dès le développement: Maîtrisez vos applications Java en production avec Grafana Résumé Imaginez, c’est vendredi après-midi. Vous êtes impatient de profiter de votre week-end quand un ingénieur Ops vous transfère un problème bloquant avec une erreur HTTP 500. Il n’a en effet, pas réussi à identifier l’origine en raison d’un manque d’informations contextuelles. Après quelques heures, vous parvenez enfin à l’identifier en le reproduisant et en le déboguant sur votre ordinateur. Si vous avez l’habitude de faire face à de tels problèmes, cela signifie sûrement que vous avez négligé le développement d’une des fonctionnalités les plus importantes de votre application : l’observabilité ! Au cours de cet atelier, nous implémenterons des bonnes pratiques de logging et mettrons en oeuvre toute une stack d’observabilité: des librairies à la plateforme en se basant sur les produits Grafana (Tempo, Loki, Prometheus). Nous saurons donc comment adapter nos applications pour la production et faire des Ops nos meilleurs amis! Workshop ","date":"2022-07-02","objectID":"/talks/:1:3","series":null,"tags":null,"title":"Talks","uri":"/talks/#lobservabilité-dès-le-développement-maîtrisez-vos-applications-java-en-production-avec-grafana"},{"categories":null,"content":" 1.4 Let’s Learn to Identify Technical Requirements for Better Design Abstract Have you ever heard phrases like “it must work 24/7,” “I want 100% availability,” only to end up with “in reality, a VM will be more than sufficient”? Or conversely, “No SLA, my platform is not critical, it just needs to run precisely at 6:54 AM on the first day of the month”? If these situations sound familiar, don’t miss out! Whether these Non-Functional Requirements are explicit or not, they are the keystone of any architecture aligned with client needs. Drawing on two fictional examples (any resemblance to reality is purely coincidental, or maybe not), we will explore how to navigate the pitfalls of overengineering and establish a pragmatic approach to identifying the right architecture for the right business need. By the end of this presentation, we’ll know how to identify those elusive NFRs that will help us design better architectures while avoiding unnecessary complexity! Slides Video ","date":"2022-07-02","objectID":"/talks/:1:4","series":null,"tags":null,"title":"Talks","uri":"/talks/#lets-learn-to-identify-technical-requirements-for-better-design"},{"categories":null,"content":" 1.5 The Hitchhiker’s guide to software architecture design Abstract Designing a new platform is always tricky to set up. How to start? What is the best strategy to adopt while designing a platform? What kind of architecture should we deploy: event streaming, orchestration, or choreography? For a brand-new platform: “Donuts @ Home”, we will proceed a live architecture study. After analysing the customer needs, brainstorming, and exchanging our ideas, we will choose among all the potential solutions the least worst option. You will be asked to validate our design and the different implementation examples. At the end of this talk, you will have tips and tricks for thinking about it and starting working on architecture studies in complete peace of mind. Slides Video ","date":"2022-07-02","objectID":"/talks/:1:5","series":null,"tags":null,"title":"Talks","uri":"/talks/#the-hitchhikers-guide-to-software-architecture-design"},{"categories":null,"content":" 1.6 Real-life REST API Versioning for dummies: Strategies and Best Practices Abstract When we want to publish APIs, for instance using an API Management solution, we regularly think about versioning. This practice meets project management needs but brings a lot of complexity. Imagine, you work on a platform which exposes APIs to many customers. You must bring new features while controlling the existing ones. Which strategy to adopt? What are the potential technical tools and practices we could implement easily? During this presentation, you will unlock the secrets of API versioning I have put in place and helped me during my last projects. Through a real-life use case based on a microservices architecture, we will define the best strategy to put in place, the other potential ones and their constraints. At the end of this presentation, we will have the big picture on the diverse ways of APIs versioning. Slides Vidéo ","date":"2022-07-02","objectID":"/talks/:1:6","series":null,"tags":null,"title":"Talks","uri":"/talks/#real-life-rest-api-versioning-for-dummies-strategies-and-best-practices"},{"categories":null,"content":" 1.7 REST APIs versioning Hands On! Abstract When we want to publish APIs, for instance using an API (Application Programmable Interface) Management solution, we regularly think about versioning. This practice meets project management needs but brings a lot of complexity. Imagine, you work on a platform which exposes APIs to many customers. You must bring new features while controlling the existing ones. How to deliver new functionalities to specific customers without affecting the others? Throughout this workshop you will (re)discover and apply advice I have put in place and helped me during my last projects. Through a real-life use case based on a microservices architecture, we will define the best strategy to put in place, the other potential ones and their constraints. We will challenge them implementing new features for a new customer in this application. At the end of this workshop, we will have the big picture and put into practice diverse ways of APIs versioning. Slides Workshop ","date":"2022-07-02","objectID":"/talks/:1:7","series":null,"tags":null,"title":"Talks","uri":"/talks/#rest-apis-versioning-hands-on"},{"categories":null,"content":" 1.8 Architecture Katas: Improve your system architecture design skills in a funny way Abstract How to learn architecture ? How to improve in this field ? How do we recognize a good or a bad architecture ? Plenty of books and training sessions address this subject. The best thing is to practice! In the same way as CodingDojos, I will present to you architecture katas. Ted NEWARD created them. His idea came from the following observation : “How are we supposed to get great architects, if they only get the chance to architect fewer than a half-dozen times in their career?” One solution to this issue could be to practice regularly on several topics to gain experience. After a brief introduction of how to start designing an architecture, I will present architecture katas, and the conduct. To conclude, after this first try I will present the benefits I had benefited. Slides Video ","date":"2022-07-02","objectID":"/talks/:1:8","series":null,"tags":null,"title":"Talks","uri":"/talks/#architecture-katas-improve-your-system-architecture-design-skills-in-a-funny-way"},{"categories":null,"content":" 1.9 Java dans le cloud : avec Spring ou Quarkus ? Résumé Né au début des années 2000, le framework Spring a mis en avant la facilité de développement. Plus récemment, il a su s’adapter aux contraintes techniques liées aux applications cloud-natives. De son coté, Quarkus est plus récent, il est né en 2019 sur la base de MicroProfile avec un objectif clair : tirer le meilleur parti des plateformes Kubernetes en se concentrant sur un démarrage rapide et une faible empreinte mémoire. Nous avons donc un champion et un outsider ! Maintenant lequel choisir en fonction de vos besoins et votre contexte ? Spring ou Quarkus ? Au travers d’une démonstration “live” nous présenterons un cas contret basé sur notre expérience. Ce dernier sera implémenté avec chacune des deux stacks. Enfin, nous vous mettrons à contribution au travers de sondages pour dynamiser ensemble notre réflexion. Slides Vidéo ","date":"2022-07-02","objectID":"/talks/:1:9","series":null,"tags":null,"title":"Talks","uri":"/talks/#java-dans-le-cloud--avec-spring-ou-quarkus-"},{"categories":null,"content":" 1.10 Kubernetes \u0026 Co, beyond the hype: 10 tips for designers who want to create cloud native apps Abstract Kubernetes and cloud technologies are nowadays the new standard to deploy different cloud native applications: API, BATCHES, microservices and … monoliths! These technologies help to solve many issues but with some complexity. It could be difficult for developers and designers to identify the constraints of such architectures. In this presentation, you will (re)discover ten tips and pieces of advice I applied and found useful in my last JAVA projects (Spring, JEE). I will talk about: Application ecosystem Choice of technical solutions Development K8S design constraints And more! Slides Video ","date":"2022-07-02","objectID":"/talks/:1:10","series":null,"tags":null,"title":"Talks","uri":"/talks/#kubernetes--co-beyond-the-hype-10-tips-for-designers-who-want-to-create-cloud-native-apps"},{"categories":null,"content":"Au début de l’année, j’ai interpelé mes managers: “J’ai été retenu au Camping des Speakers”! Unanimement, j’ai eu la même réponse: “Le QUOI ???!!!” Oui j’en conviens le titre peut paraître à première vue tout sauf sérieux. Il faut dire que le lieu est un camping en Bretagne dans le golfe du Morbihan. Et pourtant ! La programmation était de qualité et a tenu ses promesses. Je vais essayer de retranscrire quelques sujets qui ont retenu mon attention. J’en oublierai sans doute beaucoup. Je m’en excuse d’avance. Tout d’abord, l’intérêt de cette conférence réside, selon moi, au-delà des talks donnés. Le cadre atypique (un camping dans le Morbihan), des conférences slideless données en extérieur en mode “feu de camp” a eu le même effet, enfin il me semble, sur tous les participantes et participants qu’ils soient speakers ou non. Chose originale que j’ai pu constater lors de ces talks, on passait beaucoup de temps à échanger avec les speakers pendant leur présentation. C’était génial pour les spectateurs, peut-être un peu moins pour la gestion du timing :). Vous l’aurez compris. L’environnement mis en place par les organisateurs permettait de réels échanges… Et on avait le temps de le faire. Ensuite, la programmation donnait la part belle à des sujets non techniques. Par exemple, j’ai enfin pu voir la conférence de mon ancienne collègue Fanny Klauk : “Rendez l’agilité aux développeur(se)s !” qui, au travers d’une histoire dont est le héros, a mis en évidence les travers des (grosses) organisations. Les solutions sont “naturelles” dans la théorie, mais permettent de donner plus d’agilité et de sens aux développeurs. Un autre sujet “non technique” m’a très intéressé, c’était “Le voyage du héros de l’IT” d’ Olivier Beautier. Après nous avoir présenté le guide du scénariste de C. VOGLER, on a construit une histoire d’une développeuse. Au travers de cette histoire, on a pu voir entre autres l’importance du mentorat dans notre métier. Puis, j’ai vu la conférence d’A. PENA “Java à la vitesse de la lumière grâce au Graal !”. Il a présenté GraalVM, une comparaison avec HotSpot et les perspectives d’évolution de cette JVM. Nul doute que le mode natif sera de plus en plus utilisé dans les prochaines années. Ensuite, avec D. APARICIO, nous avons pu voir les travers d’une (mauvaise) modélisation de données au travers de son retour d’expérience “Au secours 😨! J’ai un homonyme !! “ Il a illustré l’importance de bien modéliser pour ne pas avoir que le nom et prénom comme identifiants dans un système par ex. ainsi que les impacts dans la vie courante. Vous l’aurez compris, il y avait beaucoup de conférences intéressantes qui donnaient lieu à des discussions tout aussi passionnantes. Last but not least, j’ai eu la chance de présenter les katas d’architecture, comment les mettre en œuvre et quels sont les bénéfices qu’on peut en tirer. Je tiens à remercier Julien TOPCU et Craft Records pour l’accompagnement dans la préparation. Vous pourrez trouver les slides sur ma page speackerdeck. Pour finir, je conseille cette expérience à toutes les personnes qui veulent apprendre, échanger dans un environnement convivial et zen. Si vous n’êtes toujours pas convaincu, sachez qu’il y a également des mashmallows. Avec cet argument massue, on devrait convaincre les plus sceptiques! ;) Camping des speakers spirit Merci encore aux organisatrices et organisateurs pour ces deux jours. À l’année prochaine !! ","date":"2022-06-10","objectID":"/2022/06/10/camping-speakers-2022/:0:0","series":null,"tags":["conference"],"title":"Mon Camping des Speakers","uri":"/2022/06/10/camping-speakers-2022/#"},{"categories":null,"content":"Quand on parle du Cloud et de Kubernetes, généralement on pense aux APIs. Mais qu’en est-il des batchs? Oui, depuis plusieurs années, on pensait les éradiquer, mais ils sont encore là et on en a encore besoin pour quelques années encore. Ils ont même eu une deuxième jeunesse avec le Big Data et l’explosion des volumétries dans l’IT. Je vais essayer de faire un tour d’horizon dans cet article des batchs dans un environnement Cloud et plus particulièrement dans Kubernetes. Les exemples présentés dans cet article seront (sans doute) approfondis dans un second article et d’ores et déjà disponibles dans mon GitHub. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:0:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#"},{"categories":null,"content":" 1 Pourquoi des batchs dans le Cloud?A ce titre un peu provocateur, j’ajouterais aussi “Pourquoi des batchs dans Kubernetes ?”. Oui, aujourd’hui encore,comme j’ai pu l’indiquer précédemment, on doit créer des traitements batchs. A coté des APIs qui représentent le cas d’utilisation “standard” du Cloud, on peut également avoir à traiter des fichiers volumineux allant de plusieurs centaines de Mo à quelques Go. Parmi les cas d’utilisation qui nécessitent ce genre de traitement, on pourra avoir: Les reprises de données (suite à des erreurs ou lors d’une initialisation) Traitement suite à une réception de fichiers (par ex. traitement de fichiers OPENDATA) Si vous êtes déjà passé sur le Cloud pour vos applications transactionnelles, vous vous poserez cette question: Puis-je également déployer des batchs? ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-des-batchs-dans-le-cloud"},{"categories":null,"content":" 1.1 Pourquoi se poser cette question?Les réponses sont multiples. Elles sont tout d’abord liées à une rationalisation des environnements. Vous avez votre application dans le cloud, votre base de données y est également gérée pour éviter la latence réseau. Vous devez donc déployer des traitements tiers au plus proche de celle-ci pour vous soustraire des mêmes soucis. De plus, l’écosystème lié au cloud offre des technologies et pratiques qui rendent la vie plus simple (si, si, je vous assure) aux développeurs et ops. Le déploiement via l’Infra As Code est un bon exemple : Avoir toute l’infrastructure liée aux traitements batchs et transactionnels versionnées et instantiables à la demande est quelque chose dont on a du mal à se passer! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-se-poser-cette-question"},{"categories":null,"content":" 2 Difficulté(s) par rapport aux APIsQuand on déploie une API dans le cloud, généralement tout va bien. On peut voir rapidement que cet environnement convient bien à ce genre de traitements. Pour les batchs, c’est une autre affaire! Selon les sociétés, il peut y avoir un fort historique et beaucoup plus d’exigences que pour les APIs. Ces dernières pourront être liées aux performances, à la qualité de service ou plus simplement à l’utilisation. Il faut donc, à l’instar de toute architecture, déterminer quel sera l’environnement technique de ce type de traitement. Cette fois, on aura à concilier performances, fichiers volumineux et reprises sur erreur. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#difficultés-par-rapport-aux-apis"},{"categories":null,"content":" 2.1 Quelques technologiesOn pourra retrouver dans notre future architecture les briques suivantes: Une passerelle de fichiers (File Gateway) pour permettre l’envoi des fichiers de manière sécurisée Un stockage objet pour la distribution de fichiers ou l’archivage. Les éléments nécessaires à l’API : bases de données, HSMs, Cluster Kubernetes,… ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#quelques-technologies"},{"categories":null,"content":" 3 Modes de déclenchementSi on regarde de plus près les exigences techniques liées aux cas d’utilisation, on pourrait résumer les différents modes de déclenchement de la manière suivante: Traitement sur réception de fichiers Traitement déclenché par un ordonnanceur/orchestrateur centralisé (ex. https://dkron.io/) de manière régulière ou non. Traitement déclenché par CRON (qui est un ordonnanceur, mais un peu plus roots) J’ai volontairement exclu les traitements sur présence de messages (ex. Kafka). Je les considère plus liés au monde transactionnel. Dans les paragraphes suivants, je vais décrire des solutions d’architecture qui permettent de déployer ces traitements dans Kubernetes. J’aborderai sans doute un exemple dans un autre article ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:3:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#modes-de-déclenchement"},{"categories":null,"content":" 4 ContraintesDès qu’on s’aventure dans ce type de conception, nous aurons, au-delà des 12 factors, les contraintes suivantes à traiter: ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#contraintes"},{"categories":null,"content":" 4.1 Gestion des erreurs et indisponibilitésDans un cluster Kubernetes, le crash d’un POD n’est pas rédhibitoire. Le cluster permet de redémarrer immédiatement une autre instance. Pour les APIs, ce n’est pas un problème. Pour les batchs, c’est une autre paire de manches. Quid du crash en plein milieu du traitement d’un fichier? Il faut donc penser à ce cas (et à d’autres) et archiver les fichiers pour un éventuel rejeu. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-erreurs-et-indisponibilités"},{"categories":null,"content":" 4.2 Données et idempotence des traitementsIdéalement, les fichiers doivent avoir des lignes indépendantes qui peuvent être insérées individuellement et dans n’importe quel ordre. Aussi, chaque modification et traitement de données doivent être idempotentes. Pourquoi? Pas seulement par ce que c’est sympa et l’état de l’art, mais dans ce nouvel environnement, vous ne pourrez pas forcément garantir l’ordre des traitements. L’une des solutions potentielles de traitement est de découpler la lecture et l’insertion par du queueing (Artemis, Kafka - oui ce n’est pas du queuing, mais vous avez compris…). Dans ce cas, si votre traitement n’est pas idempotent, vous devrez lutter avec des doublons en base. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#données-et-idempotence-des-traitements"},{"categories":null,"content":" 4.3 Gestion des ressourcesImaginez, vous recevez un fichier de 1Go. Vos ressources systèmes sont des PODs avec un 1 Go de RAM. Vous voyez le soucis? Cet exemple, qui n’est pas trop éloigné de la réalité, mets en évidence l’une des contraintes techniques que vous devrez prendre dès le début de votre conception. L’une des solutions serait, par exemple, le traitement quasi systématique du streaming de fichiers et l’obligation d’avoir des fichiers avec des lignes de données indépendantes (c.-à-d. sans avoir à faire de liens inter lignes pendant le traitement). ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:3","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-ressources"},{"categories":null,"content":" 5 Traitement sur réception de fichiersDans ce cas, nous avons un processus qui est déclenché lors de la réception d’un fichier. Nous pourrons par exemple avec ce genre d’architecture un fichier qui est envoyé dans espace de stockage objet. Ce dernier est ensuite traité par un programme. J’ai fait le choix ici de mettre en oeuvre un couplage lâche (on ne se refait pas) entre l’espace de réception de fichiers et le traitement. Je traite ici le risque de crash d’un POD en gardant systématiquement les fichiers dans un stockage objet. De cette manière, si le traitement a échoué, un autre POD pourra le télécharger et rejouer le processus batch. Ce découplage permet de gérer facilement la scalabilité et les arrêts/relances de PODs. Batch démarré sur présence de fichier Dans ce cas, le batch pourra être déployé sous la forme d’un déploiement Kubernetes. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:5:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-sur-réception-de-fichiers"},{"categories":null,"content":" 6 Traitement déclenché à distance (par ex. par un orchestrateur de traitements)Maintenant, on va aborder les traitements qui sont lancés par un ordonnanceur tiers ou tout simplement lancé à distance. Généralement, dans le monde de l’entreprise, la planification des traitements est centralisée au lieu de laisser de le faire sur chaque machine avec des CRON Jobs. Dans ce cas, on a deux manières de procéder: Avoir un traitement qui fournit une API permettant de démarrer des traitements et d’avoir leurs statuts. Lancer des jobs. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-déclenché-à-distance-par-ex-par-un-orchestrateur-de-traitements"},{"categories":null,"content":" 6.1 Avec une APIIci, on conçoit les batchs comme des WEBAPPS qui fournissent des traitements batchs sur demande via des APIs. La contrainte est qu’à l’instar de la solution précédente, le programme tourne toujours et n’est vraiment utile que lorsqu’il est appelé via un endpoint REST. Ce modèle de conception peut être utilisé à mon avis si la fréquence est forte et si l’intégration d’un Job Kubernetes est problématique pour vous (voir ci-dessous). L’un des avantages que l’on pourra trouver est que le mode de déploiement est assez simple et similaire aux APIs. Batch démarré par une API ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-une-api"},{"categories":null,"content":" 6.2 Avec des jobsSi votre ordonnanceur peut exécuter le client kubectl, vous pourrez considérer les jobs kubernetes. En résumé, ils permettent de créer un POD et exécute une action en gérant les erreurs potentielles jusqu’à complétion du traitement. Par exemple, voici un job permettant de faire un “Hello World!”: yaml apiVersion: batch/v1 kind: Job metadata: name: hello-world spec: template: spec: containers: - name: helloworld image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: Never backoffLimit: 4 Une fois déployé avec Helm, vous pouvez les voir avec la commande kubectl get jobs bash minikube kubectl -- get jobs NAME COMPLETIONS DURATION AGE hello-world 0/1 25s 25s Pour les logs et voir le résultat de la commande lancé, cela se passe d’une manière assez habituelle: bash minikube kubectl -- logs hello-world-zx4wh Hello World! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-des-jobs"},{"categories":null,"content":" 7 Traitement déclenché par CRONMaintenant, on va laisser le soin au Cluster Kubernetes de lancer les différents traitements via une CRON. Bien que je ne suis pas trop fan de ne pas centraliser l’ordonnancement, cela peut être très utile si votre plateforme est centrée sur Kubernetes. Si vous êtes dans ce cas-là, vous pouvez utiliser l’objet CronJob qui n’est ni plus ni moins qu’un Job exécuté de manière périodique. yaml apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \"* * * * *\" jobTemplate: spec: template: spec: containers: - name: helloworld-cron image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: OnFailure ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:7:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-déclenché-par-cron"},{"categories":null,"content":" 8 Panorama des solutions logicielles possiblesUne fois qu’on s’est posé toutes (en tout cas certaines) les questions possibles sur nos exigences techniques et la conception, on peut voir quelles sont les technologies possibles pour implémenter des batchs “cloud natifs”. Ça ne sera pas une surprise, je vais m’attarder à la plateforme Java. Il est bien évidemment possible d’utiliser d’autres langages et frameworks tels que Go. En Java, vous avez le choix entre différents frameworks : [Spring avec spring batch]([Spring avec spring batch et/ou integration ) et/ou integration Camel qui peut être utilisé avec Spring ou Quarkus Quarkus avec la JSR 352 Si vous allez du côté du BigData, vous pouvez aussi envisager d’utiliser des technologies telles qu’Apache Spark. Ces dernières vous permettront de découper “plus facilement” vos traitements. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:8:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#panorama-des-solutions-logicielles-possibles"},{"categories":null,"content":" 9 Le diable se cache dans les détailsDéployer un batch dans Kubernetes peut se faire assez facilement (en développement) une fois qu’on a compris quelques principes. Cependant, les soucis peuvent survenir une fois arrivé en production. La gestion des erreurs est beaucoup plus complexe que les APIs. Il vous faudra donc définir avec les différentes parties prenantes quel est le meilleur fonctionnement (rejeu) en production. Il vous faudra ainsi bien identifier et évaluer les risques liés à votre application et voir quelles sont les actions à mener. Aussi, si vous devez manipuler des fichiers volumineux, il faudra faire attention au système de fichiers utilisé et ses performances. Habituellement, avec ce type d’architecture, on utilise généralement du SAN. En fonction de vos exigences, un stockage block pourra être plus adapté. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:9:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#le-diable-se-cache-dans-les-détails"},{"categories":null,"content":" 10 ConclusionPour conclure cet article, vous aurez compris que le sujet des batchs dans Kubernetes peut s’avérer assez complexe à gérer. Au-delà des technologies qui peuvent faire le job (désolé du mauvais jeu de mots), il vous faudra faire très attention à tout l’environnement dans lequel votre programme devra interagir. Les bases, le réseau, les performances de votre matériel seront des prérequis indispensables. Aussi, il vous faudra faire attention à la manière dont sont transmises les données et dont vous les traitez. Bref, il faut étudier la solution dans son ensemble du développement à l’exploitation pour s’assurer de ne rien oublier. Enfin, cet article n’est bien évidemment pas exhaustif que cela soit sur les solutions ou les contraintes à adresser. J’ai néanmoins essayé d’apporter quelques cas concrets et retours d’expérience. J’essaierai de détailler un cas concret dans un prochain article. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:10:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#conclusion"},{"categories":null,"content":" 1 L’analyse des risques: kezako ?Souvent utilisée dans la prise de décision, l’analyse des risques a plusieurs objectifs : Permettre de pondérer des risques potentiels Faciliter la prise de décision sur les actions à réaliser pour les prévenir ou tout du moins les atténuer. Mais d’abord, revenons aux bases. Comment identifier un risque ? Selon Wikipedia, voici la définition: Le risque est la possibilité de survenue d’un événement indésirable, la probabilité d’occurrence d’un péril probable ou d’un aléa. Bien évidemment, on a les risques inconnus et ceux qui sont connus. Le préalable à toute gestion de risque (tout du moins pour la définition d’architecture) est de capitaliser les connaissances et retours d’expérience qui viennent du terrain. On va donc oublier les risques inconnus dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#lanalyse-des-risques-kezako-"},{"categories":null,"content":" 1.1 Comment les définir ?Tout d’abord, il faut connaître les SLOs de la plateforme qu’on souhaite concevoir. Pourquoi ? Pour vérifier si les risques qu’on identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne électrique sera faiblement impactante pour une application avec une disponibilité \u003c 70%. La réalisation des SLIs et SLOs est un préalable pour définir le “budget d’erreur”. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les atténuer. Ensuite, pour chaque risque qu’on identifiera (souvent à partir de notre expérience), on tâchera de définir les caractéristiques suivantes: Cause Probabilité Conséquence (gravité) 1.1.1 Un exempleLa base de données est indisponible Cause: Système de fichier plein Probabilité: faible Conséquence: forte (toute la plateforme est HS) Pour déterminer la cause, il y a plusieurs méthodes, l’une des plus célèbres est celle des cinq pourquoi. Elle permet d’accéder à la cause du problème. Pour établir la probabilité, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour déterminer quels ont été les différents incidents. Pour chacun, vous devrez définir ces trois caractéristiques : cause, probabilité, conséquence. A coté de ça, vous aurez à identifier si possible le temps d’indisponibilité du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#comment-les-définir-"},{"categories":null,"content":" 1.1 Comment les définir ?Tout d’abord, il faut connaître les SLOs de la plateforme qu’on souhaite concevoir. Pourquoi ? Pour vérifier si les risques qu’on identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne électrique sera faiblement impactante pour une application avec une disponibilité \u003c 70%. La réalisation des SLIs et SLOs est un préalable pour définir le “budget d’erreur”. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les atténuer. Ensuite, pour chaque risque qu’on identifiera (souvent à partir de notre expérience), on tâchera de définir les caractéristiques suivantes: Cause Probabilité Conséquence (gravité) 1.1.1 Un exempleLa base de données est indisponible Cause: Système de fichier plein Probabilité: faible Conséquence: forte (toute la plateforme est HS) Pour déterminer la cause, il y a plusieurs méthodes, l’une des plus célèbres est celle des cinq pourquoi. Elle permet d’accéder à la cause du problème. Pour établir la probabilité, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour déterminer quels ont été les différents incidents. Pour chacun, vous devrez définir ces trois caractéristiques : cause, probabilité, conséquence. A coté de ça, vous aurez à identifier si possible le temps d’indisponibilité du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple"},{"categories":null,"content":" 1.2 SynthèseUne fois ce travail de fourmi réalisé, vous pourrez le synthétiser dans un premier temps avec ce formalisme souvent repris dans la gestion de projet: En résumé, les actions qui sont oranges ou rouges doivent être traitées et avoir un plan d’action. Prenons la définition d’une plateforme: Si votre API doit traiter de manière régulière des PAYLOADs volumineux (bon déjà, la ce n’est pas top). Le temps de traitement peut être très long et bloquer certaines ressources (ex. des sous transactions). Dans ce cas, la probabilité sera à probable et l’impact sera modéré ou majeur. Par conséquent, vous devrez le prendre en compte avec par exemple un circuit breaker. Pour aller encore plus loin, vous pouvez également évaluer les risques en fonction de votre budget d’erreur: Est-ce que l’erreur peut rentrer dans mon budget ou pas? Bref, est-ce acceptable? Si vous allez plus loin, je vous conseille la formation Coursera Site Reliability Engineering: Measuring and Managing Reliability. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#synthèse"},{"categories":null,"content":" 2 OK, j’ai identifié les risques potentiels. Qu’est-ce que j’en fait maintenant?C’est là que démarre réellement le travail d’architecture: vous devrez évaluer chaque risque en fonction des exigences fonctionnelles et technique pour savoir si elles valent la peine d’être prises en considération. Si vous avez des risques de faible impact, vous pourrez soit les “mettre sous contrôle” pour traiter d’autres problèmes, soit les traiter car ils sont “faciles” à traiter et vous permettront d’agrandir votre “budget d’erreur”. Dans ce dernier cas, vous aurez la possibilité de laisser “de coté” d’autres erreurs plus compliquées à traiter car elles rentreront dans votre budget. Bref, c’est un vrai travail de fourmi qui se base sur l’expérience du terrain. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:2:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#ok-jai-identifié-les-risques-potentiels-quest-ce-que-jen-fait-maintenant"},{"categories":null,"content":" 3 Quid de l’architecture?Si vous avez évalué les risques correctement, vous pourrez ne traiter que les risques qui en valent la peine et par conséquent n’ ajouter de la complexité que là ou ça en vaut la peine! Oui, ce travail préalable permet de faire simple! ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:3:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#_quid_-de-larchitecture"},{"categories":null,"content":" 4 Un exemple concretSi je reprends l’application bookstore que j’ai décrite dans un précédent article: ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple-concret"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API 4.1.2 Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#exigences-techniques"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API 4.1.2 Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#slos--slis"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L’API Bookstore doit être disponible 99% Nombre de réponses HTTP = 2XX ou 4XX L’API Bookstore doit répondre en moins de 1 sec Temps de réponse de l’API 4.1.2 Volumétrie 100 transactions par seconde (TPS) 100 utilisateurs simultanés ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#volumétrie"},{"categories":null,"content":" 4.2 Risques identifiésSans aller dans le détail, voici quelques risques que l’on peut identifier de prime abord dans cette architecture: Indisponibilité du service bookstore Indisponibilité du service booknumber Indisponibilité de la base de données à cause d’une forte volumétrie En vous basant sur l’expérience de vos OPS, vous pourrez également ajouter des risques liés à l’infrastructure (routeurs, DNS, …). Je ne vais pas les aborder dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#risques-identifiés"},{"categories":null,"content":" 4.3 Qualification des risquesVoici une rapide qualification: Risques Probabilité Impact Indisponibilité du service bookstore Probable Majeur Indisponibilité du service booknumber Probable Majeur Indisponibilité de la base de données Possible Catastrophique Si on se réfère au premier diagramme, il est obligatoire de les prendre en compte. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:3","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#qualification-des-risques"},{"categories":null,"content":" 4.4 Solutions d’architecture pour leur prise en compteUne fois les risques identifiés, on peut tout d’abord les confronter à notre budget d’erreur pour valider leur prise en compte dans notre conception. Dans notre cas, on va prendre le postulat qu’il faut réellement les prendre en considération et trouver une solution adaptée. Voici des exemples de solutions qui permettraient de faire descendre leur impact our leur probabilité. Risques Probabilité Impact Action/Solutions possibles Indisponibilité du service bookstore Possible Majeur Load balancing avec deux instances, Utilisation Kubernetes,. Indisponibilité du service booknumber Possible Majeur Sur le service book-number: Load balancing avec deux instances, Utilisation Kubernetes,.. Sur le service bookstore: Mettre en place un circuit breaker basé sur le timeout d’appel vers le service book-number pour garantir la SLO Indisponibilité de la base de données Possible Catastrophique - Réalisation d’un benchmark pour s’assurer qu’une instance est suffisante. - Sinon mise en place mécanisme HA ou changement de technologie ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:4","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#solutions-darchitecture-pour-leur-prise-en-compte"},{"categories":null,"content":" 5 ConclusionL’analyse des risques n’est pas récente et n’a pas été inventée par le monde de l’informatique. Elle est d’abord apparue dans la gestion de projets et fait désormais partie prenante de la définition d’architectures (enfin ça commence…). Il ne faut pas la voir seulement pour un outil de “GO-NO GO” de réunion de cellule de crise mais comme une aide à la décision pour la conception des systèmes. Il a toute sa place à coté des différentes caractéristiques que vous devrez prendre en compte ( sécurité, modularité, …). J’ai essayé de décrire comment les identifier et trouver une solution adaptée dans l’exemple. Bien évidemment, il n’est pas complet. Je pense néanmoins qu’il permet d’avoir une idée sur le sujet. Le principal avantage d’utiliser à la fois les SLOs/SLIs, le budget d’erreur et l’analyse des risques est de n’apporter de la complexité que là où c’est nécessaire. Pour certains un benchmark sera souvent utile pour confirmer votre décision. Si vous voulez aller plus loin, je vous conseille dans un premier temps de lire “Fundamentals of Software Architecture”. Ce sujet y est abordé. Enfin,si ce sujet vous intéresse, vous pouvez vous projeter au délà de l’informatique en lisant les analyses de risques réalisées par le Ministère des finances. Bonne lecture ;-) ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:5:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#conclusion"},{"categories":null,"content":" 1 Pourquoi mettre en oeuvre des GITHUB ACTIONS ?Comme j’ai pu l’expliquer dans mon précédent article, je suis passé de Wordpress à GITHUB Pages. Une fois le site déployé une première fois, on voit qu’on a perdu pas mal d’automatisations qui sont réalisées par défaut dans Wordpress. Par exemple, vous devez construire votre site, publier des nouveaux articles et vérifier que tout est OK. J’ai donc mis en oeuvre des GITHUB ACTIONS pour automatiser le plus d’actions possibles et me passer de tâches manuelles souvent rébarbatives. Si vous souhaitez découvrir les GITHUB ACTIONS, je vous conseille ce site. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:1:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pourquoi-mettre-en-oeuvre-des-github-actions-"},{"categories":null,"content":" 2 Construction du site et déploiementDès qu’on touche à Jekyll et à l’hébergement sur Github Pages, on tombe sur certaines actions à réaliser telles que celle-ci: bash bundle exec jekyll build J’ai donc réalisé les workflows suivants: ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#construction-du-site-et-déploiement"},{"categories":null,"content":" 2.1 Pour une feature branch (dans une Pull Request)Je construis le site sans le déployer pour vérifier que la construction est correcte. yaml name: Build Jekyll site on: push: branches-ignore: (1) - main - gh-pages jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: 📂 setup (2) uses: actions/checkout@v2 # include the lines below if you are using jekyll-last-modified-at # or if you would otherwise need to fetch the full commit history # however this may be very slow for large repositories! # with: # fetch-depth: '0' - name: 💎 setup ruby (3) uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: 🔨 install dependencies \u0026 build site (4) run: bundle exec jekyll build Voila ce que ce workflow réalise: Il est exécuté à chaque push excepté sur les branches main et gh-pages. Ce sont les branches que j’utilise pour le déploiement du site après la validation d’une pull request. Checkout du projet Initialisation de Ruby et téléchargement des dépendances comme le ferait la commande bundle install. Construction du site ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:1","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-une-feature-branch-dans-une-pull-request"},{"categories":null,"content":" 2.2 Pour la branche mainUne fois que la pull request est validée, le code est poussé dans la branche main. On y exécute le code suivant: yaml name: Build and deploy Jekyll site to GitHub Pages on: push: branches: - main jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: 📂 setup uses: actions/checkout@v2 - name: 💎 setup ruby uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: 🔨 install dependencies \u0026 build site run: bundle exec jekyll build - name: 🚀 deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./_site Ce dernier reprend le code du workflow suivant ( oui, j’aurai pu faire des workflows réutilisables… ) et ajoute l’étape de déploiement. Le code généré sera copié dans la branche gh-pages. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:2","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-la-branche-main"},{"categories":null,"content":" 3 Publication d’un articlePour rédiger un article, j’utilise le mécanisme de feature branch et pull request. Pour automatiser la publication, le nommage des articles avec la date etc, j’ai mis en oeuvre le workflow suivant: Je rédige les articles (comme celui-ci) et les positionne dans le répertoire _drafts: bash ls -R _drafts quelques-github-actions-utiles-pour-un-site-jekyll-heberge-sur-github-io.md J’associe un milestone à la pull request. Dès que ces derniers sont terminés, le workflow décrit ci-dessous est exécuté. Il permet, via un script python de: Identifier les articles dans le répertoire _drafts Vérifier que la date de publication spéficié dans l’en-tête est antérieure à la date courante (now()) Copier le fichier dans le répertoire _posts en le renommant avec la date en préfixe. yaml name: Publish Drafts on: (1) milestone: types: [closed] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: ref: main (2) - name: 📂 setup python uses: actions/setup-python@v2 with: python-version: '3.7.7' # install the python version needed - name: 💎 install python packages (3) run: | python -m pip install --upgrade pip - name: 🔨 execute py script (4) run: python publish_drafts.py - name: 🔨 commit files (5) run: | if git ls-files -o --exclude-standard; then git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add -A git commit -m \"publish drafts\" -a git push origin main else echo \"No file to publish\" fi Explication: Déclenchement manuel ou à la clôture d’un milestone Récupération de la branche main Installation de packages python Exécution du script python réalisé pour l’occasion Commit et push Une fois ce workflow réalisé, le workflow vu précédemment est automatiquement lancé et le site est généré une nouvelle fois. Bon ça fait deux constructions, mais au vu du temps pris, c’est négligeable. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:3:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#publication-dun-article"},{"categories":null,"content":" 4 UptimeJ’aurai pu utiliser un tiers service tel que uptime robot. Pour mon besoin, j’ai préféré opter pour un appel régulier du site et une vérification du code HTTP (200). yaml # This is a basic workflow to help you get started with Actions name: Uptime Monitoring on: schedule: (1) - cron: '*/60 * * * *' jobs: ping_site: runs-on: ubuntu-latest name: Ping the site steps: - name: Check the site id: hello uses: srt32/uptime@master with: url-to-hit: \"https://blog.touret.info/robots.txt\" (2) expected-statuses: \"200,301\" Explications Déclenchement toutes les heures de ce workflow J’ai utilisé une GITHUB ACTION existante qui ping une URL et vérifie le code retour. Dans mon cas, j’ai utilisé l’URL du fichier robots.txt et je vérifie le code retour. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:4:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#uptime"},{"categories":null,"content":" 5 ConclusionJ’ai réussi à plus ou moins automatiser tout le cycle de construction d’articles. C’est encore perfectible et loin de certaines fonctionnalités de Wordpress, mais je n’en ai pas réellement besoin. Si vous souhaitez réutiliser ces workflows et les intégrer dans sites, vous pouvez les récupérer sur ce repo GITHUB. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:5:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site hébergé sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#conclusion"},{"categories":null,"content":"L’idée me trottait dans la tête depuis quelques mois environ: migrer mon blog de Wordpress vers un site basé sur Jekyll et hébergé directement sur Github. La date de renouvellement de ma souscription Wordpress arrivant à terme, je me suis décidé à franchir le pas. s ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:0:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#"},{"categories":null,"content":" 1 L’ hébergement de sites web sur GithubGithub permet via son service Github Pages d’héberger des sites statiques (c.-à-d. pas de base de données derrière) en permettant d’associer son nom de domaine. Le certificat est automatiquement généré. Pour avoir un look un peu sympa, j’ai donc mis en oeuvre les outils suivants: Jekyll Minimal Mistakes Github Actions pour construire le site Markdown pour écrire les différents articles ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:1:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#l-hébergement-de-sites-web-sur-github"},{"categories":null,"content":" 2 Démarrer avec Minimal MistakesJe vous conseille d’aller sur ce site. Tout est bien détaillé est c’est réalisable en quelques minutes seulement. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:2:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#démarrer-avec-minimal-mistakes"},{"categories":null,"content":" 3 Migration des donnéesSans surprise, c’est la partie la moins drôle. Il faut en résumé: Exporter les données si vous êtes hébergé sur wordpress.com Les ré-importer dans une instance locale Les exporter au format Jekyll Copier le contenu généré dans un nouveau site ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#migration-des-données"},{"categories":null,"content":" 3.1 Exporter les donnéesAfin d’exporter les données et de les transformer, il faut d’abord exporter les données (articles + médias) de votre site Wordpress Vous obtiendrez deux archives: la première pour les articles, la deuxième pour les médias. Création d’une instance Wordpress pour convertir les données au format Jekyll ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#exporter-les-données"},{"categories":null,"content":" 3.2 Importer les donnéesPour faire simple, j’utilise Docker pour monter une architecture Wordpress sur mon poste. Il faut pour ça créer un fichier docker-compose.yml et insérer le contenu suivant: yaml version: \"3.9\" services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - wordpress_data:/var/www/html ports: - \"8000:80\" restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: db_data: {} wordpress_data: {} Ensuite, vous aurez à lancer la commande suivante: bash docker-compose up Une fois lancé, vous aurez à une instance Wordpress via cette URL : http://localhost:8000 Ensuite, il faut installer l’extension jekyll-exporter. La procédure peut prendre un peu de temps. Une fois effectuée, vous aurez une archive ZIP contenant un site Jekyll avec les images et articles associés. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#importer-les-données"},{"categories":null,"content":" 4 Création du siteEn attendant que ça se termine, j’ai crée un site jekyll avec le starter du thème minimal mistakes. J’ai ensuite copié les articles (répertoire /_posts) et images (/assets/img). Au premier lancement des commandes suivantes: bash bundle install bundle exec jekyll serve J’ai eu quelques erreurs. J’ai donc eu à nettoyer les fichiers via des recherche/remplace dans un éditeur Par exemple, j’ai supprimé les références author et layout yaml author: admin layout: post J’ai également ajouté pour certains articles une image pour le teaser Exemple: yaml featuredImagePreview: /assets/images/2021/07/rest-book-architecture.png ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#création-du-site"},{"categories":null,"content":" 4.1 URL des pages et compatibilité Wordpress \u003c\u003e JekyllPour me faciliter la vie dans les URLS et liens en tout genre, j’ai gardé le format des URLS de Wordpress. Pour que ça soit le format par défaut de Jekyll, il faut modifier le paramètre permalink dans le fichier _config.yml yaml permalink: /:year/:month/:day/:title/ ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#url-des-pages-et-compatibilité-wordpress--jekyll"},{"categories":null,"content":" 4.2 Flux RSS pour un tag donnéJ’utilisais une petite spécificité de Wordpress: la création d’un flux RSS pour un tag donné. Pour le mettre en place dans Jekyll, il faut configurer le plugin jekyll-feed avec les propriété suivantes: yaml feed: tags: true ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#flux-rss-pour-un-tag-donné"},{"categories":null,"content":" 5 Et maintenant ?J’ai sans doute oublié quelques renommages/suppressions réalisés ici et là. Néanmoins, le principal est évoqué dans cet article. Il ne vous reste plus qu’à éplucher la documentation du thème et de jekyll pour finaliser l’ installation de votre nouveau site. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:5:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#et-maintenant-"},{"categories":null,"content":"Il y a quelques mois déjà, je discutais avec un collègue d’ observabilité, opentracing, … avec Quarkus. On est tombé sur un super exemple réalisé par Antonio Concalves. Ce projet démontre les capacités de Quarkus sur les sujets suivants: Circuit Breaker Observabilité OpenTracing Tests … Et la on peut se demander quid de Spring? Je me doutais que ces fonctionnalités étaient soient disponibles par défaut soient facilement intégrables vu la richesse de l’écosystème. J’ai donc réalisé un clone de ce projet basé sur Spring Boot/Cloud. Je ne vais pas détailler plus que ça les différentes fonctionnalités, vous pouvez vous référer au fichier README. Il est suffisamment détaillé pour que vous puissiez exécuter et les mettre en œuvre. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:0:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#"},{"categories":null,"content":" 1 Architecture de l’applicationVous trouverez ci-dessous un schéma d’architecture de l’application au format C4. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:1:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#architecture-de-lapplication"},{"categories":null,"content":" 2 Circuit BreakerLors des appels entre le bookstore et le booknumberservice, il peut être intéressant d’ implémenter un circuit breaker pour pallier aux indisponibilités de ce dernier. Avec Spring, on peut utiliser Resilience4J au travers de Spring Cloud. Tout ceci se fait de manière programmatique Il faut tout d’abord configurer les circuit breakers au travers d’une classe Configuration. java @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createDefaultCustomizer() { return factory -\u003e factory.configureDefault(id -\u003e new Resilience4JConfigBuilder(id) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()) .circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .build()); } /** * Creates a circuit breaker customizer applying a timeout specified by the \u003ccode\u003ebooknumbers.api.timeout_sec\u003c/code\u003e property. * This customizer could be reached using this id: \u003ccode\u003eslowNumbers\u003c/code\u003e * @return the circuit breaker customizer to apply when calling to numbers api */ @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createSlowNumbersAPICallCustomizer() { return factory -\u003e factory.configure(builder -\u003e builder.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()), \"slowNumbers\"); } Grâce à ces instanciations, on référence les différents circuit breakers. Maintenant, on peut les utiliser dans le code de la manière suivante: java public Book registerBook(@Valid Book book) { circuitBreakerFactory.create(\"slowNumbers\").run( () -\u003e persistBook(book), throwable -\u003e fallbackPersistBook(book) ); return bookRepository.save(book); } Maintenant, il ne reste plus qu’à créer une méthode de « fallback » utilisée si un service est indisponible. Cette dernière nous permettra, par exemple, de mettre le payload dans un fichier pour futur traitement batch. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:2:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#circuit-breaker"},{"categories":null,"content":" 3 ObservabilitéL’observabilité est sans contexte la pierre angulaire (oui, rien que ça…) de toute application cloud native. Sans ça, pas de scalabilité, de redémarrage automatique,etc. Les architectures de ce type d’applications sont idempotentes. On a donc besoin d’avoir toutes les informations à notre disposition. Heureusement, Spring fournit par le biais d’ Actuator toutes les informations nécessaires. Ces dernières pourront soit être utilisées par Kubernetes (ex. le livenessProbe) ou agrégées dans une base de données Prometheus. Pour activer certaines métriques d’actuator, il suffit de : Ajouter la/les dépendance(s) groovy dependencies { [...] implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'io.micrometer:micrometer-registry-prometheus' [...] } Spécifier la configuration adéquate: java management: endpoints: enabled-by-default: true web: exposure: include: '*' jmx: exposure: include: '*' endpoint: health: show-details: always enabled: true probes: enabled: true shutdown: enabled: true prometheus: enabled: true metrics: enabled: true health: livenessstate: enabled: true readinessstate: enabled: true datasource: enabled: true metrics: web: client: request: autotime: enabled: true ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:3:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#observabilité"},{"categories":null,"content":" 4 OpenTracingSur les applications distribuées, il peut s’avérer compliqué de concentrer les logs et de les corréler. Certes, avec un ID de corrélation, on peut avoir certaines informations. Cependant, il faut que les logs soient bien positionnées dans le code. On peut également passer à travers de certaines informations (ex. connexion aux bases de données, temps d’exécution des APIS,…). Je ne vous parle pas des soucis de volumétrie engendrées par des index Elasticsearch/Splunk sur des applications à forte volumétrie. Depuis quelques temps, le CNCF propose un projet (encore en incubation) : OpenTracing. Ce dernier fait désormais partie d’OpenTelemetry. Grâce à cet librairie, nous allons pouvoir tracer toutes les transactions de notre application microservices et pouvoir réaliser une corrélation « out of the box » grâce à l’intégration avec Jaeger. Pour activer la fonctionnalité il suffit d’ajouter la dépendance au classpath: groovy implementation 'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter:3.3.1' et de configurer l’URL de Jaeger dans l’application yaml # Default values opentracing: jaeger: udp-sender: host: localhost port: 6831 enabled: true Une fois l’application reconstruite et redémarrée, vous pourrez visualiser les transactions dans JAEGER: ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:4:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#opentracing"},{"categories":null,"content":" 5 ConclusionJe ne vais pas exposer l’implémentation des tests unitaires et d’intégration. Si vous voulez voir comment j’ai réussi à mocker simplement les appels REST à une API distante, vous pouvez regarder cette classe pour voir une utilisation du MockServer. Aussi, n’hésitez pas à cloner, tester ce projet et me donner votre retour. J’essaierai de le mettre à jour au fur et à mesure de mes découvertes (par ex. OpenTelemetry). ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:5:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilité et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#conclusion"},{"categories":null,"content":"Quand vous avez une API, et a fortiori une application, il peut être parfois nécessaire de passer l’application en mode « maintenance ». Pour certaines applications il est parfois inutile de le traiter au niveau applicatif, car ça peut être pris géré par certaines couches de sécurité ou frontaux web par ex. (Apache HTTPD, WAF) Kubernetes a introduit ( ou popularisé ) les notions de « probes » et plus particulièrement les livenessProbes et readinessProbes. Le premier nous indique si l’application est en état de fonctionnement, le second nous permet de savoir si cette dernière est apte à recevoir des requêtes (ex. lors d’un démarrage). Je vais exposer dans cet article comment utiliser au mieux ces probes et les APIs SPRING pour intégrer dans une API un mode « maintenance » ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:0:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#"},{"categories":null,"content":" 1 Stack utiliséeDans l’exemple que j’ai développé, j’ai pu utiliser les briques suivantes: OpenJDK 11.0.10 Spring Boot 2.5.0 (web, actuator) Maven 3.8.1 Bref, rien de neuf à l’horizon 🙂 ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:1:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#stack-utilisée"},{"categories":null,"content":" 2 Configuration de Spring ActuatorPour activer les différents probes, vous devez activer Actuator. Dans le fichier pom.xml, vous devez ajouter le starter correspondant: xml \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e Puis vous devez déclarer ces differentes propriétés: ini management.endpoints.enabled-by-default=true management.health.livenessstate.enabled=true management.health.readinessstate.enabled=true management.endpoint.health.show-details=always management.endpoint.health.probes.enabled=true management.endpoint.health.enabled=true Après avoir redémarré votre application, vous pourrez connaître son statut grâce à un appel HTTP bash curl -s http://localhost:8080/actuator/health/readiness ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:2:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#configuration-de-spring-actuator"},{"categories":null,"content":" 3 Comment récupérer le statut des probes?Avec Spring, vous pouvez modifier les différents statuts avec les classes ApplicationEventPublisher et ApplicationAvailability. Par exemple, pour connaître le statut Readiness vous pouvez exécuter le code suivant: java @ApiResponses(value = { @ApiResponse(responseCode = \"200\", description = \"Checks if the application in under maitenance\")}) @GetMapping public ResponseEntity\u003cMaintenanceDTO\u003e retreiveInMaintenance() { var lastChangeEvent = availability.getLastChangeEvent(ReadinessState.class); return ResponseEntity.ok(new MaintenanceDTO(lastChangeEvent.getState().equals(ReadinessState.REFUSING_TRAFFIC), new Date(lastChangeEvent.getTimestamp()))); } Et la modification ? Grâce à la même API, on peut également modifier ce statut dans via du code: java @ApiResponses(value = { @ApiResponse(responseCode = \"204\", description = \"Put the app under maitenance\")}) @PutMapping public ResponseEntity\u003cVoid\u003e initInMaintenance(@NotNull @RequestBody String inMaintenance) { AvailabilityChangeEvent.publish(eventPublisher, this, Boolean.valueOf(inMaintenance) ? ReadinessState.REFUSING_TRAFFIC : ReadinessState.ACCEPTING_TRAFFIC); return ResponseEntity.noContent().build(); } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:3:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#comment-récupérer-le-statut-des-probes"},{"categories":null,"content":" 4 Filtre les appels et indiquer que l’application est en maintenanceMaintenant qu’on a codé les mécanismes de récupération du statut de l’application et de la mise en maintenance, on peut ajouter le mécanisme permettant de traiter ou non les appels entrants. Pour ça on va utiliser un bon vieux filtre servlet. Ce dernier aura la tâche de laisser passer les requêtes entrantes si l’application n’est pas en maintenance et de déclencher une MaintenanceException le cas échéant qui sera traité par la gestion d’erreur globale de l’application ( traité via un @RestControllerAdvice). Pour que l’exception soit bien traitée par ce mécanisme, il faut le déclencher via le HandlerExceptionResolver. java @Component public class CheckMaintenanceFilter implements Filter { private final static Logger LOGGER = LoggerFactory.getLogger(CheckMaintenanceFilter.class); @Autowired private ApplicationAvailability availability; @Autowired @Qualifier(\"handlerExceptionResolver\") private HandlerExceptionResolver exceptionHandler; /** * Checks if the application is under maintenance. If it is and if the requested URI is not '/api/maintenance', it throws a \u003ccode\u003eMaintenanceException\u003c/code\u003e * * @param request * @param response * @param chain * @throws IOException * @throws ServletException * @throws info.touret.spring.maintenancemode.exception.MaintenanceException the application is under maintenance */ @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { if (availability.getReadinessState().equals(ReadinessState.REFUSING_TRAFFIC) \u0026\u0026 !((HttpServletRequest) request).getRequestURI().equals(API_MAINTENANCE_URI)) { LOGGER.warn(\"Message handled during maintenance [{}]\", ((HttpServletRequest) request).getRequestURI()); exceptionHandler.resolveException((HttpServletRequest) request, (HttpServletResponse) response, null, new MaintenanceException(\"Service currently in maintenance\")); } else { chain.doFilter(request, response); } } } Enfin, voici la gestion des erreurs de l’API: java @RestControllerAdvice public class GlobalExceptionHandler { /** * Indicates that the application is on maintenance */ @ResponseStatus(HttpStatus.I_AM_A_TEAPOT) @ExceptionHandler(MaintenanceException.class) public APIError maintenance() { return new APIError(HttpStatus.I_AM_A_TEAPOT.value(),\"Service currently in maintenance\"); } /** * Any other exception */ @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) @ExceptionHandler({RuntimeException.class, Exception.class}) public APIError anyException() { return new APIError(HttpStatus.INTERNAL_SERVER_ERROR.value(),\"An unexpected server error occured\"); } } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:4:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#filtre-les-appels-et-indiquer-que-lapplication-est-en-maintenance"},{"categories":null,"content":" 5 ConclusionOn a pu voir comment intéragir simplement avec les APIS SPRING pour gérer le statut de l’application pour répondre à cette question :Est-elle disponible ou non? Bien évidemment, selon le contexte, il conviendra d’ajouter un peu de sécurité pour que cette API ne soit pas disponible à tout le monde 🙂 Le code exposé ici est disponible sur Github. Le Readme est suffisamment détaillé pour que vous puissiez tester et réutiliser le code. ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:5:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode « maintenance » à votre API grâce à Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#conclusion"},{"categories":null,"content":" Pourquoi utiliser GPG ? Par exemple pour signer les commits GIT. Maintenant comment faire quand on est sous Windows 10 et qu’on souhaite utiliser le sous système Linux (WSL2)? Sous GNU/Linux, l’installation et l’utilisation avec git est très simple. Avec WSL2,… il faut un peu d’huile de coude 🙂 Je vais tâcher de décrire dans cet article les différentes manipulations nécessaires pour: Importer une clé GPG existante Utiliser GPG pour signer mes commits dans WSL2 ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:0:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#"},{"categories":null,"content":" 1 Importer une clé GPG existante","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#importer-une-clé-gpg-existante"},{"categories":null,"content":" 1.1 Export de la clé GPG 1.1.1 Identifier l’ ID de la cléLancez la commande suivante: bash gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#export-de-la-clé-gpg"},{"categories":null,"content":" 1.1 Export de la clé GPG 1.1.1 Identifier l’ ID de la cléLancez la commande suivante: bash gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#identifier-l-id-de-la-clé"},{"categories":null,"content":" 1.2 Import bash gpg --import public.key gpg --import private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:2","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#import"},{"categories":null,"content":" 1.3 VérificationPour vérifier que la clé est bien configurée, vous pouvez lancer la commande suivante: bash gpg --list-secret-keys --keyid-format LONG alexandre@.... sec rsa4096/CLE_ID 2019-12-20 [SC] ******************** uid [ ultime ] Alexandre \u003calexandre@....\u003e ssb rsa4096/SUB 2019-12-20 [E] Si la clé n’est pas reconnue comme ultime ou comme de confiance, il faudra l’éditer: bash gpg --edit-key CLE_ID Please decide how far you trust this user to correctly verify other users' keys (by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menu Your decision? Si vous ne voulez pas trop vous compliquer, je vous conseille de répondre 5. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:3","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#vérification"},{"categories":null,"content":" 2 Configuration GPG pour WSL2Avant de configurer l’agent GPG, vous pouvez vous référer à cet article pour configurer GIT et GPG. La configuration est équivalente. Ensuite, créez le fichier ~/.gnupg/gpg.conf avec le contenu suivant: conf # Uncomment within config (or add this line) # This tells gpg to use the gpg-agent use-agent # Set the default key default-key CLE_ID Puis créez le fichier ~/.gnupg/gpg-agent.conf avec le contenu ci-dessous: conf default-cache-ttl 34560000 max-cache-ttl 34560000 pinentry-program /usr/bin/pinentry-curses Le cache ici est défini en secondes. Il est mis ici à 400 jours. Ce dernier fichier fait référence au programme pinentry. Vous pouvez vérifier sa présence grâce à la commande: bash ls /usr/bin/pinentry-curses Si vous ne l’avez pas, vous pouvez l’installer grâce à la commande suivante: bash sudo apt install pinentry-curses Maintenant, on peut configurer l’environnement BASH en modifiant le fichier ~/.bashrc bash # enable GPG signing export GPG_TTY=$(tty) if [ ! -f ~/.gnupg/S.gpg-agent ]; then eval $( gpg-agent --daemon --options ~/.gnupg/gpg-agent.conf ) fi export GPG_AGENT_INFO=${HOME}/.gnupg/S.gpg-agent:0:1 Redémarrez ensuite WSL2 pour que ça soit pris en compte. A la première utilisation de GPG ( par ex. lors d’un commit, vous aurez une interface Ncurses qui apparaîtra dans votre prompt WSL2. Vous aurez à renseigner le mot de passe de votre clé. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:2:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#configuration-gpg-pour-wsl2"},{"categories":null,"content":"Les confinements se suivent et se ressemblent. Me voilà à installer Ubuntu sur un nouvel ordinateur. A l’instar de l’ancien laptop que j’ai acheté pour mon aînée, j’ai acheté un DELL pour ma deuxième fille.J’ai opté pour un DELL Inspiron 5301. A l’instar de mon autre laptop, je j’ai pas pris de risques. J’ai opté pour un DELL qui est pleinement compatible avec Ubuntu. Oui j’aurai pu installer un ordinateur avec Ubuntu pré-installé, mais je n’ai pas eu le temps de faire un choix « serein ». ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:0:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#"},{"categories":null,"content":" 1 Configuration du BIOSVoila les paramètres que j’ai appliqué: Dans le menu \"Storage\" puis \"SATA Operation\": vous devez sélectionner AHCI au lieu de RAID. Dans le menu \"Change boot mode settings \u003eUEFI Boot Mode\" , vous devez désactiver le Secure Boot. Une fois réalisé, vous pouvez redémarrer en appuyant sur la touche F12. Si vous n’arrivez pas à revenir sur le BIOS pour indiquer de booter sur votre clé USB, vous obtiendrez un écran d’erreur Windows dû à la configuration AHCI. Personnellement, en redémarrant une ou deux fois, j’ai obtenu un écran de démarrage avancé qui m’a permis de sélectionner le périphérique (ma clé USB) sur lequel démarrer. Maintenant vous pouvez accéder à l’installeur Ubuntu et profiter. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:1:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#configuration-du-bios"},{"categories":null,"content":" 2 InstallationJ’ai eu plusieurs fois des popup « erreur rencontré ». Ce n’était pas bloquant. J’ai continué. Tout s’est déroulé sans encombre. Le matériel est très bien reconnu. Les seuls logiciels que j’ai installé sont pour l’instant : VLC, Minecraft ( obligatoire dans la famille ) et Chromium. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:2:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#installation"},{"categories":null,"content":"Dès qu’on veut déployer des environnements Kubernetes, helm devient une des solutions à considérer. Le déploiement des objets standards tels que deployment, autoscaler et autres se fait aisément car ces derniers ne changent pas d’un environnement à l’autre. Généralement on déploie la même infrastructure sur tous les environnements du développement à la production. Bien évidemment on pourra limiter la taille des replicas sur l’environnement de développement par exemple mais au fond, le contenu des charts sera identique. Une des difficultés que l’on pourra rencontrer c’est dans la gestion des fichiers de configuration. Je vais essayer d’exposer dans cet article comment j’ai réussi à gérer +/- efficacement (en tout cas pour moi) les fichiers de configuration dans les charts HELM. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:0:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#"},{"categories":null,"content":" 1 Les config maps et secretsLogiquement dans ce type d’architecture, les configmaps et secrets permettent le chargement des variables d’environnement et autres mots de passe. Cependant si vous utilisez certains frameworks qui nécessitent des fichiers de configuration, vous devrez charger les fichiers dans des volumes. Pour ces derniers, les volumes n’ont pas besoin d’être persistents. Par exemple dans la configuration de votre deployment, vous pourrez configurer le montage d’un volume de la manière suivante: yaml volumeMounts: - mountPath: /config name: configuration-volume readOnly: true - mountPath: /secrets name: secret-volume readOnly: true [...] volumes: - configMap: defaultMode: 420 name: configuration name: configuration-volume - name: secret-volume secret: defaultMode: 420 secretName: secrets Pour intégrer un fichier binaire, on pourra le faire de la manière suivante dans le template HELM: yaml apiVersion: v1 # Definition of secrets kind: Secret [...] type: Opaque # Inclusion of binary configuration files data: my_keystore_jks {% raw %} .Files.Get \"secrets/my_keystore.jks\" | b64enc }} {% endraw %} Vous pouvez définir les fichiers directement dans vos configmaps. Cependant, si vos fichiers sont volumineux, vous aurez du mal à les maintenir. Personnellement, j’opte pour mettre les fichiers de configuration à coté et les charger dans le configmap. On pourra procéder de la manière suivante: yaml apiVersion: v1 kind: ConfigMap [...] data: my_conf: {% raw %}{{- (.Files.Glob \"conf/*\").AsConfig | nindent 2 }} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:1:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#les-config-maps-et-secrets"},{"categories":null,"content":" 2 Livrables agnostiquesUne bonne pratique de développement logiciel est d’externaliser la configuration de vos environnements (ex. l’URL JDBC de la base de données) des livrables. Les charts HELM n’échappent à la règle. On peut stocker la configuration de chaque environnement dans le chart, mais dans ce cas, on perdra beaucoup de souplesse lors des mises à jour des propriétés et cela nous imposera une nouvelle version. On a plusieurs niveaux d’externalisation. Le premier est dans le chart. Vous pouvez externaliser les différentes valeurs dans le fichier values.yml. Ci dessous un exemple avec un autoscaler: yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: labels: [...] spec: maxReplicas: {% raw %} {{.Values.myapp.maxReplicaCount }}{% endraw %} minReplicas: {% raw %} .Values.myapp.minReplicaCount }}{% endraw %} scaleTargetRef: apiVersion: apps/v1 kind: Deployment [...] targetCPUUtilizationPercentage: {% raw %} .Values.myapp.replicationThreesold }}{% endraw %} Les valeurs sont décrites comme suit: yaml myapp: minReplicaCount: \"2\" maxReplicaCount: \"6\" replicationThreesold: 80 Pour externaliser les valeurs d’environnement, vous pourrez donc externaliser un autre fichier values.yml qui sera appliqué au déploiement. Les valeurs de ce dernier surchargeront les valeurs définies dans le chart. Il est important de noter également que les données présentes dans les fichiers de configuration (ex. fichier application.properties) peuvent être « variabilisées » et surchargées par le même mécanisme. Vous aurez à utiliser la commande tpl. yaml apiVersion: v1 kind: ConfigMap metadata: name: configuration labels: [...] data: application.properties: |- {% raw %}{{ tpl (.Files.Get \"conf/application.properties\") . | nindent 4}} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:2:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#livrables-agnostiques"},{"categories":null,"content":" 3 ConclusionVous l’aurez compris, les charts HELM n’échappent pas aux règles déjà connues de gestion des environnements et des livrables. Même si il y a quelques subtilités à connaître pour intégrer des fichiers de configuration par exemple, les grands principes restent les mêmes. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:3:0","series":null,"tags":null,"title":"Gérer « efficacement » les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#conclusion"},{"categories":null,"content":"Depuis quelques années, Kubernetes (K8S) et son écosystème deviennent l’environnement d’ exécution à la mode. Certaines personnes veulent déployer sur cet environnement en mettant en avant ses capacités de scalabilité. D’autres font du bashing (souvent) justifié sur la complexité et le coût de mise en œuvre d’une telle plateforme. Vous l’aurez compris, cette technologie n’échappe pas au cycle du hype et à la fameuse courbe du Gartner. Après quelques expériences sur cette plateforme ( et beaucoup sur d’autres 😀 ) je vais essayer de peser le pour et le contre qui m’apparaissent importants. Bien évidemment, ce n’est que mon avis, j’ai sans doute omis certaines informations qui pourraient être indispensables pour d’ autres. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:0:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#"},{"categories":null,"content":" 1 Pourquoi et dans quelles conditions il ne faut pas utiliser K8S ?Avant de présenter les avantages des applications cloud, je vais essayer de réaliser l’anti thèse de mon propos. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-et-dans-quelles-conditions-il-ne-faut-pas-utiliser-k8s-"},{"categories":null,"content":" 1.1 En avez vous (vraiment) besoin ?Vaste sujet et question délicate pour la population informaticienne qui a tendance à suivre les tendances du marché. Cycle Hype Avant de foncer tête baissée dans cette technologie qui est très intéressante au demeurant, il est important de se poser ces quelques questions: Est-ce que mes SLO sont contraignantes? Quel le cycle de déploiement de mes applications? Qui gère les environnements ? Bref, il faut savoir si le jeu en vaut la chandelle. Si vous avez une application qui doit scaler dynamiquement, encaisser les pics, et avoir du zero downtime durant les mises à jour, Kubernetes est fait pour vous. Si vous avez une application de gestion qui n’a pas d’exigences fortes si ce n’est de répondre aux besoins fonctionnels, l’utilisation de Kubernetes est discutable. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#en-avez-vous-vraiment-besoin-"},{"categories":null,"content":" 1.2 Êtes vous taillé pour ?Kubernetes et son écosystème peuvent s’avérer complexes à appréhender. Si votre entreprise opte pour une utilisation « on premise« , c’est pire. Vous devrez avoir une équipe dédiée qui gérera cette plateforme et offrir une expertise aux équipes de développement. Ne vous trompez pas. Si votre rôle est de développer des applications métier, il vous sera très difficile d’avoir également une expertise sur l’administration de cette plateforme. Vous pourrez l’utiliser et être à l’aise, mais l’administration d’une telle technologie est très compliquée. Le seul conseil que je pourrais vous donner, c’est de ne partir sur Kubernetes que si vous avez une équipe support à disposition. C’est vrai si vous utilisez des services du Cloud tels que Google Cloud ou AWS. Ça l’est encore plus si vous utilisez des services « on premise » tels qu’ Openshift. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#êtes-vous-taillé-pour-"},{"categories":null,"content":" 1.3 Est-ce que vos développements sont « cloud native » ?Au delà de la plateforme, vous devrez monter en compétence sur le développement et la conception de vos applications. Il vous faudra prendre en considération les 12 facteurs clés dans vos applications. Il n’est pas forcément la peine de passer sur des microservices. Il est également possible de faire des monolithes modulaires qui peuvent être légers et stateless. Beaucoup de ces facteurs sont communément admis comme des bonnes pratiques de développement logiciel (ex. Il faut une intégration continue). Aussi, cela va sans dire, il faut également monter (réellement) en compétence sur les conteneurs et leurs contraintes. Si vous n’avez pas l’habitude de travailler avec des conteneurs ( construction, déploiement, disponibilité d’une registry). Il est préférable de définir une trajectoire avec des étapes intermédiaires. Bref, tous ces sujets doivent être adressés et compris pour toutes les parties prenantes de vos équipes que ça soit les développeurs, les chefs de projet et les équipes métiers à une moindre mesure. Cette technologie représente réellement un grand pas à franchir. Si vous ne vous sentez pas de le faire, ou si vous devez gagner en maturité sur ces sujets, attendez avant de vous lancer sur Kubernetes. On ne pourra jamais vous reprocher de ne pas opter sur Kubernetes si vous ne remplissez pas tous les pré-requis. Pour ce qui est du contraire… ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#est-ce-que-vos-développements-sont-cloud-nativehttpswwwredhatcomfrtopicscloud-native-apps-"},{"categories":null,"content":" 1.4 Avez vous des interactions avec des services tiers qui sont compatible avec Kubernetes ?Quand vous restez dans votre cluster Kubernetes, généralement, tout va bien. Dès que vous avez des interactions avec des services tiers, ça peut se compliquer. En effet, généralement vous devrez vous connecter à des services tiers qui ne sont pas orienté cloud : des boitiers crypto, des passerelles de transfert, … Il se peut que certains protocoles soient également incompatibles avec Kubernetes. Il vous faudra vous assurer que tout la galaxie de logiciels et systèmes gravitant autour de votre application sera compatible avec une telle architecture. Ceci n’est pas une mince affaire. L’aide d’une équipe support (voir ci-dessus) vous sera d’une grande utilité. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:4","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#avez-vous-des-interactions-avec-des-services-tiers-qui-sont-compatible-avec-kubernetes-"},{"categories":null,"content":" 2 Pourquoi sauter le pas ?","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-sauter-le-pas-"},{"categories":null,"content":" 2.1 La scalabilité et la résistance à la pannePersonnellement, la première fonctionnalité qui m’a intéressé c’est la gestion de la scalabilité. Si vous avez des objectifs de 99.9% de disponibilité. Kubernetes sera une plus value indéniable dans votre architecture. Après quelques jours heures à batailler avec les fichiers YAML, vous pourrez gérer automatiquement la scalabilité en fonction de plusieurs indicateurs qu’ils soient techniques (ce sont les plus faciles à gérer) ou un peu plus métier en utilisant Prometheus – et oui encore une technologie supplémentaire à connaître. En effet, au lieu de vous en soucier une fois arrivé en production, vous aurez lors du développement l’obligation de prendre en considération l’observabilité de votre application. Par exemple, vous aurez à renseigner si votre application est prête et/ou disponible pour traiter les requêtes. Ces indicateurs vous permettront de scaler automatiquement et de re-créer si nécessaire un POD en cas de panne. J’ai trouvé que cette pratique était vertueuse. Bien évidemment, pas besoin d’être sur Kubernetes pour avoir de l’observabilité dans des applications. Par contre, ici, c’est obligatoire et implémenté dès le développement. La scalabilité automatique est aussi très intéressante. On a souvent vu des serveurs en production qui n’étaient pas suffisamment utilisés. Ici vous n’aurez que les instances nécessaires pour votre cas d’utilisation. La contrainte que l’on peut voir à cette fonctionnalité et qu’on ne maitrise pas complètement le nombre d’instances disponibles. C’est Kubernetes qui s’en charge en prenant en compte le paramétrage que vous aurez renseigné dans vos templates HELM. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#la-scalabilité-et-la-résistance-à-la-panne"},{"categories":null,"content":" 2.2 Le déploiementAvant de déployer (dans la vraie vie), vous aurez à mettre en place un pipeline CI/CD qui orchestre les différents déploiements sur tous vos environnements. Attention, ce n’est pas une mince affaire 🙂 ! Une fois réalisé, vous verrez automatiquement le gain. Vos déploiements seront réellement fluides. Bon OK, on peut le faire sur des VMS standards. Mais on peut améliorer la procédure de déploiement pour mettre en place du zero downtime pour ne pas interrompre le service lors d’un déploiement. yaml strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#le-déploiement"},{"categories":null,"content":" 2.3 L’Infrastructure As Code Quand on pense à Kubernetes, et au cloud, on ne pense pas trop à l’Infrastructure As Code au début. Cependant, cette pratique est pour moi l’une des plus utiles. En effet, avoir votre système décrit dans des fichiers, versionnés vous permet de le tester dès le développement. Ça évite ( dans la majorité des cas ) les erreurs lors des installations d’environnement. La mise à jour des logiciels est largement accélérée. Bien évidemment, il existe Terraform et Ansible pour le provisionning des environnements. Ici je trouve qu’on pousse le concept encore plus loin. L’automatisation est à mon avis poussé à paroxysme. Prenons par exemple la gestion des systèmes d’exploitation. La mise à jour sur des serveurs physiques ou virtuels peut prendre énormément de temps et générer des erreurs. Avec de l’infra as code, ceci est testé et validé automatiquement via des tests unitaires dès l’environnement de développement. On peut suivre la gestion des environnements via un gestionnaire de sources et la promotion vers les autres environnements (recette[1-n], pré-production, production) est grandement accélérée. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#linfrastructure-as-code"},{"categories":null,"content":" 3 ConclusionBon, vous l’aurez peut être compris, cette galaxie de technologies est intéressante et peut vous aider dans vos projets. Avant d’arriver à l’utiliser sereinement, il vous faudra sans doute définir une trajectoire et appréhender plusieurs sujets avant d’arriver à déployer vos applications sur un cloud interne ou externe. J’espère que cet article vous aura permis de mettre en évidence les pour et contre d’une telle technologie et le cas échéant vous donnera envie de franchir le pas. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:3:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au delà de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#conclusion"},{"categories":null,"content":"Derrière ce nom pompeux qui peut effrayer, je vais essayer d’expliquer dans cet article comment on peut versionner facilement ses partitions et les publier sur le web. En cherchant comment mettre de la documentation technique avec des diagrammes PlantUml dans des repos GITLAB et générés avec des pipelines, je me suis mis dans la tête de faire la même chose avec des partitions 🙂 Depuis plusieurs années, j’utilise lilypond pour créer mes partitions. C’est un peu difficile de s’y mettre, mais une fois la syntaxe assimilée, la saisie d’une partition est beaucoup plus efficace. Le rendu des partitions est vraiment optimisé. Si vous voulez plus de détails sur le pourquoi du comment je vous conseille cette page. Vous trouverez des exemples sur le site. J’ai donc eu l’idée de: Stocker ces partitions sur un repo github (jusque là rien d’exceptionnel) Générer automatiquement les partitions au format PDF, PNG et MIDI via une github action (ça commence à devenir intéressant…) Les publier avec les github pages (tant qu’à faire 🙂) ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:0:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#"},{"categories":null,"content":" 1 StockagePourquoi stocker dans un référentiel de sources tel que Github ? Pour les non informaticiens : les partitions sont stockées au format texte. latex \\version \"2.12.1\" \\header { title=\"Try a little tenderness\" composer=\"Harry Woods, Jimmy Campbell \u0026 Reg Connely\" subtitle = \"Commitments Version\" %poet = \"Poete\" instrument = \"Piano\" editor = \"L'éditeur\" %meter=\\markup {\\bold {\"Remarque sur le rhythme\"}} style = \"Soul\" maintainer = \"Alexandre Touret\" maintainerEmail = \"alexandre.touret@free.fr\" maintainerWeb = \"http://blog.touret.info\" lastupdated = \"\" source = \"Music room\" footer = \"Footer\" copyright =\\markup {\\fontsize #-1.5 \"Delivered by A TOURET\"} } upper= \\relative c'{ \\clef treble \\time 4/4 \\tempo 4=176 \\key g \\major d'2 (b4 e d2 b4 a g2 g2 \u003ce g,\u003e2) \u003cfis, c' d\u003e \\bar \"||\" GIT et GITHUB permettent de versionner facilement et pouvoir faire facilement un retour arrière en cas d’erreur. Aussi, GITHUB offre des fonctionnalités « sociales » et collaboratives qui facilitent la revue des modifications ( en cas de travail à plusieurs ). Bref, ça offre la sécurité d’une sauvegarde et la possibilité d’un retour arrière en cas d’erreur. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:1:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#stockage"},{"categories":null,"content":" 2 Générer les partitions avec une github actionLes github actions sont des outils permettant: GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. J’ai donc décidé de créer un workflow qui permet de générer les partitions au format lilypond. J’ai mis à disposition le code sur github sous licence GNU GPLv3. Elle est utilisable telle quelle. Pour créer l’action, il faut créer un fichier action.yml à la racine du repo. Voici le contenu java name: 'Lilypond Generator' description: 'Run Lilypond tool with the given set of arguments to generate music sheets' author: '@alexandre-touret' inputs: args: description: 'Arguments for Lilyponid' required: true default: '-h' runs: using: 'docker' image: 'Dockerfile' args: - ${{ inputs.args }} branding: icon: 'underline' color: 'blue' Vous aurez compris que ce fichier fait référence à une image Docker. Cette dernière n’est ni plus ni moins qu’une Debian avec lilypond d’installé. Pour l’utiliser dans un repo github, on peut créer une action qui l’utilise. Voici un exemple: java jobs: build_sheets: runs-on: ubuntu-latest env: LILYPOND_FILES: \"*.ly\" steps: - name: Checkout Source uses: actions/checkout@v1 - name: Get changed files id: getfile run: | echo \"::set-output name=files::$(find ${{github.workspace}} -name \"${{ env.LILYPOND_FILES }}\" -printf \"%P\\n\" | xargs)\" - name: LILYPOND files considered echo output run: | echo ${{ steps.getfile.outputs.files }} - name: Generate PDF music sheets uses: alexandre-touret/lilypond-github-action@master with: args: -V -f --pdf ${{ steps.getfile.outputs.files }} A la dernière ligne on peut passer les arguments nécessaires à lilypond. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:2:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#générer-les-partitions-avec-une-github-action"},{"categories":null,"content":" 3 PublicationLa c’est l’étape la plus facile :). Il suffit d’activer les github pages et de commiter et pusher les partitions générées java - name: Push Local Changes run: | git config --local user.email \"${{ secrets.GIT_USERNAME }}\" git config --local user.name \"${{ secrets.GIT_EMAIL }}\" git add . git commit -m \"Add changes\" -a - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} Il suffit de créer une page index.md à la racine et d’ajouter des liens vers les partitions générées ( dans mon cas, ça se passe dans le répertoire /docs ). Vous pouvez trouver un exemple ici. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:3:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#publication"},{"categories":null,"content":" 4 ConclusionVoila comment on peut générer un site avec des partitions crées avec Lilypond. Vous trouverez les différents liens ci-dessous. Peut être que je publierai cette action sur le marketplace une fois que j’aurai publié une documentation digne de ce nom :). Action Exemple d’utilisation Exemple de site généré ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:4:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#conclusion"},{"categories":null,"content":"A mes heures perdues, je travaille sur un « POC/side project qui n’aboutira pas et je m’en fiche » basé sur Quarkus. J’ ai choisi d’utiliser les langages et composants suivants : Kotlin Quarkus Gradle Kubernetes pour le déploiement Oui, tant qu’à faire, autant aller dans la hype … Mon projet est sur GITHUB. Pour automatiser certaines actions et, disons-le, par fierté personnelle, j’ai choisi d’automatiser certaines actions par la mise en œuvre de pipelines CI/CD. Depuis peu, GITHUB a intégré un mécanisme de pipeline : GITHUB Actions. Ça permet, entre autres, de lancer des processus automatisé sur un push ou sur une action pour un commit GIT. La force de l’outil est, selon moi, de facilement s’intégrer avec beaucoup de services du cloud ( sonarcloud, google cloud, heroku,…). On aime ou on n’aime pas, mais chez Microsoft, l’intégration ils savent faire. Par exemple, si on veut lancer une compilation lors d’un push, on peut placer un fichier .github/workflows/build.xml avec le contenu : yaml name: CI on: [push] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build with Gradle without testing run: ./gradlew build -x test Coté GITHUB, vous verrez l’exécution sur un écran dédié Vous pouvez créer autant de workflows que vous souhaitez (si votre projet est en libre accès). Pour chaque workflow, on peut définir et utiliser des jobs. Les logs d’exécution sont disponibles dans ce même écran: ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:0:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#"},{"categories":null,"content":" 1 Worflows implémentésJ’ai choisi d’implémenter les workflows suivants: CI: Build sur la feature branch CD: Build sur master branch et déploiement On obtient donc dans mon cas: Ce n’est pas parfait. Loin de là. Dans la « vraie vie », pour une équipe de dev, je l’améliorerai sans doute par un build docker dans les features branches, une validation formelle et bloquante de l’analyse sonar, etc. Pour un dev perso ça suffit largement. Le contenu de la branche master est compilé et une image docker est crée pour être déployée automatiquement dans GKE. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:1:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#worflows-implémentés"},{"categories":null,"content":" 2 Analyse SONARJ’ai choisi d’utiliser sonarcloud pour analyser mon code. C’est gratuit pour les projets opensource. L’analyse se fait simplement: yaml sonarCloudTrigger: name: SonarCloud Trigger runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: SonarCloud Scan env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }} run: ./gradlew jacocoTestReport sonarqube Dans ce job j’utilise deux secrets. Ce sont des tokens qui permettent de ne pas stocker en dur les données dans les repos GITHUB. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:2:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#analyse-sonar"},{"categories":null,"content":" 3 Création d’une image Docker et déploiement dans le registry GITHUBIci aussi, ça se fait simplement. La preuve : yaml jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build in JVM Mode with Gradle without testing run: ./gradlew quarkusBuild [1] - name: Branch name run: echo running on branch ${GITHUB_REF##*/} - name: Build the Docker image Quarkus JVM run: docker build -f src/main/docker/Dockerfile.jvm -t docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest . [2] - name: Login against github docker repository env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: docker login -u ${GITHUB_ACTOR} -p ${GITHUB_TOKEN} docker.pkg.github.com [3] - name: Publish the Docker image Quarkus JVM run: docker push docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest [4] Création du binaire Création de l’image docker en utilisant la commande docker et le Dockerfile fourni par Quarkus Identification sur la registry Docker de GITHUB Déploiement de l’image Pour plus de détails sur la variable GITHUB_TOKEN, vous pouvez lire cet article de la documentation. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:3:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#création-dune-image-docker-et-déploiement-dans-le-registry-github"},{"categories":null,"content":" 4 Déploiement dans Google Kubernetes EngineMon application est pour l’instant architecturée comme suit (attention c’est compliqué): Pour la déployer dans Google Kubernetes Engine, j’ai besoin d’ implémenter cette « architecture » par les objets Kubernetes suivants: J’utilise les objets suivants: Des services pour exposer la base de données ainsi que l’application Un deployment pour l’application Des pods car à un moment, il en faut… Un statefulset pour la base de données Vous pourrez trouver la définition de tous ces objets au format yaml via ce lien. J’ai fait très simple. Logiquement j’aurai du créer un volume pour les bases de données ou utiliser une base de données en mode PAAS. Pour lancer le déploiement, il faut au préalable créer un secret ( fait manuellement pour ne pas stocker d’objet yaml dans le repository GITHUB) pour se connecter au repo GITHUB via la commande suivante: bash kubectl create secret docker-registry github-registry --docker-server=docker.pkg.github.com --docker-username=USER--docker-password=PASSWORD --docker-email=EMAIL On peut faire pareil pour les connexions base de données. J’ai mis dans un configmap pour ne pas trop me prendre la tête… Après le déploiement via le pipeline se fait assez simplement: yaml [...] - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master with: version: '286.0.0' service_account_email: ${{ secrets.GKE_SA_EMAIL }} service_account_key: ${{ secrets.GKE_SA_KEY }} project_id: ${{ secrets.GKE_PROJECT }} # Get the GKE credentials so we can deploy to the cluster - run: |- gcloud container clusters get-credentials \"${{ secrets.GKE_CLUSTER }}\" --zone \"${{ secrets.GKE_ZONE }}\" # Deploy the Docker image to the GKE cluster - name: Deploy run: |- kubectl apply -f ./k8s J’utilise les « actions » fournies par Google. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:4:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#déploiement-dans-google-kubernetes-engine"},{"categories":null,"content":" 5 ConclusionPour que ça marche il y a pas mal d’étapes préalables ( des tokens à générer, un utilisateur technique, …). J’ai essayé de les référencer dans le README du projet. Si vous voulez tester l’intégration Kubernetes dans le cloud google, sachez que vous pouvez disposer d’un crédit de 300€ valable un an. Attention, avec ce genre d’architecture, ça part vite… ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:5:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour déployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#conclusion"},{"categories":null,"content":"Mon PC Lenovo a un SSD. Le temps de démarrage est actuellement de 11 sec. Ça commence à faire pas mal… J’ai eu donc envie de me pencher sur l’optimisation du démarrage ( encore une fois) . Voici comment gagner (facilement) quelques secondes au démarrage. Boot time Tout d’abord, vous devez analyser les services qui prennent du temps au démarrage. Vous pouvez le faire avec cette commande: bash systemd-analyze plot \u003e plot.svg J’ai obtenu le graphique suivant: Boot initial ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:0:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#"},{"categories":null,"content":" 1 Configuration GRUBLa première manipulation à réaliser est de désactiver le timeout de GRUB. Pour celà, vous pouvez modifier la variable GRUB_TIMEOUT dans le fichier /etc/default/grub: ini GRUB_TIMEOUT=0 Ensuite, vous devez mettre à jour la configuration GRUB en exécutant cette commande: bash sudo update-grub2 Au prochain reboot, vous ne verrez plus le menu GRUB. ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:1:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-grub"},{"categories":null,"content":" 2 Configuration NetworkManagerDans mon cas, le service NetworkManager-wait-online.service prenait près de 9 secondes. Après avoir lu plusieurs billets et rapports de bug, je me suis aperçu que je pouvais le désactiver au boot. Vous pouvez le faire en lançant la commande suivante bash sudo systemctl disable NetworkManager-wait-online.service ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:2:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-networkmanager"},{"categories":null,"content":" 3 Configuration AptUn autre service qui prenait pas mal de temps était apt-daily.timer qui vérifiait au boot qu’il y avait des mises à jour de l’OS. Après quelques recherches, j’ ai vu qu’on pouvait soit le désactiver ( ce qui n’est pas recommandé pour les mises à jour de sécurité ) soit décaler la recherche. J’ai choisi cette solution. Vous devez donc exécuter la commande suivante: bash sudo systemctl edit apt-daily.timer Et renseigner le contenu suivant: ini [Timer] OnBootSec=15min OnUnitActiveSec=1d AccuracySec=1h RandomizedDelaySec=30min Ce service sera donc lancé 15 minutes après le boot. Ce qui est largement suffisant. [EDIT] Vous pouvez appliquer la même configuration pour le service apt-daily-upgrade en exécutant la commande: bash sudo systemctl edit apt-daily-upgrade.timer Ensuite, vous pouvez recharger la configuration en exécutant cette commande: bash sudo systemctl daemon-reload ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:3:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-apt"},{"categories":null,"content":" 4 RésultatsAprès ces quelques manipulations qui peuvent prendre 5 minutes grand maximum, j’ai réussi à optimiser le boot en réduisant le démarrage à 5 secondes! Boot (après) Vous pourrez trouver le détail ci-dessous: Détail du Boot (après) ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:4:0","series":null,"tags":["debian","planetlibre"],"title":"Améliorer le temps de démarrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#résultats"},{"categories":null,"content":"Avec les contraintes liées au confinement, les répétitions se font de plus en plus rares. Pour ne pas perdre la main, il y a quelques logiciels qui permettent de jouer d’un instrument et d’ improviser tout en ayant une bande son en fond musical. Il y a plusieurs logiciels payants/propriétaires sur différentes plateformes: Band in a box irealpro Garage band Jjazzlabs J’ai découvert ce dernier récemment en naviguant sur le site Linux Mao. Il a l’avantage d’être gratuit (le moteur est sous licence LGPL 3.0, pas le logiciel en tant que tel), de fonctionner sous GNU/LINUX, d’ offrir un son pas mal du tout et de permettre la configuration de la dynamique au fur et à mesure du morceau. Je vais expliquer comment l’installer sur Debian. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:0:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#"},{"categories":null,"content":" 1 Configuration Midi","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration-midi"},{"categories":null,"content":" 1.1 Activation des périphériques virtuels MIDICréer un fichier /etc/modules.load.d/midi.conf avec le contenu suivant: bash snd-virmidi Ensuite créer le fichier /etc/modprobe.d/midi.conf avec le contenu suivant: bash options snd-virmidi midi_devs=1 Logiquement à ce stade, lors du prochain reboot, vous aurez un périphérique virtuel MIDI activé. En attendant vous pouvez lancer la commande suivante bash $ sudo modprobe snd-virmidi midi_devs=1 ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:1","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#activation-des-périphériques-virtuels-midi"},{"categories":null,"content":" 1.2 Synthétiser du MIDIPour faire fonctionner ce logiciel, il faut installer une banque de son ( au format SF2) et un logiciel permettant de l’utiliser pour synthétiser du MIDI. La banque de son recommandée est disponible via ce lien. Téléchargez là et copiez la dans un répertoire accessible. Pour le second, il vous faudra installer fluidsynth. Voic les quelques commandes à lancer: bash $ sudo apt install fluid-synth qsynth ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:2","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#synthétiser-du-midi"},{"categories":null,"content":" 1.3 Petite vérification…Avant d’ aller plus loin dans la configuration de fluidsynth, vous pouvez vous assurer que tout est OK en récupérant un fichier MIDI et en lançant la commande suivante: bash $ fluidsynth -a pulseaudio -m alsa_seq -l -i /opt/JJazzLab-2.0-Linux/JJazzLab-SoundFont.sf2 MIDI_sample.mid Normalement vous devriez avoir du son. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:3","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#petite-vérification8230"},{"categories":null,"content":" 1.4 Configurer fluidsynthLancez qsynth et cliquez sur le bouton « configuration » Vous trouverez ci-dessous la configuration que j’ai appliqué. Elle diffère légèrement de celle présentée dans la documentation. Pensez à redémarrer fluidsynth après application de ces nouveaux paramètres. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:4","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-fluidsynth"},{"categories":null,"content":" 1.5 Configurer aconnectDisclaimer: La c’est la partie la plus obscure… Il faut maintenant « brancher » la sortie du synthétiseur virtuel MIDI à fluidsynth pour que le son MIDI soit interprété par ce dernier à travers sa banque de son. Ce n’est pas intuitif, je vous avais prévenu … Je ne vous parle pas de la pseudo interface graphique à aconnect. La ligne de console est plus parlante ( c’est pour dire ) . Exécutez la commande suivante: bash $ aconnect -lo client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Dans mon cas, je vais avoir à connecter le client 24:0 au synthétiseur 128:0 grâce à la commande : bash $ aconnect 24:0 128:0 Maintenant, si on relance la commande aconnect -lo on obtient le résultat suivant: bash client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' Connexion À: 128:0 client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Connecté Depuis: 24:0 Attention, cette commande devra être lancée ( ainsi que fluidsynth) avant chaque démarrage de jjazzlab. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:5","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-aconnect"},{"categories":null,"content":" 2 Installation de JjazzlabTéléchargez les binaires sur ce site, puis décompressez l’archive dans le répertoire /opt par ex. Vous devez également installer java bash $ sudo apt install openjdk-11-jdk Ensuite, vous devez créer le fichier ~/.local/share/applications/jjazzlab.desktop avec le contenu suivant: ini [Desktop Entry] Type=Application Name=JJazzLab GenericName=JJazzLab Icon= Exec=\"/opt/JJazzLab-2.0-Linux/bin/jjazzlab\" Terminal=false Categories=Audio;Music;Player;AudioVideo; Maintenant vous pouvez directement démarrer JJazzlab via le menu. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:2:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#installation-de-jjazzlab"},{"categories":null,"content":" 3 ConfigurationUne fois jjazzlab démarré, vous devez aller dans le menu « Tools\u003eOptions » et sélectionnez les valeurs suivantes: Ouvrez un fichier example (ex. sunny ) Cliquez sur le menu décrit par un clavier Puis configurez comme suit: Maintenant vous pouvez télécharger les standards fournis sur le site et improviser dessus 🙂 ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:3:0","series":null,"tags":["debian","musique","planetlibre"],"title":"Répéter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration"},{"categories":null,"content":"Voici un rapide article sur un problème rencontré récemment. Lors de l’exécution d’un container docker, j’ai eu une erreur SIGSEGV 139. Un crash avec aucune log. Bref que du bonheur 🙂 Avant d’aller plus loin voici mon environnement: Debian 10 Docker CE 19.03.8 Après quelques recherches, je me suis rendu compte qu’on pouvait reproduire ce comportement en exécutant cette commande: bash docker run -it gcc:4.8.5 Une des raisons trouvées serait un problème de compatibilité avec le noyau 4.8.5 (oui ça remonte…). Une solution est d’activer l’émulation vsyscall. Voici la configuration à effectuer: Dans le fichier /etc/default/grub, ajouter la ligne suivante: ini GRUB_CMDLINE_LINUX_DEFAULT=\"quiet vsyscall=emulate\" Puis lancer les commandes suivantes: bash $ sudo update-grub $ sudo reboot Maintenant le container devrait pouvoir s’exécuter correctement. ","date":"2020-04-01","objectID":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Erreur 139 à l'exécution d'un container docker","uri":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/#"},{"categories":null,"content":"Suite aux premières annonces de distanciation sociale ( avant que le confinement soit effectif ) j’ai acheté en catastrophe un PC portable. Les critères étaient : 8Go de RAM, un disque SSD … et la compatibilité GNU/LINUX :). N’ayant pas trop de temps pour chercher la bonne affaire ( technologique et financière ), j’ ai acheté un Dell Inspiron 14-3493. Je n’ai pas pris trop de risques. Bien que livré avec Windows 10, ce modèle est déjà certifié compatible Ubuntu. L’installation d’Ubuntu se passe très bien. C’est plié en moins de 30mn. Du coup, je ne la détaillerai pas dans cet article – si vous êtes intéressé, vous pouvez consulter cet article. Pour les pré-requis, c’est une autre paire de manches … Voilà les différentes actions que j’ai réalisé au préalable ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:0:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#"},{"categories":null,"content":" 1 Redémarrer l’ordinateur et accéder au BIOSLà, j’ai un peu galéré pour accéder au BIOS. La seule manipulation que j’ai trouvé et de lancer le menu « Démarrage avancé » puis sélectionner « Utiliser un périphérique ». Vous pouvez donc sélectionner le disque dur. Au boot en appuyant sur la touche F12 et/ou F2, vous pouvez accéder au BIOS. ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:1:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#redémarrer-lordinateur-et-accéder-au-bios"},{"categories":null,"content":" 2 Configuration du BIOSVoila les paramètres que j’ai appliqué: Dans le menu “SATA Operation”: vous devez sélectionner AHCI au lieu de RAID. Dans le menu “Change boot mode settings \u003eUEFI Boot Mode” , vous devez désactiver le Secure Boot. Une fois réalisé, vous pouvez redémarrer en appuyant sur la touche F2 et/ou F12. Si vous n’arrivez pas à revenir sur le BIOS pour indiquer de booter sur votre clé USB, vous obtiendrez un écran d’erreur Windows dû à la configuration AHCI. Personnellement, en redémarrant une ou deux fois, j’ai obtenu un écran de démarrage avancé qui m’a permis de sélectionner le périphérique (ma clé USB) sur lequel démarrer. Maintenant vous pouvez accéder à l’installeur Ubuntu et profiter 🙂 ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:2:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#configuration-du-bios"},{"categories":null,"content":" 3 Après l’installationJe n’ai rien fait de particulier si ce n’est configurer le trackpad. Pour cela, j’ai installé gnome-tweaks. Mis à part ça, tout fonctionne très bien! ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:3:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#après-linstallation"},{"categories":null,"content":"Java 8 est encore largement utilisé dans les entreprises aujourd’hui. Il y a même certains frameworks qui n’ont pas encore sauté le pas. Je vais essayer d’exposer dans cette article les étapes à réaliser pour migrer (simplement) votre application JAVA8 en JAVA 11. Dans cet article, je prendrai comme postulat que l’application se construit avec Maven. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:0:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#"},{"categories":null,"content":" 1 Pré-requisTout d’abord vérifiez votre environnement d’exécution cible! Faites un tour du coté de la documentation et regardez le support de JAVA. Si vous utilisez des FRAMEWORKS qui utilisent des FAT JARS, faites de même (ex. pour spring boot, utilisez au moins la version 2.1.X). Ensuite, vous aurez sans doute à mettre à jour maven ou gradle. Préférez les dernières versions. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:1:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#pré-requis"},{"categories":null,"content":" 2 Configuration mavenLes trois plugins à mettre à jour obligatoirement sont : maven-compiler-plugin maven-surefire-plugin maven-failsafe-plugin ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#configuration-maven"},{"categories":null,"content":" 2.1 Maven compiler plugin xml \u003cplugin\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cversion\u003e3.8.1\u003c/version\u003e \u003cconfiguration\u003e \u003crelease\u003e11\u003c/release\u003e \u003cencoding\u003eUTF-8\u003c/encoding\u003e \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:1","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-compiler-plugin"},{"categories":null,"content":" 3 maven surefire / failsafe pluginPour ces deux plugins, ajouter la configuration suivante: xml \u003cplugin\u003e \u003cartifactId\u003emaven-surefire-plugin\u003c/artifactId\u003e \u003cversion\u003e2.22.2\u003c/version\u003e \u003cconfiguration\u003e [...] \u003cargLine\u003e--illegal-access=permit\u003c/argLine\u003e [...] \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:3:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-surefire--failsafe-plugin"},{"categories":null,"content":" 4 Mise à jour des librairiesBon,la il n’y a pas de magie. Vous devez mettre à jour toutes vos librairies. Mis à part si vous utilisez des librairies exotiques, la plupart supportent JAVA 11 maintenant. C’est une bonne opportunité de faire le ménage dans vos fichiers pom.xml 🙂 ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:4:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#mise-à-jour-des-librairies"},{"categories":null,"content":" 5 APIS supprimées du JDKSi vous faites du XML, SOAP ou que vous utilisiez l’API activation, vous devez désormais embarquer ces librairies. Le JDK ne les inclut plus par défaut. Par exemple: xml \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-core\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-impl\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ejavax.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-api\u003c/artifactId\u003e \u003cversion\u003e2.3.1\u003c/version\u003e \u003c/dependency\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:5:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#apis-supprimées-du-jdk"},{"categories":null,"content":" 6 Modularisation avec JIGSAWBon là … je vous déconseille de partir directement sur la modularisation, surtout si vous migrez une application existante. Bien que la modularité puisse aider à réduire vos images docker en construisant vos propres JRE et d’améliorer la sécurité, elle apporte son lot de complexité. Bref pour la majorité des applications, je vous déconseille de l’intégrer. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:6:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#modularisation-avec-jigsaw"},{"categories":null,"content":" 7 ConclusionAvec toutes ces manipulations, vous devriez pouvoir porter vos applications sur JAVA11. Il y aura sans doute quelques bugs. Personnellement, j’en ai eu avec CGLIB vs Spring AOP sur une classe instrumentée avec un constructeur privé. Sur ce coup j’ai contourné ce problème ( je vous laisse deviner comment 🙂 ). ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:7:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#conclusion"},{"categories":null,"content":"Depuis quelques temps je me mets à Gradle. Après de (trop?) nombreuses années à utiliser Maven (depuis la version 0.9…), je me risque à modifier mon environnement de build. Du moins sur des projets démo. Quand on a fait pas mal de Maven, on est un peu dérouté au début. On a d’un coté, la plupart des actions qui sont configurées de manière implicite et de l’autre on peut tout coder/étendre ou presque. Je ne vais pas me risquer à faire un comparatif des deux outils. Gradle ( donc fortement orienté ) en a fait un. Je vais plutôt décrire avec cet article comment on peut démarrer rapidement en configurant son environnement pour être utilisé en entreprise. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:0:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#"},{"categories":null,"content":" 1 InstallationLe plus simple est d’utiliser SDKMAN. Voici la manipulation pour l’installer: java $ curl -s \"https://get.sdkman.io\" | bash $ source \"$HOME/.sdkman/bin/sdkman-init.sh\" $ sdk install gradle 6.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:1:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#installation"},{"categories":null,"content":" 2 Configuration d’un proxyEt oui comment souvent, passer le proxy d’entreprise est la moitié du boulot :). Pour le configurer de manière globale (c.-à-d. pour tous vos projets) sur votre poste de travail, vous devez créer un fichier gradle.properties dans le répertoire $HOME/.gradle : java systemProp.http.proxyHost=proxy systemProp.http.proxyPort=8888 systemProp.http.nonProxyHosts=localhost|127.0.0.1 systemProp.https.proxyHost=proxy systemProp.https.proxyPort=8888 systemProp.https.nonProxyHosts=localhost|127.0.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:2:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-proxy"},{"categories":null,"content":" 3 Configuration d’un miroir Nexus ou ArtifactoryA l’instar du proxy, on va essayer de mettre en place une configuration globale. Pour ce faire, on va utiliser les init scripts. Cette fonctionnalité est très intéressante. Elle permet de centraliser des actions et configurations. Pour créer un script, il faut tout d’abord créer un fichier .gradle dans le répertoire $HOME/.gradle/init.d. Voici un exemple pour Nexus: java allprojects { buildscript { repositories { mavenLocal() maven {url \"https://url-nexus\"} } } repositories { mavenLocal() maven { url \"https://url-nexus\"} } } ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:3:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-miroir-nexus-ou-artifactory"},{"categories":null,"content":" 4 Configuration du déploiement dans Nexus / ArtifactoryLe déploiement dans Nexus est possible via le plugin maven publish. La configuration fournie dans la documentation est tellement bien faite ( comme le reste d’ailleurs ) que je ne vais que mettre un lien vers celle-là: Voici le lien. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:4:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-du-déploiement-dans-nexus--artifactory"},{"categories":null,"content":" 5 ConclusionAprès ces quelques actions vous pourrez démarrer des builds avec gradle tout en étant compatible avec un environnement « Maven ». Enjoy 🙂 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:5:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#conclusion"},{"categories":null,"content":"Je suis en train de mettre en œuvre des tests de performance avec Gatling. Un des principaux outils libres de tests de performance. J’ai eu récemment à résoudre un « petit » soucis : je souhaitai partager des variables entre plusieurs scénarios. Il existe pas mal de solutions sur stackoverflow. J’ai condensé certaines d’entre elles pour les adapter à mon besoin. Ces variables sont issues de exécution d’une seule requête et sont automatiquement injectées dans les scénarios suivants. Ce mécanisme permet par exemple de récupérer un jeton d’un serveur d’identification et de l’injecter pour le scénario que l’on souhaite tester. Pour ce faire, il faut ajouter une variable de type LinkedBlockingDeque et injecter le contenu choisi via la session java val holder = new LinkedBlockingDeque[String]() ... val firstScenario = scenario(\"First Simulation\") .exec(http(\"first scenario\") .post(\"/base/url1\") .check(jsonPath(\"$.my_variable\").find.saveAs(\"variable\"))) .exec(session =\u003e { holder.offerLast(session(\"variable\").as[String]) session} ); Maintenant on peut l’utiliser dans un autre scénario comme feeder: java val secondScenario = scenario(\"Second Simulation\") .feed(sharedDataFeeder) Voici l’exemple complet En espérant que cela puisse aider à certain.e.s d’entre vous 🙂 ","date":"2019-11-21","objectID":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/:0:0","series":null,"tags":["gatling","planetlibre","scala"],"title":"Partager des variables entre scénarios gatling","uri":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/#"},{"categories":null,"content":"Une fois n’est pas coutume, voici un article qui reprend des basiques de la programmation. J’aborde une stack JAVA, mais c’est applicable à d’autres langages. Il existe une fonctionnalité très intéressante dans Spring (et dans J(akarta)EE) que l’on oublie assez souvent : l’AOP ou encore la programmation par aspect. Cette manière de programmer permet notamment de séparer le code fonctionnel et technique. Si vous faites du JAVA, vous utilisez déjà l’AOP. En effet, quand vous faites une insertion en base via JPA dans un EJB ou un bean annoté @Transactional, une transaction est initiée au début de la méthode et fermée à la fin. Avec Spring et notamment dans Spring boot, voici comment initier l’AOP. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:0:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#"},{"categories":null,"content":" 1 Configuration mavenAjouter le starter AOP: xml \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-aop\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:1:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#configuration-maven"},{"categories":null,"content":" 2 Activation des aspectsDans la configuration ci-dessous, je prendrai comme exemple le logging des méthodes ( un log en début de méthode et un log en fin ). La définition des aspects se fait dans des classes annotées par @Configuration. java @Configuration @Aspect @ConditionalOnProperty(name = \"debug.enabled\", havingValue = \"true\") public class DebuggingConfiguration { private static final Logger LOGGER = LoggerFactory.getLogger(DebuggingConfiguration.class); private static final String WITHIN_MY_PACKAGE = \"within(my.package..*)\"; /** * Log before execution * * @param joinPoint the current method */ @Before(WITHIN_MY_PACKAGE) public void logBeforeExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"Beginning of method : [{}]\", joinPoint.getSignature().getName()); } } /** * Log after execution * * @param joinPoint the current method */ @After(WITHIN_MY_PACKAGE) public void logAfterExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"End of method : [{}]\", joinPoint.getSignature().getName()); } } } L’utilisation de l’ annotation @ConditionalOnProperty me permet d’activer cette classe de configuration seulement si la propriété debug.enabled est initialisée à true. Les annotations @Before et @After indiquent à Spring AOP quand exécuter ces méthodes ou sur quelles méthodes. Dans mon cas, quand les méthodes appelées sont définies dans les classes d’un package défini. Pour plus de détails sur la syntaxe et les possibilités, vous pouvez vous référer à la documentation. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:2:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#activation-des-aspects"},{"categories":null,"content":"Après avoir soumis mon article sur le coaching des développeurs, je me suis rendu compte que j’ai oublié pas mal de points qui, à bien y réfléchir, me paraissent essentiels. Dans mon précédent article ( the first blood pour le coup ) je me suis attardé sur le « quoi » : toutes les actions que j’ai testé dans l’encadrement des jeunes développeurs et des développeurs en général. Maintenant, je vais essayer de m’attarder sur le « comment » : ma démarche, la posture que l’on doit adopter ( ce n’est que mon ressenti ) etc. Je vais commencer par ce dernier point. Quand on est architecte, développeur sénior ou bien encore tech lead, on est amené à encadrer techniquement des développeurs. Vous pouvez adopter plusieurs postures: A ce stade de lecture de cet article, vous vous dites, quelle est la bonne photo et donc la posture à adopter ? A mon avis, elles sont à proscrire individuellement. Je pense qu’il faut les panacher. Tout d’abord, il faut se souvenir de notre début de carrière et se rappeler du code que l’on a réalisé. J’ai par exemple gardé les premiers programmes réalisés en entreprise ( Servlet, JSP, JAVA 1.2, des méthodes de 3km de long, de la duplication de code en veux tu en voila, …) . Ça me permet de relativiser, d’être assez compréhensif et d’éviter de prendre les gens de haut. Cependant, cette prise de conscience ne doit pas vous empêcher de faire progresser votre entourage et surtout de leur faire éviter les écueils que vous avez vécu. Les ateliers et documentation que vous pourrez leur transmettre sont donc primordiaux. Par exemple, faire lire « Clean Code » ou « Effective Java » aux développeurs – je ne l’oblige pas mais incite fortement – est un moyen de leur faire gagner du temps dans leur apprentissage du code. Ensuite, même si vos padawans vous voient soit comme Pascal le grand frère ou maître Yoda (pour flatter mon égo), il ne faut pas oublier les exigences que vous avez fixé. L’industrie logicielle a gagnée en maturité en favorisant par exemple l’industrialisation via les outils de CI/CD ou bien encore en facilitant l’application de principes de qualité via des outils d’analyse des dépendances (dependency track) et du code (sonarqube). Vous devez vous adapter, favoriser l’adoption de ces pratiques et imposer quelques étapes qualité de préférence automatisée via de la CI. Pour favoriser l’adoption de toutes vos exigences, je conseille d’y aller progressivement. Il ne faut pas oublier que votre objectif est de faire « grandir » vos collègues. Pour cela essayez de les adapter et les faire évoluer dans le temps. Par exemple, pour les tests unitaires, commencez pas mettre en place les différents indicateurs qui vous permettront de mesurer la couverture de code. Ensuite, exigez un niveau de couverture de code (ex. 30%). Suivez le, via les quality gates SonarQube et enfin augmentez le progressivement : 30% , 40%,… Si vous commencez dès le début par un objectif trop haut, ce dernier paraîtra inatteignable et découragera tout le monde. Mieux vaut commencer volontairement très bas pour favoriser l’adoption. Dans un autre domaine, pour vos workflows GIT, vous pouvez commencer dans un premier temps par le workflow de feature branch. Ce dernier posera les bases des pipelines CI, des merge requests et des bonnes pratiques liées à la gestion de configuration. Une fois tout le cérémonial lié à GIT assimilé par votre équipe, passer à GITFLOW sera beaucoup simple. Bref, cette démarche revient à parler de conduite du changement. Il faut identifier vos exigences minimales. Celles-ci doivent être acceptées par votre hiérarchie ET par vos collègues. Sans ça vous échouerez! Si ils vous soumettent quelques idées ou adaptations, n’hésitez pas à les incorporer. Ça peut faciliter l’adoption! Ensuite, planifiez une progression sur 1 ou 2 ans. Cela donnera à vos collègues dans un premier temps des premiers objectifs atteignables puis une marge de progression leur permettant de s’améliorer. Enfin, n’hésitez pas à faire un bilan ( par ex. a","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ? The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#"},{"categories":null,"content":" 1 ConclusionA mon avis le management et l’encadrement de personnes n’est pas à prendre à la légère. Votre attitude ainsi que la démarche que vous voulez mettre en œuvre feront autant voir plus que toute la documentation et formations que vous mettrez en place. ","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ? The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#conclusion"},{"categories":null,"content":"Auparavant, dans nos tests, quand on voulait mocker des méthodes « final » ou statiques, on devait passer par PowerMock. Depuis peu, si on utilise Mockito ( \u003e2.1) , on n’a plus besoin d’ajouter PowerMock pour mocker des méthodes « final ». Bon il reste toujours la gestion des méthodes statiques à gérer autrement qu’avec Mockito, mais cela va dans le bon sens. Voici comment activer en quelques commandes le mocking des méthodes « final ». Dans le répertoire src/test/resources, il faut créer un répertoire mockito-extensions avec un fichier nommé org.mockito.plugins.MockMaker. bash src/test/resources └── mockito-extensions └── org.mockito.plugins.MockMaker A l’intérieur de ce fichier, vous devrez ajouter le contenu suivant : bash mock-maker-inline Avec cette configuration, vous pourrez dorénavant mocker des méthodes « final » 🙂 Enjoy ","date":"2019-08-16","objectID":"/2019/08/16/mocker-des-methodes-final-avec-mockito/:0:0","series":null,"tags":["java","mockito","planetlibre","tests-unitaires"],"title":"Mocker des méthodes « final » avec Mockito","uri":"/2019/08/16/mocker-des-methodes-final-avec-mockito/#"},{"categories":null,"content":"Juste pour un pense bête, voici comment paramétrer GIT et GITHUB/GITLAB pour signer les commits avec GPG. ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:0:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#"},{"categories":null,"content":" 1 Configuration GPGExécutez la commande suivante : bash gpg --full-generate-key Sélectionnez une clé RSA (question 1) de 4096 bits (question 2). Une fois cette commande effectuée, vous pouvez récupérer votre clé GPG avec cette commande: bash gpg --list-secret-keys --keyid-format LONG alexandre@.... /home/alexandre/.gnupg/pubring.kbx ---------------------------------- sec rsa4096/XXXXXXXXXX 2019-08-09 [SC] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX uid [ ultime ] Alexandre Touret \u003cmon.mail.github.ou.gitlab@monprovider.fr\u003e ssb rsa4096/XXXXXXXXXX 2019-08-09 [E] Ensuite, il faut exécuter cette commande bash gpg --armor --export XXXXXXXXXX ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:1:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-gpg"},{"categories":null,"content":" 2 Configuration GITIndiquez la clé GPG à GIT bash git config --local user.signingkey XXXXXXXXXXXX Et indiquez que vous voulez signer tous vos commits bash git config --local commit.gpgsign true Si vous ne faites pas cette dernière commande, vous devrez ajouter l’option -S à chaque exécution de la commande git commit. Exemple: bash git -a -S -m \"Ajout javadoc\" ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:2:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-git"},{"categories":null,"content":" 3 Configuration GITHUBSur Github ( il y a la même chose sur gitlab), vous pouvez dans vos paramètres ajouter cette clé . De cette manière, vos prochains commits envoyés seront vérifiés. En espérant que ça serve à d’autres 🙂 ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:3:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"Vérifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-github"},{"categories":null,"content":"En changeant de société l’année dernière j’ai eu l’impression de monter d’un cran dans la pyramide des ages. Pour faire plus simple, je me suis senti un peu plus vieux. Si vous avez quelques années d’expérience dans le développement ou tout simplement dans la technique, vous avez déjà eu l’occasion de coacher ou d’encadrer techniquement des jeunes diplômés. Et oui, c’est un signe ! Maintenant vous avez assez de recul ( pour ne pas dire que vous êtes vieux/vieille) pour encadrer techniquement des jeunes ingénieur.e.s Certes vous n’avez pas fait le choix de partir vers la gestion de projet ou le management. Cependant l’encadrement technique ( vous pouvez l’appeler mentorat, tutorat, apprentissage,… ) est nécessaire pour faire monter en compétence les nouveaux arrivants et les rendre autonomes. Je vais essayer de mettre en lumière quelques pratiques que je mets en œuvre et que j’ai pu remettre au goût du jour depuis un an. Si vous avez des idées, avis, n’hésitez pas à les mettre en commentaire. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#"},{"categories":null,"content":" 1 DocumentationIl y a plusieurs types de documentation que je partage. Tout d’abord, j’ai partagé quelques sites et ouvrages qui me paraissent indispensables. Clean Code arrive en premier. Effective Java en second. A mon avis, ça ne sert pas à grand chose d’aller plus loin dans le développement si on n’a pas acquis les notions décrites dans ces livres! Puis vient le refactoring puis les design patterns. Ensuite, j’essaye de partager via notre chat interne les quelques solutions trouvées dans les projets. Enfin, j’ essaye de m’ astreindre à mettre à jour la documentation. Oui c’est un combat de tous les jours 😀 Ça commence par les exemples de code. J’essaye d’ avoir des repos git assez lisibles (c.-à-d. avec un README intelligible) et un code à jour correspondant aux normes en vigueur. Un exemple, j’ai crée un projet permettant d’ illustrer la mise en œuvre des tests unitaires et d’intégration dans un projet standard (spring, tomcat, docker,…). Ces éléments nécessitent un travail important, que ça soit à la création ou pour tenir à jour la documentation. Cependant, ça me permet de ne pas me répéter, et d’ illustrer via un cas pratique ce que j’attends dans les Merge Requests. En effet, chaque développement est assujetti à une Definition of Done ( tests, qualité, …) . Il faut donc que la qualité de la documentation soit en rendez vous ! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#documentation"},{"categories":null,"content":" 2 VeilleAu delà de la documentation, je « pousse » aux différents dev, les articles que je trouve pertinent pendant ma veille technologique. J’invite également tout le monde à en faire. Je ne peux pas les obliger. Maintenant comme je peux le dire régulièrement. Si on souhaite rester dans la technique, il faut se tenir à jour. La veille (sites web, confs, livres,…) en est le meilleur moyen. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:2:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#veille"},{"categories":null,"content":" 3 Ateliers / WorkshopsOrganiser un workshop ou atelier d’une heure ou deux max est un bon moyen de fédérer les troupes. J’essaye d’organiser deux types d’atelier. Le premier est uni directionnel : Une personne présente un sujet technique et les autres en profitent. Ça permet tout d’abord de diffuser plus simplement certains messages. Par exemple, j’ai organisé une présentation de 30 mn sur l’utilisation de NULL dans le code et l’utilisation des Optional. Le deuxième est plus long à préparer. C’est un atelier organisé à la manière d’un hands on sur un sujet très précis. Pendant 1H ou 2H, l’équipe planche sur un sujet. La session est organisé et animé idéalement par un ou plusieurs membres de l’équipe ( ça ne vous empêche pas d’avoir votre mot à dire lors de la préparation 😀 ). Récemment j’ai co-organisé un hands on « Clean Code » en illustrant quelques notions qui nous paraissaient essentielles. Ces évènements sont évidemment chronophages mais offrent un certains retour sur investissement. Outre la présentation technique des différents sujets, les membres de l’équipe se forment et apprennent. Ils peuvent voir en situation les différentes notions que vous évoquez (en fait je les rabâche) lors des MR ou pendant les revues de code. Aussi, je pense que ça contribue à une certaine émulation technologique. Ça prend du (beaucoup de) temps, mais ça en vaut la peine! L’idéal dans ce genre d’exercice est quand tout le monde propose des sujets. Pas seulement l’architecte ou le lead dev. Les développeurs peuvent prendre le lead dans cet exercice. Ca permet d’une part de les valoriser, de les faire monter en compétence. Quoi de mieux pour approfondir un sujet que de monter un talk et/ou hands on dessus ? ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:3:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#ateliers--workshops"},{"categories":null,"content":" 4 Revues de codeJe ne vais pas aborder dans ce chapitre les revues de code que l’on peut faire dans le cadre des projets, lors des MR par exemple. Pour certaines personnes, surtout les juniors, je fais régulièrement une revue de code alternative. Je passe une 1/2 heure, une heure max sur un bout de code que le dev m’aura sélectionné. Je lis le code avec le développeur et je donne quelques axes d’amélioration: design patterns, tests unitaires, refactoring,… Tout y va. Ça permet de se poser et d’aborder quelques sujets: la programmation fonctionnelle, les IO en java,… ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:4:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#revues-de-code"},{"categories":null,"content":" 5 Pour aller plus loinBien évidemment, beaucoup d’autres actions peuvent être mises en place. La plupart de l’accompagnement que je peux réaliser se fait quotidiennement, dans les projets. Pour aller un peu plus loin, un collègue a mis en place un système de mentorat pour accompagner les jeunes développeurs et accélérer leur montée en compétence. Cette idée est très intéressante et peut être appliquée dans beaucoup de contextes. Si vous avez des idées, questions, remarques, pratiques que vous développez chez vous, n’hésitez pas à les partager! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:5:0","series":null,"tags":null,"title":"Comment coacher des jeunes développeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#pour-aller-plus-loin"},{"categories":null,"content":"Si vous provisionnez vos VM VirtualBox avec Vagrant, vous avez sans doute eu l’idée d’automatiser le provisionning des machines virtuelles. Dans mon cas une VM GNU/Linux basée sur Debian 9. Pour cela, soit vous faite tout manuellement et après les mises à jour deviennent fastidieuses, soit vous appliquez un script shell au démarrage de vagrant, soit vous utilisez Ansible. Ansible est un outil opensource permettant d’automatiser le provisionning et la mise à jour des environnements à distance (via SSH). L’avantage par rapport à des outils tels que Puppet, est qu’il ne nécessite pas l’installation d’agent. Je vais essayer de vous montrer comment mettre en place le provisionning via Ansible pour VirtualBox. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:0:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#"},{"categories":null,"content":" 1 Configuration de VagrantDans le fichier Vagrantfile, on active le provisionning via Ansible: ini config.vm.provision \"ansible_local\" do |ansible| ansible.playbook = \"site.yml\" ansible.install_mode = \"pip\" ansible.version = \"2.7.10\" end Cette configuration fait référence à un fichier « playbook » site.yml. C’est la configuration qui sera appliqué lors du provisionning . Que ça soit à la création ou pour les mises à jour. Voici un exemple de contenu: yaml - name: VirtualBox hosts: all become: yes become_user: \"root\" become_method: \"sudo\" roles: - common: vars_files: - vars/environment.yml Ce fichier est la racine de notre configuration Ansible. On y référence les rôles appliqués et les fichiers d’ environnement. Voici un exemple de rôle: yaml - name: \"Remove useless packages from the cache\" apt: autoclean: yes force_apt_get: yes - name: \"Remove dependencies that are no longer required\" apt: autoremove: yes force_apt_get: yes - name: \"Update and upgrade apt packages (may take a while)\" become: true apt: upgrade: dist update_cache: yes force_apt_get: yes - name: \"Install useful packages\" become: true apt: name: - gcc - g++ - ... - zsh - firewalld state: present update_cache: no - name: ansible create directory example file: path: \"{{ home }}/.m2\" state: directory owner: \"{{ username }}\" group: \"{{ username }}\" - name: Install Maven settings.xml copy: src: settings.xml dest: \"{{ home }}/.m2/settings.xml\" owner: \"{{ username }}\" group: \"{{ username }}\" - name: \"Install Maven\" raw: \"curl -sL \\\"http://mirror.ibcp.fr/pub/apache/maven/maven-3/{{ maven_version }}/binaries/apache-maven-{{ maven_version }}-bin.tar.gz\\\" -o /opt/apache-maven.tar.gz \u0026\u0026 tar -zxf /opt/apache-maven.tar.gz -C /opt\" become: true become_user: root become_method: sudo - name: \"Change Maven Rights\" file: path: /opt/* state: touch modification_time: \"preserve\" access_time: \"preserve\" owner: \"{{ username }}\" group: \"{{ username }}\" Les variables d’environnement permettent de variabiliser certains champs de vos rôles. On peut trouver par exemple les versions de certains outils déployés bash maven_version: 3.5.4 username: vagrant home: /home/vagrant docker_compose_version: 1.22.0 Il y a une quantité impressionnante de modules Ansible que l’on peut utiliser. Que ça soit pour lancer des commandes shell ou lancer des services. Contrairement à la création d’un script shell qui pourrait faire les mêmes actions à la création, on peut facilement gérer la mise à jour de la VM car Ansible détecte les modifications lors de son exécution. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-de-vagrant"},{"categories":null,"content":" 1.1 Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: 1.1.0.1 ansible.cfg ini [defaults] hostfile = hosts 1.1.0.2 hosts ini [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-spécifique-pour-virtualbox"},{"categories":null,"content":" 1.1 Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: 1.1.0.1 ansible.cfg ini [defaults] hostfile = hosts 1.1.0.2 hosts ini [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#ansiblecfg"},{"categories":null,"content":" 1.1 Configuration spécifique pour VirtualBoxPour VirtualBox, j’ai ajouté deux fichiers de configuration supplémentaires à la racine: 1.1.0.1 ansible.cfg ini [defaults] hostfile = hosts 1.1.0.2 hosts ini [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#hosts"},{"categories":null,"content":" 2 Provisionning","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#provisionning"},{"categories":null,"content":" 2.1 A la créationle provisionning peut se faire au lancement de vagrant via la commande: bash vagrant up Pour faire une mise à jour Directement dans la box, vous pouvez lancer les commandes suivantes : bash sudo mount -t vboxsf vagrant /vagrant Puis, vous pouvez lancer les commandes suivantes dans la box: bash su - cd /vagrant export ANSIBLE_CONFIG=/vagrant ansible-playbook site.yml ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#a-la-création"},{"categories":null,"content":"En attendant de prendre mon train, j’essaye de me remettre de cette nouvelle édition. Cette année JAVA est revenu au premier plan. Que ça soit via la spécification microprofile, quarkus , graalvm ou encore par les problématiques de migration JDK 8 -\u003e 11. On a pas mal vu des architectures micro services à base de service mesh (istio) et kubernetes. A coté des sujets techniques, un des sujets majeurs était le bien être et la bienveillance au travail. Les vidéos des conférences seront bientôt retransmises sur le channel Youtube de DevoxxFR. D’une manière générale, le niveau des conférences est toujours très bon. J’ai particulièrement apprécié les confs suivantes. N’hésitez pas à les visionnez une fois qu’elles seront disponibles sur Youtube. Cycle de vie des applications dans k8s Créer facilement des microservices avec Eclipse microprofile Back to Basics, ne perdez plus de temps avec les dates Hexagonal at scale Comment concevoir une API REST D’Architecte à MetaArchitecte: Une évolution nécessaire Il y a aussi certaines conférences ou j’ai eu un bon écho : Oubliez JavaEE, voilà JakartaEE Back to Basics, ne perdez plus de temps avec les dates Comprendre les GC à faible latence Je pense qu’il y a encore bien d’autres conférences qui ont été très intéressantes. J’ ai quelques heures de visionnage à prévoir dans mon agenda 🙂 . Quoi qu’il en soit, merci aux organisateurs pour cette édition. C’était top! Rendez vous l’année prochaine ! ","date":"2019-04-20","objectID":"/2019/04/20/devoxx-2019/:0:0","series":null,"tags":["conférence","devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2019","uri":"/2019/04/20/devoxx-2019/#"},{"categories":null,"content":"Après avoir mis à jour mon mot de passe Spotify ( oui, il faut modifier régulièrement ses mots de passe ) , j’ai eu un petit soucis sur MoodeAudio ( version 4.4) et notamment sur la connexion avec Spotify. Après quelques recherches sur le forum de moodeaudio, j’ai trouvé la correction qui allait bien. Voici comment faire : D’abord on se connecte via SSH sur le raspberry pi bash $ ssh pi@192.168.0.xx Puis on lance la commande: bash $ sudo mv /var/local/www/spotify_cache/credentials.json /home/pi/ $ sudo reboot Normalement, Spotify Connect devrait fonctionner après le redémarrage 🙂 ","date":"2019-03-15","objectID":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/:0:0","series":null,"tags":["moodeaudio","planetlibre","spotify"],"title":"Au secours! Spotify Connect ne fonctionne plus sur MoodeAudio","uri":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/#"},{"categories":null,"content":"Dans la série j’équipe ma maison en Raspberry PI, j’ai décidé de me doter d’une station radio connectée qui me permettrait de « moderniser » un peu ma chaîne HI-FI. Mes besoins sont: Connexion en analogique à une chaîne HI-FI Jouer des MP3/FLAC stockés dans un NAS Jouer des web radios (ex. FIP, TSF JAZZ) Connexion SPOTIFY Une interface web sympa Après quelques recherches, j’ai donc opté pour une solution basée sur un DAC JustBoom, un Raspberry PI et la distribution MoodeAudio. Voici le DAC que l’on branche directement sur le port GPIO du Raspberry PI: L’installation et la configuration du DAC se sont très bien passées. L’installation se fait comme avec des LEGOs. Pour la configuration, j’ai testé dans un premier temps Volumio puis MoodeAudio. Pour l’instant, je reste sur cette dernière. Toutes les fonctionnalités que je souhaite sont en standard. Pas besoin de plugins tiers. Toutes les étapes d’ installation et de configuration pour que le DAC soit reconnu sont décrites ici. Les gens de chez JustBoom ont bien documenté la configuration pour les principales distributions. Le seul reproche que je trouve à MoodeAudio est l’ergonomie. Sur un téléphone, ce n’est pas top. Surtout sur l’accès aux menus d’administration. J’ai du également ajouter des radios manuellement alors que dans Volumio, avec le plugin TuneIn, ça pouvait se faire automatiquement. Je me suis basé sur les informations fournies par ce site. Quoi qu’il en soit, tout ce que je souhaitais fonctionne super bien! Spotify Connect, l’écoute de TSF JAZZ, la lecture des morceaux de ma bibliothèque fonctionnent nickel ! ","date":"2019-03-07","objectID":"/2019/03/07/une-radio-connectee-diy/:0:0","series":null,"tags":["planetlibre","raspberry-pi"],"title":"Une radio connectée DIY","uri":"/2019/03/07/une-radio-connectee-diy/#"},{"categories":null,"content":"J’ai fini l’édition 2019 du Touraine Tech. Tout d’abord, merci aux organisateurs pour l’accueil et l’organisation. C’était vraiment top! Cette année, je n’ai pas pu faire beaucoup de conférences. Mon Hands on m’ayant retenu une bonne partie de l’après midi, que ça soit durant le talk ou après pour décompresser 🙂 Mon hands on portait sur l’architecture, j’ai eu une vingtaine de personnes qui l’on suivi et ont pratiqué sur différents sujets. {% include gallery caption=“Un kata d’architecture à TNT” layout=“half” %} La cave à vin connecté a remporté un franc succès, du moins pendant la présentation des sujets 🙂 Voici le feedback que j’ai eu sur ma présentation: RDV l’année prochaine ! ","date":"2019-02-02","objectID":"/2019/02/02/touraine-tech-2019-2/:0:0","series":null,"tags":["architecture","conférence","handson","TNT19"],"title":"Touraine Tech 2019","uri":"/2019/02/02/touraine-tech-2019-2/#"},{"categories":null,"content":"Voici ma deuxième contribution pour une série d’articles sur l’opensource pour le blog de mon entreprise. Les articles précédents traitaient de l’histoire de l’opensource puis des différentes formes que peut prendre l’open source. Cette fois j’aborde les business models du monde opensource. Bonne lecture 🙂 ","date":"2019-01-23","objectID":"/2019/01/23/deuxieme-crossover-opensource-business-models/:0:0","series":null,"tags":["planetlibre"],"title":"Deuxième crossover : Opensource business models","uri":"/2019/01/23/deuxieme-crossover-opensource-business-models/#"},{"categories":null,"content":"Voila la description de la conférence/ hands que j’animerai au Touraine Tech est en ligne 🙂 Vous trouverez le descriptif sur cette page. ","date":"2019-01-14","objectID":"/2019/01/14/objectif-top-architecte/:0:0","series":null,"tags":["tourainetech"],"title":"Objectif Top Architecte !","uri":"/2019/01/14/objectif-top-architecte/#"},{"categories":null,"content":"Mon sujet de talk « Objectif Top Architecte » a été retenu pour l’édition 2019 de Touraine Tech. Réservez le 1 février 2019 dans votre agenda ! Tout d’abord merci aux organisateurs pour leur confiance. Je suis vraiment honoré d’être sélectionné une deuxième année consécutive. Cette année, j’animerai un hands on sur l’architecture. Je vais tâcher de vulgariser quelques principes qui me paraissent importants et animer un « coding dojo de l’architecture ». Pas besoin d’être architecte ou (vraiment, … mais vraiment pas besoin) d’avoir une certification TOGAF pour y participer 🙂 ","date":"2018-12-25","objectID":"/2018/12/25/touraine-tech-2019/:0:0","series":null,"tags":["architecture","conférence","tourainetech"],"title":"Touraine Tech 2019","uri":"/2018/12/25/touraine-tech-2019/#"},{"categories":null,"content":"Il y a quelques jours, je cherchais comment tracer rapidement et simplement les entrées sorties d’une API REST en appliquant quelques formatages, des filtres, et des insertions en base si besoin. Travaillant sur une stack SpringBoot, vous allez me dire : oui tu peux faire des filtres. Pour être franc, j’ai essayé d’ appliquer des interceptor et filtres mais dans mon contexte, ça ne collait pas. Me voilà donc à la recherche d’une solution faisant le taff et qui soit peu intrusive dans mon contexte. J’ai trouvé par hasard au fil de mes lectures sur Stackoverflow le framework logbook réalisé par … Zalando ( et oui, ils ne font pas que des chaussures) en licence MIT. Ce composant ne fait qu’une seule chose, mais il le fait bien ! Il permet entre autres de s’intégrer dans une stack JAVA ( JAX-RS ou SpringMVC), de filtrer, récupérer les différentes informations des requêtes et réponses et enfin de formatter selon l’envie (ex. JSON). Voici un exemple de mise en œuvre dans un projet SpringBoot: Dans le fichier pom.xml, ajouter cette dépendance: xml \u003cdependency\u003e \u003cgroupId\u003eorg.zalando\u003c/groupId\u003e \u003cartifactId\u003elogbook-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.11.2\u003c/version\u003e \u003c/dependency\u003e Dans une de vos classes Configuration, définir la factory de Logbook java @Bean public Logbook createLogBook() { // too easy : return Logbook.create(); return Logbook.builder() .condition(Conditions.requestTo(\"/helloworld\")) .formatter(new JsonHttpLogFormatter()).build(); } Dans mon cas j’ai fait un filtre en n’incluant que l’ API /helloworld et j’ai formatté en JSON. On peut également modifier le processus d’écriture pour ne pas écrire dans un fichier mais en base par ex. Ensuite, j’ai ajouté la configuration du logger dans le fichier application.properties ini logging.level.org.zalando.logbook:TRACE Et voila ! Dans la console, lors d’un appel ou d’une réponse à mon API, j’ai le message suivant : bash 018-12-01 15:14:18.373 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"remote\",\"type\":\"request\",\"correlation\":\"c6b345013835273f\",\"protocol\":\"HTTP/1.1\",\"remote\":\"127.0.0.1\",\"method\":\"GET\",\"uri\":\"http://127.0.0.1:8080/helloworld\",\"headers\":{\"accept\":[\"/\"],\"host\":[\"127.0.0.1:8080\"],\"user-agent\":[\"curl/7.52.1\"]}} 2018-12-01 15:14:18.418 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"local\",\"type\":\"response\",\"correlation\":\"c6b345013835273f\",\"duration\":48,\"protocol\":\"HTTP/1.1\",\"status\":200,\"headers\":{\"Content-Length\":[\"11\"],\"Content-Type\":[\"text/plain;charset=UTF-8\"],\"Date\":[\"Sat, 01 Dec 2018 14:14:18 GMT\"]},\"body\":\"Hello world\"} Vous remarquerez que les requêtes / réponses peuvent désormais être associés grâce à un identifiant de corrélation. On peut facilement déterminer le temps de traitement d’une requête ou encore faciliter les recherches. Vous trouverez tout le code dans ce repo github. ","date":"2018-12-01","objectID":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/:0:0","series":null,"tags":["logbook","planetlibre","spring","springboot"],"title":"Tracer (facilement) les entrées sorties d'une API REST","uri":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/#"},{"categories":null,"content":"En attendant d’avoir plus d’imagination, voici un rapide tuto pour gérer plusieurs référentiels GIT avec des clés SSH différentes. Imaginons que vous deviez vous connecter sur différents serveurs GIT (ex. github et gitlab) avec des emails différents et donc des clés RSA différentes ( oui je sais ce cas n’arrive pas souvent ). Le tout sous Windows et GNU/LINUX. Sous GNU/LINUX ont peut le gérer différemment via la commande ssh-add. Pour pouvoir gérer ceci de manière simple, j’ai fait la manipulation suivante : Dans le répertoire ~/.ssh, j’ai crée les différentes clés avec la doc fournie par GITHUB. Puis, j’ai crée le fichier ~/.ssh/config avec le contenu suivant: text Host monhost1.fr HostName monhost1.fr User git IdentityFile ~/.ssh/id_rsa Host monhost2.fr HostName monhost2.fr User git IdentityFile ~/.ssh/nouvellecle_rsa Et voilà ! Après avoir fait les différentes configurations coté serveur (c.-a-d. ajout des clés publiques), je peux interagir avec les différents serveurs (pull, push). En espérant que ça puisse servir à d’autres. ","date":"2018-11-16","objectID":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/:0:0","series":null,"tags":["git","planetlibre"],"title":"Gérer plusieurs clés et plusieurs repo GIT","uri":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/#"},{"categories":null,"content":"je n’ai pas écrit beaucoup de choses sur mon blog ces derniers temps. C’était en partie dû au fait que j’étais en train d’écrire un article avec R. SEMETEYS pour le blog de mon entreprise. Cet article est disponible ici. Il essaye de synthétiser l’histoire de l’open source. J’espère que vous ne serez pas rebuté par l’anglais ( c’est un exercice 🙂 ) Bonne lecture 🙂 ","date":"2018-10-29","objectID":"/2018/10/29/premier-cross-over/:0:0","series":null,"tags":["planetlibre"],"title":"Premier cross over \u0026#8230;","uri":"/2018/10/29/premier-cross-over/#"},{"categories":null,"content":"Bon, ça fait quelques temps que je n’ai rien posté… Voici un rapide tuto pour installer docker-ce sur une debian9. Oui, je sais, docker est déjà présent sur les dépôts, mais si vous souhaitez avoir une version un peu plus récente, vous pouvez passer par l’installation de la version ce fournie par docker. ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#"},{"categories":null,"content":" 1 Pré-requisSupprimer les éventuelles installations de docker et docker-compose bash #apt-get remove docker docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:1:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#pré-requis"},{"categories":null,"content":" 2 InstallationLancer les commandes suivantes: bash # apt-get install apt-transport-https ca-certificates # curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add \u0026#8211; # add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\" Puis lancer bash # apt update # apt install docker-ce ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation"},{"categories":null,"content":" 2.1 Installation de docker-composeLancer les commandes suivantes: bash # curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # chmod a+x /usr/local/bin/docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation-de-docker-compose"},{"categories":null,"content":" 3 Configuration des droitsPour lancer docker depuis un utiliser non root, il faut lancer les commandes suivantes: bash # groupadd docker # adduser monutilisateur docker # usermod -aG docker monutilisateur Après ceci, vaut mieux redémarrer le pc … ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:3:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-des-droits"},{"categories":null,"content":" 4 Configuration du démonVoici quelques config à appliquer pour que le démon soit accessible par des outils tels que le plugin maven ou encore configurer l’accès à un proxy ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-démon"},{"categories":null,"content":" 4.1 Configuration du portExécuter la commande: bash # systemctl edit docker.service Entrer le code suivant: ini [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock Et l’enregistrer sous /etc/systemd/system/docker.service.d/docker.conf ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-port"},{"categories":null,"content":" 4.2 Configuration du proxyAvec la même commande bash # systemctl edit docker.service Entrer la configuration suivante: ini [Service] Environment=\"HTTP\\_PROXY=http://mon\\_proxy:mon_port/\" Environment=\"NO_PROXY=127.0.0.1\" ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:2","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-proxy"},{"categories":null,"content":" 4.3 Activation des configurationsLancer les commandes suivantes: bash # systemctl daemon-reload # systemctl restart docker ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:3","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#activation-des-configurations"},{"categories":null,"content":" 5 ValidationMaintenant, vous pouvez valider votre configuration avec la commande: bash $ docker run hello-world ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:5:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#validation"},{"categories":null,"content":"L’édition 2018 de DEVOXX touche bientôt à sa fin. Pour ceux qui ne connaissent pas cette conférence, c’est LA conférence sur le développement en France. A titre personnel, je peux plus apprendre en trois jours à cette conférence qu’en formation. Tout d’abord un grand merci aux organisateurs. Ils assurent réellement. Bon, pour l’année prochaine, n’hésitez à retenir ma conférence 😉 Si vous n’avez pas eu la chance d’assister aux trois jours, il faut savoir que vous pourrez voir les rediff sur la chaine youtube. ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:0:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#"},{"categories":null,"content":" 1 Les tendancesVoici les tendances que j’ai retenu : Spring, spring et encore spring Du réactif en veux tu en voila Du DDD sinon rien Du devops et le plus impressionnant pour moi était la conférence de JOSHUA BLOCH(!!!) sur Effective Java. Pas tant dans le contenu, car il reprenait peu ou prou celui du livre, mais de voir une personne de ce calibre (dans le monde JAVA, c’est une rock star) en France, c’est assez impressionnant. Les keynotes ( dont celle sur le smart building) étaient dans l’ensemble très intéressantes. Les conférences étaient également d’un très bon niveau. J’ai pu découvrir par exemple l’avance que peut avoir l’Estonie sur l’IT ( voir le projet x-road ) par rapport à la France qui lance le projet french road. Voici quelques conférences qui m’ont plu et interpelé : Être architecte en 2018 Le smart building Chaos Engineering Effective java Pourquoi vous avez besoin d’une clean architecture Sécurité des web applications Après java 8, java 9 et 10 Je ne vais pas trop les décrire ( voire pas du tout ), elles seront disponibles prochainement sur la chaine youtube . ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:1:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#les-tendances"},{"categories":null,"content":"Depuis quelques jours, je teste Apache Camel pour la mise en œuvre de médiations. Apache Camel est un framework assez ancien. Il est similaire à Spring Intégration et permet l’ implémentation de patterns d’intégration. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:0:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#"},{"categories":null,"content":" 1 Les patterns d’intégrationQu’est-ce qu’un pattern d’intégration allez-vous me dire ? C’est une solution d’architecture ou plus simplement une recette de cuisine permettant d’avoir une solution toute prête à une problématique d’intégration donnée. L’ensemble de ces patterns est décrit sur ce site ( ne vous attardez pas sur le look des années 90 … ). Exemple : Camel permet simplement de gérer l’intégration via un DSL. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#les-patterns-dintégration"},{"categories":null,"content":" 1.1 Choix d’implémentationsOn peut faire pas mal de choses avec ce FRAMEWORK et de plusieurs manières. J’ai fait les choix d’implémentation suivants : Tout se fera avec SPRING … et pas en XML 🙂 Il faut que toutes les médiations soient testables J’exécute le code dans un FATJAR ( pourquoi avec springboot ) ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:1","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#choix-dimplémentations"},{"categories":null,"content":" 2 Configuration de la routeApache Camel définit les médiations dans des routes. Elles se définissent assez rapidement . Les routes commencent par une instruction from et se terminent par une ou plusieurs instructions to. Pour mon exemple, j’extrais les données d’une table et les stocke dans un fichier. Tout se configure par des URLs. La première permet d’extraire les données via JPA/HIBERNATE. Une entité Address permet le requêtage. La seconde permet le stockage dans un fichier texte JSON. Elles sont externalisées dans des fichiers de configuration pour faciliter les tests et accessibles via SPRING. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:2:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#configuration-de-la-route"},{"categories":null,"content":" 3 Lancement de la routeLe lancement de la route se fait dans une méthode main() : ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:3:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#lancement-de-la-route"},{"categories":null,"content":" 4 TestsCamel fournit une API de test assez bien fournie. Elle permet notamment de mocker des endpoints existants (ex. : le fichier de sortie de mon cas de test). Dans mon cas, j’ai décidé de remplacer la base de données que j’interroge en input par une base HSQLDB chargée en mémoire. Le fichier de sortie est, lui, remplacé dynamiquement par un mock. Pour ce faire, j’ai utilisé les « adviceWith » ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:4:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#tests"},{"categories":null,"content":" 5 Pour aller plus loinIl y a pas mal d’exemples sur le GITHUB de CAMEL. Vous pouvez également acheter le livre « Camel In Action ». Ca ne vaut pas Effective Java 🙂 , mais vu qu’il est écrit par le principal développeur, c’est une très bonne référence. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:5:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Intégration et médiation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#pour-aller-plus-loin"},{"categories":null,"content":"Et oui, il y a un equalizer dans debian….Pulse Audio dispose d’un equalizer. Bon ce n’est encore très user friendly, mais ça fonctionne! ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:0:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#"},{"categories":null,"content":" 1 Installation de l’equalizer bash apt-get install pulseaudio-equalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:1:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#installation-de-lequalizer"},{"categories":null,"content":" 2 ActivationAjouter les lignes suivantes dans le fichier /etc/pulse/default.pa ini load-module module-equalizer-sink load-module module-dbus-protocol Relancer le démon pulseaudio bash \\# pulseaudio -k \u0026\u0026 pulseaudio -D A ce stade, vous devriez avoir dans le panneau de configuration la référence à l’equalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:2:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#activation"},{"categories":null,"content":" 3 LancementEn ligne de commande ( je vous disais que ce n’était pas trop user-friendly), lancer la commande bash $ qpaeq \u0026 On obtient cette interface: Arrivé à ce niveau, je suis quand même un peu déçu/ Il n’y a pas une vrai intégration dans debian ( pas de lanceur pour l’equalizer ) et il n’y a pas de presets configurés ( #souvienstoiwinamp) J’ai essayé de poster mon soucis sur IRC, mais je n’ai pas encore eu de réponse. Je pense soumettre un bug dans les prochains jours. ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:3:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#lancement"},{"categories":null,"content":"Vagrant est un outil permettant de construire des environnements de travail virtualisés hébergés sur vmware, virtualbox ou encore docker. Il permet par exemple de construire et gérer une VM dans un seul et même workflow et d’éviter les exports et partages de machines virtuelles ( tout est déclaré dans un seul et même fichier ). Voici comment je l’ai installé sur ma debian 9. ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:0:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#"},{"categories":null,"content":" 1 InstallationLe paquet fourni dans la distribution n’est pas compatible avec la version de virtualbox fournie dans le repo virtualbox.org. j’ai donc installé la version disponible sur le site de vagrant. bash # dpkg -i vagrant\\_2.0.2\\_x86_64.deb ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:1:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation"},{"categories":null,"content":" 2 Configuration","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#configuration"},{"categories":null,"content":" 2.1 ProxySi vous avez un proxy, il faut effectuer le paramétrage suivant bash $ export http_proxy= »http://user:password@host:port » $ export https_proxy= »http://user:password@host:port » $ vagrant plugin install vagrant-proxyconf $ export VAGRANT\\_HTTP\\_PROXY= »http://user:password@host:port » $ export VAGRANT\\_NO\\_PROXY= »127.0.0.1\u0026Prime; bash $vagrant box add \\ precise64 https://files.hashicorp.com/precise64.box `$ export VAGRANT_DEFAULT_PROVIDER`=virtualbox [/code] ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:1","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#proxy"},{"categories":null,"content":" 3 Installation d’une VMVoici un exemple pour une VM virtualbox basée sur ubuntu bash $ mkdir ~/vagrant $ cd ~/vagrant $ vagrant init pristine ubuntu-budgie-17-x64 $ vagrant up [/code] Avec ces quelques commandes j’obtiens un environnement ubuntu hébergé sur virtualbox sans avoir à installer et configurer la vm. Pour l’instant je ne rentre pas trop dans les détails de la construction des images. Peut-être que je m’y plongerai prochainement ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:3:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation-dune-vm"},{"categories":null,"content":"J’ai eu la chance d’être sélectionné pour la première édition de la conférence TouraineTech. Tout d’abord, je tiens à remercier toute l’équipe du Touraine Tech pour l’accueil et l’organisation de cette conférence. Ma présentation s’intitulait: Jenkins2 le retour (d’expérience). Je faisais un retour d’expérience sur la mise en œuvre de Jenkins 2 et des pipelines. Elle était au format quickie (15mn). J’ai pas mal préparé la présentation car c’était ma première dans ce domaine. Je trouve que ça s’est pas trop mal passé. J’ai fait quelques erreurs dans mes slides ou tout du moins je trouve que je n’ai pas eu l’effet escompté. Quoi qu’il en soit, je suis plutôt content du résultat. Les retours ont été assez satisfaisants. Voici le retour des participants : Pour ceux qui regrettaient de ne pas avoir de démos, j’en suis désolé, mais le format de 15mn ne s’y prêtait pas trop . Si j’avais eu plus de temps, j’aurai fait des démonstrations qui auraient beaucoup mieux illustré mon propos. Pour conclure, je pense rééditer cette expérience. Ça m’a vraiment plu. Je n’ai plus qu’à trouver un sujet pour l’édition 2019 de Touraine Tech 🙂 ","date":"2018-02-26","objectID":"/2018/02/26/ma-presentation-au-touraine-tech/:0:0","series":null,"tags":["planetlibre","tourainetech"],"title":"Ma présentation au Touraine Tech","uri":"/2018/02/26/ma-presentation-au-touraine-tech/#"},{"categories":null,"content":"Dans la série, j’essaye de sauvegarder toutes mes configurations, voici ce que j’ai fait pour configurer correctement cygwin. Pour ceux qui ne connaissent pas ou qui n’ont pas la chance d’utiliser windows au travail, cygwin est un shell avec tous les outils GNU. En attendant d’avoir windows 10 ( au travail ) et un BASH intégré, il n’y a pas mieux. Du moins à mon humble avis. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:0:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#"},{"categories":null,"content":" 1 GIT","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#git"},{"categories":null,"content":" 1.1 ComplétionOn a besoin des fichiers suivants git-completion.bash git-prompt.sh Je les ai téléchargé et placé dans le répertoire $HOME. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:1","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#complétion"},{"categories":null,"content":" 1.2 Activation de la configuration et affichage de la branche en cours dans le promptJ’ai activé la configuration git en exécutant les scripts précédemment téléchargés. Voici la personnalisation que j’ai paramétré dans la variable d’environnement PS1: J’ ai également activé des propriétés qui étaient en commentaire dans ce fichier. Je ne les ai pas listée pour ne pas trop surcharger l’article 🙂 ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:2","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#activation-de-la-configuration-et-affichage-de-la-branche-en-cours-dans-le-prompt"},{"categories":null,"content":" 1.3 Configuration Nom et sécurité ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:3","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#configuration-nom-et-sécurité"},{"categories":null,"content":" 2 VIMQue serait un prompt sans vim ? J’ai installé une suite de plugin : The ultimate vimrc. Il faut cloner le repo GIT et lancer un script bash git clone --depth=1 https://github.com/amix/vimrc.git ~/.vim_runtime sh ~/.vim_runtime/install_awesome_vimrc.sh ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:2:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#vim"},{"categories":null,"content":"Désolé de remettre ça. Je remets sur mon blog ma configuration Debian. Histoire de ne pas la perdre tant qu’elle est dans mon historique . Voici ce que j’ai réalisé post-installation: ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:0:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#"},{"categories":null,"content":" 1 Ajout dépôts supplémentairesDans le fichier /etc/apt/sources.list, ajouter les repo contrib et non-free . Activer également les mises à jour de sécurité. ini deb http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb http://security.debian.org/debian-security stretch/updates main non-free contrib deb-src http://security.debian.org/debian-security stretch/updates main non-free contrib # stretch-updates, previously known as 'volatile' deb http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:1:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#ajout-dépôts-supplémentaires"},{"categories":null,"content":" 2 Logiciels tiers","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#logiciels-tiers"},{"categories":null,"content":" 2.1 Etcher#echo \"deb https://dl.bintray.com/resin-io/debian stable etcher\" | sudo tee /etc/apt/sources.list.d/etcher.list bash \u003cpre\u003e#apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 379CE192D401AB61 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#etcher"},{"categories":null,"content":" 2.2 Virtualbox bash # wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add - Dans le fichier /etc/apt/sources.list.d/virtualbox.list ini deb https://download.virtualbox.org/virtualbox/debian stretch contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#virtualbox"},{"categories":null,"content":" 2.3 SpotifyDans le fichier /etc/apt/sources.list.d/spotify.list ini deb http://repository.spotify.com stable non-free ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:3","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#spotify"},{"categories":null,"content":" 3 Installation paquets supplémentaires bash # apt-get update # apt-get install firmware-iwlwifi virtualbox-5.2\\ ttf-mscorefonts-installer easytag tuxguitar-jsa htop\\ frescobaldi gparted grsync ntfs-config chromium autofs\\ openjdk-8-jdk openjdk-8-jre gnome-tweak-tool ntfs-config \\ ntfs-3g cifs-utils geogebra-gnome arduino libmediainfo \\ libmediainfo0v5 network-manager-openvpn-gnome dirmngr \\ spotify-client spotify-client-gnome-support \\ etcher apt-transport-https etcher-electron vim \\ fonts-powerline audacity ffmpeg lame unrar rar gdebi \\ sound-juicer traceroute scala net-tools nmap \\ gnome-shell-pomodoro hplip dig dnsutils build-essential \\ linux-headers-amd64 firmware-linux-nonfree lshw ethtool \\ libsane-hpaio xsane autofs vlc ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:3:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-paquets-supplémentaires"},{"categories":null,"content":" 4 Configuration autofsPour ceux qui ne connaissent pas , autofs est un outil permettant de monter directement des partages nfs et cicfs à l’utilisation et non au démarrage de l’ordinateur. Dans le fichier /etc/auto.master ini /mnt/SERV1/nfs /etc/auto.nfs --ghost, --timeout=60 /mnt/SERV1/cifs /etc/auto.SERV1.cifs --ghost, --timeout=60 /mnt/SERV2 /etc/auto.cifs --ghost, --timeout=60 ensuite insérer la configuration adéquate dans les fichiers référencés : ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#configuration-autofs"},{"categories":null,"content":" 4.1 auto.cicfs ini data -fstype=cifs,credentials=/home/USER/.cred-file,user=littlewing,uid=1000,gid=1000 ://192.168.0.XX/REPERTOIRE Les identifiants / mots de passe sont stockés dans un fichier .cred-file stocké à la racine du répertoire utilisateur. Voici un exemple : ini username=user password=password Le fichier auto.SERV1.cifs reprend la même structure ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autocicfs"},{"categories":null,"content":" 4.2 auto.nfs ini REP1 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP1 REP2 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP2 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autonfs"},{"categories":null,"content":" 5 Installation d’atomJ’ai choisi d’installer atom via le package .deb fourni par github. Afin d’automatiser l’installation et la mise à jour, voici le script que j’ai réalisé : bash #!/bin/sh SETUP_ROOT=/tmp wget -O $SETUP_ROOT/atom.deb \"https://atom.io/download/deb\" echo \"Installation du paquet...\" dpkg -i $SETUP_ROOT/atom.deb echo \"Fini :)\" Ce script est placé dans le répertoire /usr/local/sbin et lancé comme suit : bash # upgrade-atom.sh ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:5:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-datom"},{"categories":null,"content":" 6 Installation de FirefoxAfin d’avoir la dernière version de firefox, voici le script que j’ai réalisé: bash #!/bin/sh SETUP_ROOT=/tmp BIN_ROOT=/usr/local/firefox DATE=`date +%Y-%m-%d` OLD_EXE=/usr/lib/firefox-esr/firefox-esr wget -O $SETUP_ROOT/FirefoxSetup.tar.bz2 \"https://download.mozilla.org/?product=firefox-latest\u0026os=linux64\u0026lang=fr\" echo \"Extraction de l'archive...\" tar xjf $SETUP_ROOT/FirefoxSetup.tar.bz2 -C /usr/local echo \"Changement des droits utilisateur\" chown -R :users $BIN_ROOT chmod a+x $BIN_ROOT/firefox echo \"Sauvegarde de l'ancien binaire et Creation des liens symboliques\" if [ -e $OLD_EXE ] then OLD_BINARY=${OLD_EXE}_orig_${DATE} mv $OLD_EXE $OLD_BINARY fi ln -s $BIN_ROOT/firefox $OLD_EXE chmod a+x $OLD_EXE echo \"Fini :)\" ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:6:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-de-firefox"},{"categories":null,"content":" 7 MinecraftVoila l’étape la plus importante, du moins pour mes enfants … J’ai crée le script /usr/local/bin/minecraft.sh bash #!/bin/bash cd /usr/local/minecraft java -Xmx1G -Xms512M -cp /usr/local/minecraft/Minecraft.jar net.minecraft.bootstrap.Bootstrap J’ai placé le JAR en question dans le répertoire /usr/local/minecraft. Enfin, j’ai crée le fichier « lanceur gnome » /usr/share/applications/minecraft.desktop ini [Desktop Entry] Name=Minecraft Comment= Categories=Game;BoardGame; Exec=/usr/local/bin/minecraft.sh Icon=Minecraft_Block Terminal=false Type=Application StartupNotify=true J’ai également mis une icone SVG dans le répertoire /usr/share/icons/ ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:7:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#minecraft"},{"categories":null,"content":" 8 Optimisation du bootAprès toutes ces installations, il faut vérifier que les performances, notamment au démarrage ne sont pas trop altérées Pour avoir le détail du boot, il faut utiliser la commande systemd-analyze bash #systemd-analyze blame 8.113s NetworkManager-wait-online.service 2.549s apt-daily-upgrade.service 803ms networking.service 228ms colord.service 213ms dev-sda1.device 145ms systemd-timesyncd.service 128ms ModemManager.service 102ms autofs.service .... On peut également voir le chemin critique avec cette commande: bash #systemd-analyze critical-chain The time after the unit is active or started is printed after the \"@\" character. The time the unit takes to start is printed after the \"+\" character. graphical.target @8.944s └─multi-user.target @8.944s └─autofs.service @8.841s +102ms └─network-online.target @8.841s └─NetworkManager-wait-online.service @723ms +8.113s └─NetworkManager.service @642ms +80ms └─dbus.service @612ms └─basic.target @612ms └─paths.target @612ms └─acpid.path @610ms └─sysinit.target @608ms └─systemd-backlight@backlight:acpi_video0.service @1.042s +8ms └─system-systemd\\x2dbacklight.slice @1.042s └─system.slice @119ms └─-.slice @108ms ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#optimisation-du-boot"},{"categories":null,"content":" 8.1 Désactivation des servicesPar exemple, si vous voulez désactiver le service virtualbox au démarrage bash # systemctl disable vboxautostart-service.service et ainsi de suite pour tous les services inutiles au démarrage ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#désactivation-des-services"},{"categories":null,"content":" 9 Analyse du démarrage d’un servicePour analyser le démarrage d’un service, on peut utiliser la commande journalctl bash # journalctl -b -u NetworkManager-wait-online.service ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:9:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#analyse-du-démarrage-dun-service"},{"categories":null,"content":" 10 ConclusionAprès toutes ces étapes, j’ai un système opérationnel. Il manque pas mal d’outils ( ex. maven, npm, intellij,…). Ces outils tiennent plus du poste de développement. ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:10:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#conclusion"},{"categories":null,"content":"Bon, mon site part en carafe. J’ai donc décidé de changer d’hébergeur.Pour faire bref, je quitte l’autre.net et je migre mon blog sur wordpress.com. Bien évidemment, je n’ai aucune sauvegarde…Je repars de zéro A bientôt! ","date":"2018-02-09","objectID":"/2018/02/09/migration-de-mon-blog/:0:0","series":null,"tags":null,"title":"Migration de mon blog","uri":"/2018/02/09/migration-de-mon-blog/#"},{"categories":null,"content":"I expose on this blog my tests, technical musings and explorations. Among other things, it helps me remind me what I did months/years ago and sharing it to the community. By default, the content is published under the Creative Commons CC BY license. Obviously, I accept remarks, fixes (nobody is perfect). However, I reserve the right to publish or not a comment. Usually, I publish it except if I consider it as a troll. Stop You probably understood it’s a personal blog and not a professional one. I publish also articles on the Worldline Technical Blog. Finally, views and opinions exposed on this blog are my own and not of my employer. ","date":"2018-02-08","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#"},{"categories":null,"content":" 1 AI usage What about Generative AI? The content on this blog was written by me, not by an AI! Generally speaking, I would rather not write an article than fully generate it entirely from a LLM. I only use LLM : to fix my english articles and make them sounding like more “native to get sometimes inspiration or use them as a search engine on steroids By the way, this precision is inspired from the article of D. GERMAIN and C. WILLIAMS. And now, enjoy! 🙂 ","date":"2018-02-08","objectID":"/about/:1:0","series":null,"tags":null,"title":"About","uri":"/about/#ai-usage"},{"categories":null,"content":" Photo by Jac Alexandru You can reach / follow me through: LinkedIn GitHub BlueSky Mastodon Youtube ","date":"2018-02-08","objectID":"/contact/:0:0","series":null,"tags":null,"title":"Contact","uri":"/contact/#"}]