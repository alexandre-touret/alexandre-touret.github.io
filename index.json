[{"categories":null,"content":" 1 The sad realityPicture this: it‚Äôs Friday afternoon, and you‚Äôre eagerly looking forward to unwinding for the weekend. Suddenly, an Ops engineer alerts you about a critical issue‚Äîa stubborn HTTP 500 error that‚Äôs causing a major roadblock. Despite the dedicated efforts of the Ops engineers, the root cause remains elusive due to a lack of contextual information. Hours pass by, but you take it upon yourself to delve into the problem. Eventually, after reproducing and debugging the issue on your computer, you uncover the issue. Does this sound like science fiction? If you‚Äôve experienced a similar scenario, you‚Äôre likely familiar with the challenges posed by unidentified end users and their unique usage patterns‚Äîenter Ops and observability! I‚Äôve previously delved into the topic of observability. Here are a bunch of articles I wrote on this blog or on the Worldline Tech Blog: Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry Observabilit√© et Circuit Breaker avec Spring Enabling distributed tracing on your microservices Spring app using Jaeger and OpenTracing In this article, I aim to highlight the importance of putting in place observability during the earliest stages of a project. I will then outline how to merge logs and traces from a good old Spring Boot application on the Grafana Stack to gain clearer insights into your platform‚Äôs workings. By doing so, you can transform your relationship with Ops teams, making them your best friends. What about the code? The examples provided in this article come from this project hosted on Github. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:1:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#the-sad-reality"},{"categories":null,"content":" 2 A definition of ObservabilityWe can shortly define it as this: Observability is the ability to understand the internal state of a complex system. When a system is observable, a user can identify the root cause of a performance problem by examining the data it produces, without additional testing or coding. This is one of the ways in which quality of service issues can be addressed. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:2:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#a-definition-of-observability"},{"categories":null,"content":" 3 A short presentation of the Grafana stackThe Grafana stack aims at a tool which allows you to query, visualise, alert and explore all of your metrics. You can aggregate them through a wide range of data sources. With regard to the topic of this article,it will provide us all you need to collect logs, metrics and traces (and beyond) to monitor and understand the behaviour of your platforms. I will therefore particularly focus on: Grafana: The dashboard engine Loki: The log storage engine Tempo: The trace storage engine By the way, I also configured in this project a Prometheus TSDB to store metrics. To get it started easily, I just created a Docker Compose stack to run it on your desktop. You can run it with these commands: cd docker docker compose up ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:3:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#a-short-presentation-of-the-grafana-stack"},{"categories":null,"content":" 4 Logs, Traces \u0026 MonitoringLet‚Äôs go back to the basics: To make a system fully observable, the following abilities must be implemented: Logs Traces Metrics They can be defined as follows: ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:4:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#logs-traces--monitoring"},{"categories":null,"content":" 5 LogsWhen a program fails, OPS usually tries to identify the underlying error analyzing log files. It could be either reading the application log files or using a log aggregator such as Elastic Kibana or Splunk. In my opinion, most of the time developers don‚Äôt really care about this matter. It is mainly due to they did not experience such a trouble. For two years, I had to administrate a proprietary customer relationship management solution. The only way to analyse errors was navigating through the logs, using the most appropriate error levels to get the root cause. We didn‚Äôt have access to the source code (Long live to open source programs). Hopefully the log management system was really efficient. It helped us get into this product and administrate it efficiently. Furthermore, I strongly think we should systematise such experiences for developers. It could help them (us) know what is behind the curtain and make more observable and better programs. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:5:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#logs"},{"categories":null,"content":" 5.1 Key principlesYou must first dissociate the logs you make while you code (e.g., for debugging) from the production logs. The first should normally remove the first. For the latter, you should apply some of these principles: Identify and use the most appropriate level (DEBUG, INFO, WARN, ERROR,‚Ä¶) Provide a clear and useful message for OPS (yes you make this log for him/her) Provide business context (e.g., the creation of the contract 123456 failed) Logs must be read by an external tool (e.g., using a log aggregator) Logs must not expose sensitive data: You must think about GDPR, PCI DSS standards If you want to dig into log levels and the importance to indicate contextual information into your logs, I suggest you reading this article from my colleague Nicolas Carlier. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:5:1","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#key-principles"},{"categories":null,"content":" 6 What about Grafana LokiFor this test, I chose to use loki-logback-appender to send the logs to Loki. About this appender I chose to use this appender for testing purpose. If you deploy your application on top of Kubernetes, you would probably opt for a more suitable solution such as FluentD. The configuration for a Spring Boot application is pretty straightforward: You must add first the appender to your classpath: implementation 'com.github.loki4j:loki-logback-appender:1.4.2' and create a logback-spring.xml to configure it: \u003cappender name=\"LOKI\" class=\"com.github.loki4j.logback.Loki4jAppender\"\u003e \u003chttp\u003e \u003curl\u003ehttp://localhost:3100/loki/api/v1/push\u003c/url\u003e \u003c/http\u003e \u003cformat\u003e \u003clabel\u003e \u003cpattern\u003eapp=${name},host=${HOSTNAME},level=%level\u003c/pattern\u003e \u003creadMarkers\u003etrue\u003c/readMarkers\u003e \u003c/label\u003e \u003cmessage\u003e \u003cpattern\u003e {\"level\":\"%level\",\"class\":\"%logger{36}\",\"thread\":\"%thread\",\"message\": \"%message\",\"requestId\": \"%X{X-Request-ID}\"} \u003c/pattern\u003e \u003c/message\u003e \u003c/format\u003e \u003c/appender\u003e Et voil√†! About the format It is just my 2 cents: more and more I tend to produce structured logs using JSON for instance. It is usually easier to manipulate them all along the log ingestion tools chain (e.g, with LogStash. After restarting your application: gradle bootRun After running some API calls with the following command: http :8080/api/events You can now get logs browsing Grafana ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:6:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#what-about-grafana-loki"},{"categories":null,"content":" 7 TracesUpon initial inspection, one might consider the existing setup sufficient. However, I highly recommend delving into the realm of Distributed Tracing, a technology I have previously introduced (refer to the aforementioned discussion). Not only it will be first really useful when you deploy distributed architectures but also for the other kind of platforms. The true value of distributed tracing becomes evident not only in the deployment of distributed architectures but across various platforms. In the complex landscape of production issues, identifying the root cause or understanding why a specific SQL query failed or took an extended duration can be challenging. Traditionally, attempts to replicate such issues in alternative environments often fall short due to the inherent complexities of data, server configurations, and benchmarking. This technology empowers you to gain valuable insights that were previously elusive. When grappling with production issues, you no longer need to rely solely on replication efforts; distributed tracing provides a clear and comprehensive perspective on what might be amiss. To sum up: Try it, you‚Äôll like it! ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#traces"},{"categories":null,"content":" 7.1 The setupThere is several ways to set it up. Nowadays, OpenTelemetry is the de facto standard. Most of the solutions are compatible with it. Nevertheless, after challenging some APMs, I found some missing features which are really useful in real life projects. For instance, you can not easily ignore URLs, for instance the actuator endpoints, from the traces you will manage. You can do that in just one property with the Elastic APM agent. There is an issue about this feature. I suggest using the agents. It is less intrusive than other solutions. For instance if you use the spring boot gradle plugin you can configure it as following: plugins { id 'java' id 'org.springframework.boot' version '3.2.1' id 'io.spring.dependency-management' version '1.1.4' } ext { opentelemetryAgentVersion = '1.32.0' // Mettez la version appropri√©e } group = 'info.touret.observability' version = '0.0.1-SNAPSHOT' java { sourceCompatibility = '21' } repositories { mavenCentral() } dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'io.micrometer:micrometer-registry-prometheus' testImplementation 'org.springframework.boot:spring-boot-starter-test' implementation 'com.github.loki4j:loki-logback-appender:1.4.2' implementation \"io.opentelemetry.javaagent:opentelemetry-javaagent:${opentelemetryAgentVersion}\" } task copyJavaAgent(type: Copy) { from configurations.detachedConfiguration( dependencies.create(\"io.opentelemetry.javaagent:opentelemetry-javaagent:${opentelemetryAgentVersion}\") ) into \"${project.getLayout().getBuildDirectory()}/javaagents\" rename { 'javaagent.jar' } } processResources.dependsOn copyJavaAgent bootRun { doFirst { jvmArgs = [\"-javaagent:${project.getLayout().getBuildDirectory()}/javaagents/javaagent.jar\"] } // // systemProperties = [ // 'otel.traces.sampler': 'parentbased_traceidratio', // 'otel.traces.sampler.arg': '0.2' // ] } tasks.named('test') { useJUnitPlatform() } After restarting your application, you can reach the API with this command: http :8080/api/events This API is really simple. To illustrate how to handle errors using both the Spring stack and the Grafana stack, an error is always thrown using the Problem Detail RFC 7807 while reaching it. Here the service: @Service public class ObservabilityService { public void breakMethod() { throw new IllegalStateException(\"Breaking method issue\"); } } And the controller which returns the error: @GetMapping(\"/api/event\") public ResponseEntity\u003cObservabilityEventDto\u003e getEvent() throws ErrorResponseException { try { observabilityService.breakMethod(); var observabilityEventDto = new ObservabilityEventDto(UUID.randomUUID().toString(), \"OK\"); return ResponseEntity.ok(observabilityEventDto); } catch (Exception e) { var observabilityEventDto = new ObservabilityEventDto(UUID.randomUUID().toString(), \"Error\"); LOGGER.error(e.getMessage()); throw new ErrorResponseException(HttpStatus.INTERNAL_SERVER_ERROR, ProblemDetail.forStatus(HttpStatus.INTERNAL_SERVER_ERROR), e); } } Using Problem Detail responses, you will get such a response when an error occurs: http :8080/api/events HTTP/1.1 500 Connection: close Content-Type: application/problem+json Date: Wed, 17 Jan 2024 08:09:20 GMT Transfer-Encoding: chunked { \"instance\": \"/api/events\", \"status\": 500, \"title\": \"Internal Server Error\", \"type\": \"about:blank\" } After testing this service a few times, you can now see the traces on your Grafana dashboard. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:1","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#the-setup"},{"categories":null,"content":" 7.2 Head or Tail sampling?One significant drawback of implementing this technology lies in the potential performance overhead it introduces to the instrumented application. In cases where high-pressure APIs generate or broadcast SPANs for every transaction, there‚Äôs a substantial risk of significantly impacting the Service Level Objectives (SLOs) of your platform. A viable solution to mitigate this challenge involves sampling the traces, such as retaining only 20% of the transactions. There are two primary approaches: Head Sampling: In this method, SPANs are sampled and filtered directly from the producer (e.g., a backend). This is essential for heavily utilized platforms and proves to be the most efficient, as it produces only the necessary spans, thereby avoiding the dissemination of unnecessary SPANs. However, it comes with the trade-off of potentially losing critical traces involving failures. The sampling rate is purely statistical (e.g., 10 or 20% of SPANs sampled and broadcast). Tail Sampling: Alternatively, SPANs are sampled retrospectively, often through tools like the Open Telemetry Collector. While this method allows for filtering SPANs based on various criteria, such as the transaction status, it does not address the overhead issue. All SPANs are initially broadcast and then filtered, making it less suitable for heavily used scenarios. Both approaches have their pros and cons, and the choice depends on the specific requirements of the platform. For an in-depth exploration of this issue, you can refer to this article. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:7:2","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#head-or-tail-sampling"},{"categories":null,"content":" 8 Correlating Logs \u0026 TracesNow, you have on one side the logs of your applications, and on the other the traces. To dig into errors and see what is behind the curtain of any error logged, it is really import to correlate both. For that, you must specify in your logs the traceID and spanID of the corresponding trace. Hopefully, logback and the Loki appender can help you on this! We therefore will modify the pattern of the logs in the logback-spring.xml file: \u003cpattern\u003e {\"level\":\"%level\",\"TraceID\":\"%mdc{trace_id:-none}\",\"spanId\":\"%mdc{span_id:-none}\",\"class\":\"%logger{36}\",\"thread\":\"%thread\",\"message\": \"%message\",\"requestId\": \"%X{X-Request-ID}\"} \u003c/pattern\u003e As a developer point of view, the job is done :) Now, it is time for the OPS/SRE to configure Grafana to link Loki and Tempo through the TraceID field. For that, you can create a derived field directly in the datasource configuration: datasources: - name: Loki type: loki access: proxy uid: loki url: http://loki:3100 jsonData: maxLines: 1000 derivedFields: - datasourceUid: tempo matcherRegex: '\\\"TraceID\\\": \\\"(\\w+).*\\\"' name: TraceID # url will be interpreted as query for the datasource url: '$${__value.raw}' # optional for URL Label to set a custom display label for the link. urlDisplayLabel: 'View Trace' - name: Tempo type: tempo access: proxy uid: tempo url: http://tempo:3200 jsonData: nodeGraph: enabled: true serviceMap: datasourceUid: 'mimir' tracesToLogs: datasourceUid: loki filterByTraceID: true filterBySpanID: false mapTagNamesEnabled: false Now you will be able to browse directly to the corresponding trace from your log event and the other way around. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:8:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#correlating-logs--traces"},{"categories":null,"content":" 9 MetricsNow, let us deep dive into the metrics of our application! We can do that through Prometheus. We can configure now Prometheus to grab the metrics exposed by our application. To do that, we need first to activate the Prometheus endpoint: We need to add this dependency first: implementation 'io.micrometer:micrometer-registry-prometheus' And enable the corresponding endpoint: management.endpoints.web.exposure.include=health,info,prometheus After enabling it, as a developer point of view, it is done :-) The prometheus statistics can be scrapped by Prometheus itself using this configuration scrape_configs: - job_name: prometheus honor_timestamps: true scrape_interval: 15s scrape_timeout: 10s metrics_path: /actuator/prometheus scheme: http static_configs: - targets: - host.docker.internal:8080 Finally, you can directly browse it through Grafana to integrate all of these metrics into your dashboards üéâ. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:9:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#metrics"},{"categories":null,"content":" 10 ConclusionI endeavored to provide you with a comprehensive overview of what an OPS professional could anticipate while investigating an issue and the corresponding topics that require attention. As you probably figured out, we only applied just a bunch of configuration sets. One of the key merits of these tools lies in their non-intrusiveness within the code itself. To cut long story short: it is not a big deal! Integrating these configurations can be a significant stride forward, providing invaluable assistance to the entire IT team, from development to operations, as they navigate and troubleshoot issues‚Äîwhether in production or elsewhere. I will finish this article by my opinion on such topics: regardless of the targeted tools, this set of configuration must be considered as the first feature to implement for every cloud native application. ","date":"2024-01-16","objectID":"/2024/01/16/observability-from-zero-to-hero/:10:0","series":null,"tags":["Observability","Spring","Java","Grafana"],"title":"Mastering Observability: Empowering Developers from Zero to Hero with Spring \u0026 the Grafana stack","uri":"/2024/01/16/observability-from-zero-to-hero/#conclusion"},{"categories":null,"content":" 1 2023 in a NutshellAs we approach 2024, it‚Äôs time to cast a professional eye back on 2023. Throughout the year, I‚Äôve balanced work on a customer project alongside my contributions to the Worldline TechRel1 initiative. Each involvement has fueled the other, offering a reciprocal flow of inspiration. Involvement in real-life projects has often sparked new ideas for talks and topics to delve into. In turn, my experiences in Dev Rel have offered fresh perspectives and external feedback, enriching the implementation process. Highlights of my speaking engagements this year include: 6 tech conferences 4 meetups A Worldline tech event in Barcelona An online presentation (i.e., BBL) for ABBEAL This year, unlike the previous one, I found myself presenting two talks or a talk and a workshop at the same conference‚Äîa challenging but exciting experience! Consequently, I delivered a similar number of talks compared to 2022. My primary topics this year included: Rest API Versioning The Hitchhiker‚Äôs Guide to Software Architecture Design The Architecture Katas Notably, I co-presented the second talk with my colleague Rapha√´l Semeteys, extending my collaborative speaking initiatives that commenced in 2022 with Jean-Fran√ßois James (I shared the stage with him at LyonJUG). This collaborative process has been immensely rewarding. It helps me push me beyond my comfort zone, enabling me to approach various topics from a fresh perspective. A prime example was our exploration of discussing software architecture in a more light-hearted and unconventional manner, which initially seemed improbable to me. In fact, I intend to explore more partnerships in the future. Moreover, I participated in the recent JChateau Edition, my initiation into the world of ‚Äúunconferences‚Äù (JunConf)‚Äîan incredibly enriching experience! Interacting with inspiring individuals like Jean-Michel Doudoux, Andres Almirez, and Jos√© Paumard was truly inspiring. During the year, I released six articles on my blog and one on the Worldline engineering blog. I express my gratitude to my employer for the continued opportunities and to the organizers for their unwavering trust and hospitality. Your support has been instrumental in my journey. The Worldline TechRel initiative has been instrumental in exploring new topics and facilitating collaborations with colleagues. It has enabled me to share, gather feedback on submissions, and conduct live rehearsals (Thanks Marie-Alice Blete \u0026 Philippe Vincent and the others). Heartfelt thanks to all who have supported me along this journey! ","date":"2023-12-20","objectID":"/2023/12/20/2023-wrap-up/:1:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2023","uri":"/2023/12/20/2023-wrap-up/#2023-in-a-nutshell"},{"categories":null,"content":" 2 What Lies Ahead?Through the 2Worldline TechRel initiative, I‚Äôve had the privilege of meeting exceptional individuals and advancing in various technical domains. Crafting and presenting talks on technical subjects demand in-depth knowledge, and this has already sparked ideas for the upcoming year. Kicking off the new year with a bang, I am thrilled to announce a workshop at NDC London in late January (details here). Additionally, I will be co-presenting a new talk with my colleague Philippe Duval at Touraine Tech. Wishing everyone a fantastic New Year‚Äôs Eve, and if you come across this article in 2024: Happy New Year!! It‚Äôs akin to Dev Rel but aims to include a broader tech community encompassing OPS, SRE, etc.¬†‚Ü©Ô∏é Similar to Dev Rel, but aspires to involve all tech crew members (e.g., OPS \u0026 SRE).¬†‚Ü©Ô∏é ","date":"2023-12-20","objectID":"/2023/12/20/2023-wrap-up/:2:0","series":null,"tags":["wrap up"],"title":"Reflecting on 2023","uri":"/2023/12/20/2023-wrap-up/#what-lies-ahead"},{"categories":null,"content":"Just out of curiosity, I downloaded and sat up Rancher Desktop on my laptop. I daily use Docker and Docker compose on top of WSL2 using home made mechanism/tooling I would then see if Rancher Desktop fits well in this case and could help me. In this (very short) article, we‚Äôll go over the necessary steps to configure WSL2 Ubuntu virtual machines and Docker with Rancher Desktop. If you want to get into Rancher Desktop in another way and discover how to install Skaffold, you can read this article. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:0:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#"},{"categories":null,"content":" 1 Install \u0026 configure Rancher Desktop The setup is quite straightforward. Follow the instructions provided in the official documentation to get Rancher Desktop up and running on your Windows machine. Rancher Desktop allows you to run Docker and Docker Compose seamlessly within a WSL2 environment. During the setup process, I chose to install Moby to use Docker and Docker Compose. After installing Rancher Desktop, you will need to ensure your virtual machine (VM) is connected to expose the Docker daemon and related commands. You can find detailed steps in the Rancher Desktop documentation under WSL Preferences. Don‚Äôt forget that in some cases, you may need to restart both WSL2 and Rancher Desktop for the changes to take effect. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:1:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#install--configure-rancher-desktop"},{"categories":null,"content":" 2 Configure Docker Credential StoreWhen you start your Docker compose infrastructure and encounter an error like this: Error saving credentials: error storing credentials - err: exit status 1, You‚Äôll need to configure Docker‚Äôs credential store. To resolve this issue, follow these steps: Inside your WSL2 VM, create or edit the ~/.docker/config.json file. Add the following content to the config.json file: { \"credsStore\": \"wincred.exe\" } This configuration points to the docker-credential-wincred.exe binary and will resolve the credential storage problem when using Docker. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:2:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#configure-docker-credential-store"},{"categories":null,"content":" 3 Get container‚Äôs output in the consoleA common issue with Docker containers in Rancher Desktop is the lack of output in the console when running a container, such as with the command docker run hello-world. This issue is well-documented in this GitHub issue. To view the container‚Äôs output in the console, you need to start your commands with the -i option. For example: For instance: docker run -i hello-world This option tells Docker to attach to the container‚Äôs standard input, allowing you to see the output directly in your console. ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:3:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#get-containers-output-in-the-console"},{"categories":null,"content":" 4 ConclusionI hope this article has been helpful for you, and you‚Äôre now ready to supercharge your development workflow with Rancher Desktop and Docker! ","date":"2023-11-09","objectID":"/2023/11/09/rancher_desktop_wsl2/:4:0","series":null,"tags":["WSL2","Rancher_Desktop","Docker"],"title":"Configuring WSL2 for Seamless Compatibility with Rancher Desktop","uri":"/2023/11/09/rancher_desktop_wsl2/#conclusion"},{"categories":null,"content":"In my last article, I dug into Distributed Tracing and exposed how to enable it in Java applications. We didn‚Äôt see yet how to deploy an application on Kubernetes and get distributed tracing insights. Several strategies can be considered, but the main point is how to minimize the impact of deploying APM agents on the whole delivery process. In this article, I will expose how to ship APM agents for instrumenting Java applications deployed on top of Kubernetes through Docker containers. To make it clearer, I will illustrate this setup by the following use case: We have an API ‚ÄúMy wonderful API‚Äù which is instrumented through an Elastic APM agent. The data is then sent to the Elastic APM. Now, if we dive into the ‚ÄúWonderful System‚Äù, we can see the Wonderful Java application and the agent: Elastic APM vs Grafana/OpenTelemetry In this article I delve into how to package an Elastic APM agent and enable Distributed Tracing with the Elastic APM suite. You can do that in the same way with an OpenTelemetry Agent. Furthermore, Elastic APM is compatible with OpenTelemetry. We can basically implement this architecture in two different ways: Deploying the agent in all of our Docker images Deploying the agent asides from the Docker images and using initContainers to bring the agent at the startup of our applications We will then see how to lose couple application docker images to the apm agent one. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:0:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#"},{"categories":null,"content":" 1 Why not bringing APM agents in our Docker images?It could be really tempting to put the APM agents in the application‚Äôs Docker image. Why? Because you just have to add the following lines of code in our Docker images definition: RUN mkdir /opt/agent COPY ./javaagent.jar /opt/agent/javaagent.jar Nonetheless, if you want to upgrade your agent, you will have to repackage it and redeploy all your Docker images. For regular upgrades, it will not bother you, but, if you encounter a bug or a vulnerability, it will be tricky and annoying to do that. What is why I prefer loose coupling the ‚Äúbusiness‚Äù applications Docker images to technical tools such as APM agents. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:1:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#why-not-bringing-apm-agents-in-our-docker-images"},{"categories":null,"content":" 2 Deploy an APM agent through initContainersWhile looking around how to achieve this, I came across to the Kubernetes initContainers. This kind of container is run only once during the startup of every pod. A bunch of commands is ran then on top of it. For our current use case, it will copy the javaagent into a volume such as an empty directory volume. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#deploy-an-apm-agent-through-initcontainers"},{"categories":null,"content":" 2.1 Impacts in the ‚ÄúWonderful Java Application Docker imageThe main impact is to declare a volume in your Docker image: VOLUME /opt/agent It will be used by both the Docker container and the initContainer. We can consider it as a ‚Äúbridge‚Äù between these two ones. We also have to declare one environment variable: JAVA_OPTS. For instance: ENV JAVA_OPTS=$JAVA_OPTS [...] ENTRYPOINT [\"sh\", \"-c\", \"java ${JAVA_OPTS} org.springframework.boot.loader.JarLauncher\"] Il will be used during the deployment to set up our Wonderful Java Application. Now, let‚Äôs build our initContainer‚Äôs Docker image. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:1","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#impacts-in-the-_wonderful-java-application_-docker-image"},{"categories":null,"content":" 2.2 InitContainer Docker Image creationIt is really straightforward. We can use for example, the following configuration: FROM alpine:latest RUN mkdir -p /opt/agent_setup RUN mkdir /opt/agent COPY ./javaagent.jar /opt/agent_setup/javaagent.jar VOLUME /opt/agent ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:2","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#initcontainer-docker-image-creation"},{"categories":null,"content":" 2.3 Kubernetes configurationWe can now set up our Kubernetes Deployment to start the corresponding container and copy the Java agent. kind: Deployment spec: containers: - name: java-app image: repo/my-wonderful-java-app:v1 volumeMounts: - mountPath: /opt/agent name: apm-agent-volume initContainers: - command: - cp - /opt/agent_setup/javaagent.jar - /opt/agent name: apm-agent-init image: repo/apm-agent:v1 volumeMounts: - mountPath: /opt/agent name: appd-agent-volume volumes: - name: appd-agent-volume emptyDir: {} Why not just copying the Java agent directly in the initContainer Docker image execution? The copy must be run with a command specified in the initContainer declaration and cannot be done during the initContainer execution (i.e., specified in its Dockerfile). Why? The volume is mounted just after the initContainer execution and drops the JAR file copied earlier. ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:2:3","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#kubernetes-configuration"},{"categories":null,"content":" 3 Start the Java Application with the agentLast but not least, we can now configure the pods where we run our Java applications. We will use the JAVA_OPTS environment variable to configure the location of the Java agent, and the Elastic APM Java system properties. For instance: JAVA_OPTS=-javaagent:/opt/agent/javaagent.jar -Delastic.apm.service_name=my-wonderful-application -Delastic.apm.application_packages=org.mywonderfulapp -Delastic.apm.server_url=http://apm:8200 You can then configure your Kubernetes deployment as: spec: containers: - name: java-app env: - name: JAVA_OPTS value: -javaagent:/opt/agent/javaagent.jar -Delastic.apm.service_name=my-wonderful-application -Delastic.apm.application_packages=org.mywonderfulapp -Delastic.apm.server_url=http://apm:8200 Et voila! ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:3:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#start-the-java-application-with-the-agent"},{"categories":null,"content":" 4 ConclusionWe have seen how to pack and deploy Distributed Tracing java agents and Java Applications built on top of Docker images. Obviously, my technical choice of using an InitContainer can be challenged regarding your technical context and how you are confortable with your delivery practices. You probably noticed I use an emptyDir to deploy the Java agent. Normally it will not be a big deal, but I advise you to check this usage with your Kubernetes SRE/Ops/Administrator first. Anyway, I think it is worth it and the tradeoffs are more than acceptable because this approach are, in my opinion, more flexible than the first one. Hope this helps! ","date":"2023-11-01","objectID":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/:4:0","series":null,"tags":["Distributed_Tracing","Java","APM","Docker","Elastic"],"title":"Streamline Java Application Deployment: Pack, Ship, and Unlock Distributed Tracing with Elastic APM on Kubernetes","uri":"/2023/11/01/pack-ship-java-deployment-distributed-tracing-elasticapm/#conclusion"},{"categories":null,"content":"Picture Credit: Nick FEWINGS ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:0:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#"},{"categories":null,"content":" 1 IntroductionIn today‚Äôs dynamic landscape, Distributed Tracing has emerged as an indispensable practice. It helps to understand what is under the hood of distributed transactions, providing answers to pivotal questions: What comprises these diverse requests? What contextual information accompanies them? How extensive is their duration? Since the introduction of Google‚Äôs Dapper, a plethora of tracing solutions has flooded the scene. Among them, OpenTelemetry has risen as the frontrunner. Other alternatives such as Elastic APM and DynaTrace are also available. This toolkit seamlessly aligns with APIs and synchronous transactions, catering to a broad spectrum of scenarios. However, what about asynchronous transactions? The necessity for clarity becomes even more pronounced in such cases. Particularly in architectures built upon messaging or event streaming brokers, attaining a holistic view of the entire transaction becomes arduous. Why does this challenge arise? It‚Äôs a consequence of functional transactions fragmenting into two loosely coupled subprocesses: Hopefully you can rope OpenTelemetry in it to shed light. What about the main concepts of Distributed Tracing? I will not dig into the concepts of Distributed tracing in this article. If you are interested in it, you can read my article on the Worldline Tech Blog. I will explain in this article how to set up and plug OpenTelementry to gather asynchronous transaction traces using Apache Camel and Artemis. The first part will use Jaeger and the second one, Tempo and Grafana to be more production ready. All the code snippets are part of this project on GitHub. (Normally) you can use and run it locally on your desktop. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:1:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#introduction"},{"categories":null,"content":" 2 Jaeger","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#jaeger"},{"categories":null,"content":" 2.1 ArchitectureThe SPANs are broadcast and gathered through OpenTelemetry Collector. It finally sends them to Jaeger. Here is the architecture of such a platform: ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture"},{"categories":null,"content":" 2.2 OpenTelemetry CollectorThe cornerstone of this architecture is the collector. It can be compared to Elastic LogStash or an ETL. It will help us get, transform and export telemetry data. Source: https://opentelemetry.io/docs/collector/ For our use case, the configuration is quite simple. First, here is the Docker Compose configuration: otel-collector: image: otel/opentelemetry-collector:0.75.0 container_name: otel-collector command: [ \"--config=/etc/otel-collector-config.yaml\" ] volumes: - ./docker/otel-collector-config.yaml:/etc/otel-collector-config.yaml ports: - \"1888:1888\" # pprof extension - \"8888:8888\" # Prometheus metrics exposed by the collector - \"8889:8889\" # Prometheus exporter metrics - \"13133:13133\" # health_check extension - \"4317:4317\" # OTLP gRPC receiver - \"55670:55679\" # zpages extension and the otel-collector-config.yaml: # (1) receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] # (2) exporters: # prometheus: # endpoint: \"0.0.0.0:8889\" # const_labels: # label1: value1 logging: jaeger: endpoint: jaeger:14250 tls: insecure: true # zipkin: # endpoint: http://zipkin:9411/api/v2/spans # tls: # insecure: true # (3) processors: batch: extensions: health_check: pprof: endpoint: :1888 zpages: endpoint: :55679 # (4) service: extensions: [pprof, zpages, health_check] pipelines: traces: receivers: [otlp] processors: [batch] exporters: [logging, jaeger] metrics: receivers: [otlp] processors: [batch] exporters: [logging] Short explanation If you want further information about this configuration, you can browse the documentation. For those who are impatient, here are a short explanation of this configuration file: Where to pull data? Where to store data? What to do with it? What are the workloads to activate? ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#opentelemetry-collector"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: \u003cdependency\u003e \u003cgroupId\u003eorg.apache.camel.springboot\u003c/groupId\u003e \u003cartifactId\u003ecamel-opentelemetry-starter\u003c/artifactId\u003e \u003c/dependency\u003e In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don‚Äôt know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration \u003cprofile\u003e \u003cid\u003eopentelemetry\u003c/id\u003e \u003cactivation\u003e \u003cproperty\u003e \u003cname\u003eapm\u003c/name\u003e \u003cvalue\u003eotel\u003c/value\u003e \u003c/property\u003e \u003c/activation\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.apache.maven.plugins\u003c/groupId\u003e \u003cartifactId\u003emaven-dependency-plugin\u003c/artifactId\u003e \u003cexecutions\u003e \u003cexecution\u003e \u003cid\u003ecopy-javaagent\u003c/id\u003e \u003cphase\u003eprocess-resources\u003c/phase\u003e \u003cgoals\u003e \u003cgoal\u003ecopy\u003c/goal\u003e \u003c/goals\u003e \u003cconfiguration\u003e \u003cartifactItems\u003e \u003cartifactItem\u003e \u003cgroupId\u003eio.opentelemetry.javaagent\u003c/groupId\u003e \u003cartifactId\u003eopentelemetry-javaagent\u003c/artifactId\u003e \u003cversion\u003e${opentelemetry-agent.version}\u003c/version\u003e \u003coverWrite\u003etrue\u003c/overWrite\u003e \u003coutputDirectory\u003e${project.build.directory}/javaagents\u003c/outputDirectory\u003e \u003cdestFileName\u003ejavaagent.jar\u003c/destFileName\u003e \u003c/artifactItem\u003e \u003c/artifactItems\u003e \u003c/configuration\u003e \u003c/execution\u003e \u003c/executions\u003e \u003c/plugin\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cagents\u003e \u003cagent\u003e${project.build.directory}/javaagents/javaagent.jar\u003c/agent\u003e \u003c/agents\u003e \u003c!-- \u003csystemPropertyVariables\u003e--\u003e \u003c!-- \u003cotel.traces.sampler\u003eparentbased_traceidratio\u003c/otel.traces.sampler\u003e--\u003e \u003c!-- \u003cotel.traces.sampler.arg\u003e0.2\u003c/otel.traces.sampler.arg\u003e--\u003e \u003c!-- \u003c/systemPropertyVariables\u003e--\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/profile\u003e The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .route","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don‚Äôt know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .route","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#libraries-to-add"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don‚Äôt know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .route","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#what-about-the-code-1"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don‚Äôt know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .route","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#the-java-agent"},{"categories":null,"content":" 2.3 What about the code?The configuration to apply is pretty simple and straightforward. To cut long story short, you need to include libraries, add some configuration lines and run your application with an agent which will be responsible for broadcasting the SPANs. 2.3.1 Libraries to addFor an Apache Camel based Java application, you need to add this starter first: org.apache.camel.springboot camel-opentelemetry-starter In case you set up a basic Spring Boot application, you only have to configure the agent (see below). 2.3.2 What about the code?This step is not mandatory. However, if you are eager to get more details in your Jaeger dashboard, it is advised. In the application class, you only have to put the @CamelOpenTelemetry annotation. @CamelOpenTelemetry @SpringBootApplication public class DemoApplication { [...] If you want more details, you can check the official documentation. 2.3.3 The Java AgentThe java agent is responsible for instrumenting Java 8+ code, capturing metrics and forwarding them to the collector. In case you don‚Äôt know what is a Java Agent, I recommend watching this conference. Its documentation is available on GitHub. The detailed list of configuration parameters is available here. You can configure it through environment, system variables or a configuration file. For instance, by default, the OpenTelemetry Collector default endpoint value is http://localhost:4317. You can alter it by setting the OTEL_EXPORTER_OTLP_METRICS_ENDPOINT environment variable or the otel.exporter.otlp.metrics.endpoint java system variable (e.g., using -Dotel.exporter.otlp.metrics.endpoint option ). In my example, we use Maven configuration to download the agent JAR file and run our application with it as an agent. Example of configuration opentelemetry apm otel org.apache.maven.plugins maven-dependency-plugin copy-javaagent process-resources copy io.opentelemetry.javaagent opentelemetry-javaagent ${opentelemetry-agent.version} true ${project.build.directory}/javaagents javaagent.jar org.springframework.boot spring-boot-maven-plugin ${project.build.directory}/javaagents/javaagent.jar The variables in comment (e.g., otel.traces.sampler) can be turned on if you want to sample your forwarded data based on a head rate limiting. Before running the whole application (gateway, producer,consumer), you must ramp up the infrastructure with Docker compose. The source is available here. cd containers docker compose up You can now start both the producer and the consumer: mvn clean spring-boot:run -Popentelemetry -f camel-producer/pom.xml mvn clean spring-boot:run -Popentelemetry -f camel-consumer/pom.xml The gateway can also be turned on and instrumented in the same way. You can run it as: mvn clean spring-boot:run -Popentelemetry -f gateway/pom.xml 2.3.4 How is made the glue between the two applications?The correlation is simply done using headers. For instance, in the consumer application, when we consume the messages as: from(\"activemq:queue:HELLO.WORLD?disableReplyTo=true\") .route","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#how-is-made-the-glue-between-the-two-applications"},{"categories":null,"content":" 2.4 DashboardTo get traces, I ran this dumb command to inject traces into Jaeger: while true ; http :9080/camel/test; end Now, you can browse Jaeger (http://localhost:16686) and query it to find trace insights: Number of different apps If you dig into one transaction, you will see the whole transaction: One transaction And now, you can correlate two sub transactions: Two sub transactions ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:2:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard"},{"categories":null,"content":" 3 Tempo \u0026 GrafanaThis solution is pretty similar to the previous one. Instead of pushing all the data to Jaeger, we will use Tempo to store data and Grafana to render them. We don‚Äôt need to modify the configuration made in the existing Java applications. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo--grafana"},{"categories":null,"content":" 3.1 ArchitectureAs mentioned above, the architecture is quite the same. Now, we have the collector which broadcast data to Tempo. We will then configure Grafana to query to it to get traces. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:1","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#architecture-1"},{"categories":null,"content":" 3.2 Collector configurationThe modification of the Collector is easy (for this example). We only have to specify the tempo URL. receivers: otlp: protocols: grpc: endpoint: \"0.0.0.0:4317\" http: endpoint: \"0.0.0.0:4318\" prometheus: config: scrape_configs: - job_name: 'test' metrics_path: '/actuator/prometheus' scrape_interval: 5s static_configs: - targets: ['host.docker.internal:8080'] exporters: otlp: endpoint: tempo:4317 tls: insecure: true service: pipelines: traces: receivers: [otlp] exporters: [otlp] ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:2","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#collector-configuration"},{"categories":null,"content":" 3.3 Tempo configurationI used here the standard configuration provided in the documentation: server: http_listen_port: 3200 distributor: receivers: # this configuration will listen on all ports and protocols that tempo is capable of. jaeger: # the receives all come from the OpenTelemetry collector. more configuration information can protocols: # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver thrift_http: # grpc: # for a production deployment you should only enable the receivers you need! thrift_binary: thrift_compact: zipkin: otlp: protocols: http: grpc: opencensus: ingester: max_block_duration: 5m # cut the headblock when this much time passes. this is being set for demo purposes and should probably be left alone normally compactor: compaction: block_retention: 1h # overall Tempo trace retention. set for demo purposes metrics_generator: registry: external_labels: source: tempo cluster: docker-compose storage: path: /tmp/tempo/generator/wal remote_write: - url: http://prometheus:9090/api/v1/write send_exemplars: true storage: trace: backend: local # backend configuration to use wal: path: /tmp/tempo/wal # where to store the wal locally local: path: /tmp/tempo/blocks overrides: metrics_generator_processors: [service-graphs, span-metrics] # enables metrics generator search_enabled: true ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:3","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#tempo-configuration"},{"categories":null,"content":" 3.4 Grafana configurationNow we must configure Grafana to enable querying into our tempo instance. The configuration is made here using a configuration file provided during the startup The datasource file: apiVersion: 1 datasources: # Prometheus backend where metrics are sent - name: Prometheus type: prometheus uid: prometheus url: http://prometheus:9090 jsonData: httpMethod: GET version: 1 - name: Tempo type: tempo uid: tempo url: http://tempo:3200 jsonData: httpMethod: GET serviceMap: datasourceUid: 'prometheus' version: 1 ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:4","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#grafana-configuration"},{"categories":null,"content":" 3.5 DashboardAs we have done before, we must start the infrastructure using Docker Compose: cd containers docker compose -f docker-compose-grafana.yml up Then, using the same rocket scientist maven commands, we can run the same commands and browse now Grafana (http://localhost:3000) to see our traces: Transactions Deep dive into one transaction ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:3:5","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#dashboard-1"},{"categories":null,"content":" 4 ConclusionWe saw how to highlight asynchronous transactions and correlate them through OpenTelemetry and Jaeger or using Tempo \u0026 Grafana. It was voluntarily simple. If you want to dig into OpenTelemetry Collector configuration, you can read this article from Antik ANAND (Thanks to Nicolas FRANK√ãL for sharing it) and the official documentation. A noteworthy aspect of OpenTelemetry lies in its evolution into an industry-standard over time. For instance,Elastic APM is compatible with it. I then exposed how to enable this feature on Apache Camel applications. It can be easily reproduced with several stacks. Last but not least, which solution is the best? I have not made any benchmark of Distributed Tracing solutions. However, for a real life production setup, I would dive into Grafana and Tempo and check their features. I am particularly interested in mixing logs, traces to orchestrate efficient alerting mechanisms. ","date":"2023-09-05","objectID":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/:4:0","series":null,"tags":["OpenTelemetry","Java","Camel","Artemis","Grafana","Tempo"],"title":"Enhancing Asynchronous Transaction Monitoring: Implementing Distributed Tracing in Apache Camel Applications with OpenTelemetry","uri":"/2023/09/05/distributed-tracing-opentelemetry-camel-artemis/#conclusion"},{"categories":null,"content":"While chatting with one of my WL colleague, I stumbled upon Fish shell. I immediately liked its autocompletion and extensibility mechanisms. After many years using BASH and ZSH, I therefore decided to move on to this new shell. Unlike the others, it‚Äôs not POSIX-compatible. Furthermore, to get (at least) the same functionalities as OhMyZsh, I chose to install StarShip. I will then describe how I moved on and updated my existing tools such as SdkMan. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:0:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#"},{"categories":null,"content":" 1 FISH Installation OS I applied these commands on both Ubuntu20/WSL2 and Linux Mint. To install it, run this command: sudo apt install fish You must also use a font available on the NerdFonts website. By the way, you can also use the fonts available through your package manager. For instance, I chose using JetBrains Mono After downloading it, you can reload your font cache running this command: fc-cache -fv ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:1:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#fish-installation"},{"categories":null,"content":" 2 StarShip installationI ran this command: curl -sS https://starship.rs/install.sh | sh How to update StarShip To update StarShip, you must use the same command. I also added the following command at the end of ~/.config/fish/config.fish: starship init fish | source Due to some WSL2 incompatibilities, I also chose to use the plain text presets running this command: starship preset plain-text-symbols -o ~/.config/starship.toml ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:2:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#starship-installation"},{"categories":null,"content":" 3 SDKMAN updateAt this stage, SdkMan didn‚Äôt work at all. To put it alive again, I had to install Fisher and a SdkMan for fish plugin. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#sdkman-update"},{"categories":null,"content":" 3.1 Fisher installRun this command: curl -sL https://raw.githubusercontent.com/jorgebucaran/fisher/main/functions/fisher.fish | source \u0026\u0026 fisher install jorgebucaran/fisher ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:1","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#fisher-install"},{"categories":null,"content":" 3.2 SdkMan for fish pluginRun this command: fisher install reitzig/sdkman-for-fish@v2.0.0 ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:2","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#sdkman-for-fish-plugin"},{"categories":null,"content":" 3.3 Run SdkManRun this command: sdk ug Say yes and restart a shell. Now it should work. ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:3:3","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#run-sdkman"},{"categories":null,"content":" 4 NVMI had the same issue with NVM. I then installed another plugin with Fisher: fisher install jorgebucaran/nvm.fish ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:4:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#nvm"},{"categories":null,"content":" 5 GnuPGI use GnuPG for signing my GIT commits. Installing Fisher broke my setup. I then added this new configuration file $HOME/.config/fish/conf.d/config_gpgagent.fish with the following content: set -gx GPG_TTY /dev/pts/0 To activate it, restart your shell (again). ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:5:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#gnupg"},{"categories":null,"content":" 6 ConclusionI can now use FISH for my daily job. As I said first, this article is only a reminder for my next setups (aka when I will broke my GNU/Linux boxes and try to restore them). Hope it will help you! ","date":"2023-07-21","objectID":"/2023/07/21/fish-shell/:6:0","series":null,"tags":["shell","GNU/Linux"],"title":"Moving on to Fish shell (and beyond)","uri":"/2023/07/21/fish-shell/#conclusion"},{"categories":null,"content":" 1 Once upon a time an API ‚Ä¶ Second Law of Consulting ‚ÄúNo matter how it looks at first, it‚Äôs always a people problem‚Äù - Gerald M. Weinberg Once upon a time, the ACME Corporation was building a brand new IT product. It aimed at a new software to manage bookstores through a web interface and an API. In the first steps, the developers drew up a first roadmap of their API based on the expectations of their first customers. They therefore built and shipped a microservices platform and released their first service contract for their early adopters. Here is the design of this platform: The High level design More in depth To sum up To cut long story short, we have a microservices platform based on the Spring Boot/Cloud Stack exposed through an API Gateway and secured using OpenID Connect. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:1:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#once-upon-a-time-an-api-"},{"categories":null,"content":" 2 The platform and its roadmapAfter shipping it into production, they drew up a roadmap for their existing customers to both improve the existing features and bring new ones. As of now, we could think everything is hunky-dory isn‚Äôt it? While engineers worked on improving the existing API, the sales representative have contracted with new customers. They enjoyed this product and its functionalities. However, they also ask for new requirements and concerns. Some of them are easy to apply, some not. For instance, a new customer asked the ACME engineers for getting a summary for every book and additional REST operations. Easy! However, last but not least, this customer would also get a list of authors for every book whereas the existing application only provides ONE author per book. This is a breaking change! What is a breaking change? A breaking change occurs when the backward compatibility is broken between two following versions. For instance, when you completely change the service contract on your API, a client which uses the old API definition is unable to use your new one. A common theoretical approach could be to apply versions on our APIs and adapt it according to the customer. Unfortunately, the devil is in the details. I will describe in this article attention points I struggled with in my last projects. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:2:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#the-platform-and-its-roadmap"},{"categories":null,"content":" 3 What to version? How and where to apply it?After answering to the first question: Do I really need API versioning? you then have to answer to this new one: what should we consider versioning? You only have to version the service contract. In the case of a simple web application based on a GUI and an API Versioning is applied in the service contract of your API. If you change your database without impacting the APIs, why should you waste your time creating and managing a version of your API? It doesn‚Äôt make sense. On the other way around, when you evolve your service contract, you usually impact your database (e.g., see the example of breaking change above). Moreover, the version is usually specified on the ‚Äúmiddleware‚Äù side, where your expose your API. I‚Äôll come back to this point in a later section. If you want to dig into what is a breaking change and what to version, you can read this guide on the GitHub website. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#what-to-version-how-and-where-to-apply-it"},{"categories":null,"content":" 3.1 How many versions must I handle?Tough question! Throughout my different experiences struggling with API versioning, the most acceptable trade-off for both the API provider and customer/client was to only handle two versions: the current and the deprecated one. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#how-many-versions-must-i-handle"},{"categories":null,"content":" 3.2 Where?Now, you have to answer to this question: Where should I handle the version? On the Gateway? On Every Backend? On every service or on every set of services? Directly in the code managed by different packages. Usually, I prefer manage it on the gateway side and don‚Äôt bother with URL management on every backend. It could avoid maintenance on both code and tests for every release. However, you can‚Äôt have this approach on monolithic applications (see below). ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#where"},{"categories":null,"content":" 3.3 How to define it?Here are three ways to define API versions: In the URL (e.g., /v1/api/books) In a HTTP header (e.g., X-API-VERSION: v1) In the content type (e.g., Accept: application/vnd.myname.v1+json) The last one is now deprecated. The RFC 9110 deprecates now custom usages of the accept HTTP header. I strongly prefer the first one. It is the most straightforward. For instance, if you provide your books API first version, you can declare this URL in your OpenAPI specification:/v1/api/books. The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it‚Äôs less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn‚Äôt specify the header? Is there any default version? Yes, you can read the documentation to answer these questions, but who (really) does? The version declared here is pretty clear and difficult to miss. If you specify the version in a HTTP header, it‚Äôs less clear. If you have this URL /api/books and the version specified in this header: X-API-VERSION: v1, what would be the version called (or not) if you didn‚Äôt specify the header? Is there any default version? Yes, you can read the documentation, but who (really) does? The first solution (i.e., version in the URL) mandatorily conveys the associated version. It is so visible for all the stakeholders and could potentially avoir any mistakes or headaches while debugging. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:3:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#how-to-define-it"},{"categories":null,"content":" 4 What about the main software/cloud providers?Before reinventing the wheel, let‚Äôs see how the main actors of our industry deal with this topic. I looked around and found three examples: ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#what-about-the-main-softwarecloud-providers"},{"categories":null,"content":" 4.1 Google The version is specified in the URL It only represents the major versions which handle breaking changes ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#google"},{"categories":null,"content":" 4.2 Spotify The version is specified in the URL The API version is still V1 ‚Ä¶ ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#spotify"},{"categories":null,"content":" 4.3 Apple The version is specified in the URL The API version is still V1 ‚Ä¶ ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:4:3","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#apple"},{"categories":null,"content":" 5 Appropriate (or not) technologiesIn my opinion, technologies based on the monolith pattern don‚Äôt fit handling properly API Versioning. If you are not eager to execute two versions of your monolith, you would have to provide both of the two versions within the same app and runtime. You see the point? You would therefore struggle with: packaging testing both of two releases for every deployment even if a new feature doesn‚Äôt impact the deprecated version removing, add new releases in the same source code,‚Ä¶ And loosing your mind. In my opinion, best associated technologies are more modular whether during the development or deployment phases. For instance, if you built your app with Container based (Docker, Podman, K8S,..) stack, you would easily switch from one version to another, and sometimes you would be able to ship new features without impacting the oldest version. However, we need to set up our development and integration workflow to do that. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:5:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#appropriate-or-not-technologies"},{"categories":null,"content":" 6 Configuration management \u0026 delivery automationWhen I dug into API versioning, I realised it impacts projects organisation and, by this way, the following items: The source code management: one version per branch or not? The release process: How to create releases properly? Fixes, merges,‚Ä¶: How to apply fixes among branches and versions? The delivery process: How to ship you versions? Yes it IS a big deal Here is the least bad approach I think it works while addressing all of these concerns: ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#configuration-management--delivery-automation"},{"categories":null,"content":" 6.1 Source code configurationWhen you want to have two different versions in production, you must decouple your work in several GIT (what else) branches. For that, I usually put in place GitFlow. source: Atlassian Usually, using this workflow, we consider the develop branch serves as an integration branch. But, now we have two separate versions? Yes, but don‚Äôt forget we have a current version and a deprecated one. SemVer I base my versioning naming and numbers on SemVer To handle API versions, we can use release branches. You can easily declare versions regarding your API versions. For instance: release/book-api-1.0.1 release/book-api-2.0.1 We can so have the following workflow: Develop features in feature branches and merge them into the develop branch. Release and use major release numbers (or whatever) to identify breaking changes and your API version number Create binaries (see below) regarding the tags and release branches created Fix existing branches when you want to backport features brought by new features (e.g., when there is an impact on the database mapping), and release them using minor version numbers Apply fixes and create releases ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:1","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#source-code-configuration"},{"categories":null,"content":" 6.2 Delivery processAs of now, we saw how to design, create and handle versions. But, how to ship them? If you based your source code management on top of GitFlow, you would be able now to deliver releases available from git tags and release branches. The good point is you can indeed build your binaries on top of these. The bad one, is you must design and automatise this whole process in a CI/CD pipeline. Don‚Äôt forget to share it to all the stakeholders, whether developers, integrators or project leaders who are often involved in version definition. Hold on, these programs must be executed against a configuration, aren‚Äôt they? Nowadays, if we respect the 12 factors during our design and implementation, the configuration is provided through environment variables. To cut long story short, your API versioning will also impact your configuration. Thus, it becomes mandatory to externalise it and version it. You can do it in different ways. You can, for example, deploy a configuration server. It will provide configuration key/values regarding the version. If you want a live example, you can get an example in a workshop I held this year at SnowcampIO. The configuration is managed by Spring Cloud Config. You can also handle your configuration in your Helm Charts if you deploy your app on top of Kubernetes. Your configuration values will be injected directly during the deployment. Obviously if it‚Äôs a monolith, it will be strongly difficult. Why? Because you will lose flexibility on version management and the capacity on deploying several versions of your service. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:6:2","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#delivery-process"},{"categories":null,"content":" 7 Authorisation managementHere is another point to potentially address when we implement API versioning. When you apply an authorisation mechanism on your APIs using OAuthv2 or OpenID Connect, you would potentially have strong differences in your authorisation policies between two major releases. You would then restrict the usage of a version to specific clients or end users. One way to handle this is to use scopes stored in claims. In the use case we have been digging into, we can declare scopes such as: book:v1:write or number:v2:read to specify both the authorised action and the corresponding version. For example, here is a request to get an access_token from the v1 scopes: http --form post :8009/oauth2/token grant_type=\"client_credentials\" client_id=\"customer1\" client_secret=\"secret1\" scope=\"openid book:v1:write book:v1:write number:v1:read\" And the response could be: { \"access_token\": \"eyJraWQiOiIxNTk4NjZlMC0zNWRjLTQ5MDMtYmQ5MC1hMTM5ZDdjMmYyZjciLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiJjdXN0b21lcjIiLCJhdWQiOiJjdXN0b21lcjIiLCJuYmYiOjE2NzI1MDQ0MTQsInNjb3BlIjpbImJvb2t2Mjp3cml0ZSIsIm51bWJlcnYyOnJlYWQiLCJvcGVuaWQiLCJib29rdjI6cmVhZCJdLCJpc3MiOiJodHRwOi8vbG9jYWxob3N0OjgwMDkiLCJleHAiOjE2Nz I1MDQ3MTQsImlhdCI6MTY3MjUwNDQxNH0.gAaDcOaORse0NPIauMVK_rhFATqdKCTvLl41HSr2y80JEj_EHN9bSO5kg2pgkz6KIiauFQ6CT1NJPUlqWO8jc8-e5rMjwWuscRb8flBeQNs4-AkJjbevJeCoQoCi_bewuJy7Y7jqOXiGxglgMBk-0pr5Lt85dkepRaBSSg9vgVnF_X6fyRjXVSXNIDJh7DQcQQ-Li0z5EkeHUIUcXByh19IfiFuw-HmMYXu9EzeewofYj9Gsb_7qI0Ubo2x7y6W2tvzmr2PxkyWbmoioZdY9K0 nP6btskFz2hLjkL_aS9fHJnhS6DS8Sz1J_t95SRUtUrBN8VjA6M-ofbYUi5Pb97Q\", \"expires_in\": 299, \"scope\": \"book:v1:write number:v1:read openid book:v1:read\", \"token_type\": \"Bearer\" } Next, you must validate every API call with the version exposed by your API gateway and the requested scope. When a client tries to reach an API version with inappropriate scopes (e.g., using book:v1:read scope for a client which only uses the v2). You will throw this error: { \"error\": \"invalid_scope\" } ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:7:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#authorisation-management"},{"categories":null,"content":" 8 And now something completely different: How to avoid versioning while evolving your API?You probably understood that versioning is totally cumbersome. Before putting in place all of these practices, there‚Äôs another way to add functionalities on a NON-versioned API without impacting your existing customers. You can add new resources, operations and data without impacting your existing users. {: .notice‚Äìwarning} With the help of serialization rules, your users would only use the data and operations they know and are confident with. You will therefore bring backward compatibility to your API. Just in case, you can anticipate API versioning by declaring a V1 prefix on your API URL and stick to it while it‚Äôs not mandatory to upgrade it. That‚Äôs how and why Spotify and Apple (see above) still stick to the V1. ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:8:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#and-now-something-completely-different-how-to-avoid-versioning-while-evolving-your-api"},{"categories":null,"content":" 9 Wrap-upYou probably understood when getting into this topic that API versioning is a project management issue with consequences that requires tackling difficult technical ones. To sum up, you need to ask yourself these questions: Do I need it? Can I postpone API versioning by dealing with serialisation rules and just adding new data or operations? Is my architecture design compatible? Are my source code management and delivery practices compatible? After coping with all these points, if you must implement API versioning, you would need to onboard all the different stakeholders, not just developers, to be sure your whole development and delivery process is well aligned with practice. And I forgot: Good luck! ","date":"2023-03-27","objectID":"/2023/03/27/rest-api-versioning/:9:0","series":null,"tags":["REST","API","Versioning"],"title":"Real life Rest API Versioning for dummies","uri":"/2023/03/27/rest-api-versioning/#wrap-up"},{"categories":null,"content":"Il y a deux ans d√©j√†, j‚Äôai migr√© mon site Wordpress sur un site statique h√©berg√© sur Github Pages. Ce dernier √©tait bas√© sur Ruby, Jekyll et Minimal mistakes. Bien que le projet Minimal Mistakes ne donnait plus trop de signes de vie, le rendu convenait. Cependant, j‚Äô√©tais bloqu√© sur diff√©rents points: La gestion d‚Äôarticles en anglais et fran√ßais Le th√®me dark (inutile donc indispensable) Quelques fonctionnalit√©s manquantes: par ex. MermaidJS J‚Äôai donc d√©cid√© de le migrer sur Hugo. Ce g√©n√©rateur de site est bas√© sur Go et est tr√®s rapide d‚Äôex√©cution. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:0:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#"},{"categories":null,"content":" 1 D√©marrageJe n‚Äôai pas migr√© le site comme indiquait la documentation. J‚Äôai pr√©f√©r√© cr√©er un nouveau site et copier coller le contenu existant, √† savoir les images et les articles. Je vous conseille de lire la documentation qui est bien faite. Ensuite, j‚Äôai choisi le th√®me LoveIt. Pour l‚Äôinstaller, il suffit de cloner le repo dans le r√©pertoire themes. git submodule add https://github.com/dillonzq/LoveIt.git themes/LoveIt ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:1:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#d√©marrage"},{"categories":null,"content":" 2 Reprise de donn√©esJ‚Äôai copi√© les √©l√©ments suivants: Les fichiers statiques que j‚Äôavais √† disposition (CNAME, robots.txt) dans le r√©pertoire static Les images dansle r√©pertoire static/assets/images Les posts et pages statiques ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:2:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#reprise-de-donn√©es"},{"categories":null,"content":" 3 Travail sur les posts et images","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#travail-sur-les-posts-et-images"},{"categories":null,"content":" 3.1 Les postsJ‚Äôai ensuite modifi√© les noms des fichiers en enlevant les dates qui les pr√©fixaient. Ensuite, j‚Äôai modifi√© les en-t√™tes de chaque post. J‚Äôai pu le faire en automatisant avec VS CODE. Voici le pattern que j‚Äôai modifi√©: header: teaser: /assets/images/2018/02/2000px-cygwin_logo-svg.png en featuredImagePreview: /assets/images/2022/12/review.webp Ca c‚Äô√©tait le plus facile‚Ä¶ ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-posts"},{"categories":null,"content":" 3.2 Les imagesDans chaque post, j‚Äôai revu les images et leur positionnement. J‚Äôai donc pass√© chaque article manuellement. C‚Äô√©tait r√©ellement fastidieux. Pour √™tre totalement franc, je n‚Äôai paas cherch√© √† automatiser √ßa. Je pense qu‚Äôun script shell, python aurait pu faire l‚Äôaffaire. Heureusement, je n‚Äôen avais pas une centaine‚Ä¶ J‚Äôai ajout√© quand je pouvais l‚Äôen-t√™te suivant (adaptez le chemin vers l‚Äôimage ;-) ): featuredImage: /assets/images/2022/12/review.webp images: [\"/assets/images/2022/12/review.webp\"] Le premier attribut permet d‚Äôavoir une image d‚Äôen-t√™te pour l‚Äôarticle. Le second permet d‚Äôavoir l‚Äôimage lors d‚Äôun partage sur un r√©seau social (ex. Twitter) j‚Äôai ajout√© le code suivant pour centrer les images: {{\u003c style \"text-align:center\" \u003e}} ![dataflow](/assets/images/2022/08/maksym-tymchyk-vHO-yT1BDWk-unsplash.webp) {{\u003c style \u003e}} ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:3:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#les-images"},{"categories":null,"content":" 4 ConfigurationPour garder les m√™mes URLs, j‚Äôai choisi de modifier le pattern d‚Äô URL pour inclure la date. C‚Äôest un vieux reliquat de mon blog Wordpress. [languages.fr.permalinks] posts = '/:year/:month/:day/:filename/' Pour le reste, j‚Äôai copi√© coll√© l‚Äôexemple fourni par le th√®me et renseign√© les champs en fonction de ce que je voulais. J‚Äôai ensuite adapt√© le multi langue pour avoir la possibilit√© de faire des articles en anglais et en fran√ßais. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#configuration"},{"categories":null,"content":" 4.1 Moteur de rechercheJ‚Äôutilise Lunr. Voici la configuration: [languages.en.params.search] enable = true type = \"lunr\" contentLength = 4000 placeholder = \"\" maxResultLength = 10 snippetLength = 50 highlightTag = \"em\" absoluteURL = false Il faut √©galement penser √† activer la sortie au format JSON: # Options to make hugo output files [outputs] home = [\"HTML\", \"RSS\", \"JSON\"] page = [\"HTML\", \"MarkDown\"] section = [\"HTML\", \"RSS\"] taxonomy = [\"HTML\", \"RSS\"] taxonomyTerm = [\"HTML\"] ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:1","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#moteur-de-recherche"},{"categories":null,"content":" 4.2 CommentairesA l‚Äôinstar de mon blog avec Jekyll, j‚Äôutilise Utteranc.es. Voici la configuration: [params.page.comment.utterances] enable = true # owner/repo repo = \"alexandre-touret/alexandre-touret.github.io\" issueTerm = \"pathname\" label = \"\" lightTheme = \"github-light\" darkTheme = \"github-dark\" ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:4:2","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#commentaires"},{"categories":null,"content":" 5 ConclusionVous aurez bien compris que ma motivation principale derri√®re cette migration √©tait d‚Äô avoir un support multi langue un peu sympa. Vous avez dans cet article les principales actions que j‚Äôai r√©alis√©. N‚Äôh√©sitez pas √† regarder la configuration et les articles pour plus de d√©tails. ","date":"2023-03-03","objectID":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/:5:0","series":null,"tags":["jekyll","hugo"],"title":"Migrer un site Jekyll sur Hugo","uri":"/2023/03/03/migrer-un-site-jekyll-sur-hugo/#conclusion"},{"categories":null,"content":"You can read the English version below ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:0:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#"},{"categories":null,"content":" 1 2022 en quelques mots2023 est tout proche. Il est temps de faire un rapide bilan sur cette ann√©e 2022 (d‚Äôun point pro). Apr√®s avoir chang√© de projet en d√©but d‚Äôann√©e, j‚Äôai pu, gr√¢ce √† mon employeur Worldline, participer en tant que speaker √† 8 conf√©rences en fran√ßais et anglais 2 meetups une pr√©sentation en ligne √† Malt Academy J‚Äôai √©galement √©crit 6 articles sur mon blog et 4 sur le blog d‚Äôing√©nierie de Worldline. Je tiens √† remercier mon employeur pour me permettre de vivre cette exp√©rience ainsi que les organisatrices et organisateurs pour leur confiance et leur accueil (vous vous reconnaitrez). Merci √©galement √† toutes celles et ceux qui m‚Äôont aid√© √©galement √† monter en comp√©tence en tant que speaker. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:1:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-en-quelques-mots"},{"categories":null,"content":" 2 Et maintenant?Gr√¢ce √† l‚Äôinitiative 1Tech-Rel de Worldline, j‚Äôai pu faire des super rencontres et progresser techniquement dans certains domaines : Pr√©parer et proposer un sujet technique vous impose de le creuser √† fond et de le ma√Ætriser. Pour l‚Äôann√©e prochaine, on a quelques id√©es. J‚Äôesp√®re pouvoir communiquer l√†-dessus rapidement. Je vous souhaite en attendant un bon r√©veillon et si vous d√©couvrez mon article en 2023 une bonne ann√©e. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:2:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#et-maintenant"},{"categories":null,"content":" 3 2022 in few words2023 is coming. It is time to review 2022 in a professional point of view. After moving on to another project early 2022, I had the opportunity on behalf of my company Worldline, to participate as a speaker to: 8 tech conferences 2 meetups 1 online presentation at Malt Academy I also wrote 6 articles on my blog and 4 on the Worldline engineering blog. Thank you to my employer for giving me this opportunity and the organisers for their trust and their hospitality. I‚Äôm pretty sure you will recognise yourselves. Thanks then to who helped me in my speaker journey. ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:3:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#2022-in-few-words"},{"categories":null,"content":" 4 And now, something completely different?On behalf of the 2Worldline tech rel initiative, I had the chance to meet great people and move forward in some technical domains. Drawing up and submitting a talk about a technical topic requires you to dig into it and be proficient on it. I already have ideas for the next year. I hope communicating about it shortly. Best wishes for the new year eve and if you come across this article in 2023: Happy new year!! c‚Äôest comme des dev rel mais on essaye d‚Äôinclure au del√† des devs (ex. les OPS, SRE).¬†‚Ü©Ô∏é It is just like dev rel, but we would like to include all the tech crew (e.g., OPS \u0026 SRE)¬†‚Ü©Ô∏é ","date":"2022-12-31","objectID":"/2022/12/31/2022-wrap-up/:4:0","series":null,"tags":["conference","bilan"],"title":"2022 en quelques chiffres","uri":"/2022/12/31/2022-wrap-up/#and-now-something-completely-different"},{"categories":null,"content":"Pour ce dernier article de l‚Äôann√©e 2022, voici un rapide retour d‚Äôexp√©rience. Je suis actuellement en cours de pr√©paration d‚Äôun workshop pour l‚Äô√©dition 2023 de SnowcampIO. J‚Äôaborderai dans ce dernier le versioning des APIs REST. Pour illustrer ce sujet √¥ combien √©pineux, j‚Äôai r√©alis√© une plateforme ‚Äúmicroservices‚Äù en utilisant diff√©rents composants de la stack Spring. Container Tools Comments API Gateway Spring Cloud Gateway 2022.0.0-RC2 Bookstore API JAVA 17,Spring Boot 3.0.X ISBN API JAVA 17,Spring Boot 3.0.X Configuration Server Spring Cloud Config 2022.0.0-RC2 Database PostgreSQL Authorization Server JAVA 17,Spring Boot 3.0.X, Spring Authorization Server 1.0.0 En r√©sum√©, j‚Äôutilise Spring Boot, Cloud, Security, Authorization Server, Circuit Breaker, Spring Data,‚Ä¶ J‚Äôai d√©marr√© le d√©veloppement avant l‚Äôannonce officielle de la version 3.0 de Spring Boot. Ce n‚Äô√©tait pas r√©ellement obligatoire pour cet atelier, mais j‚Äôai souhait√© quand m√™me migrer cette application dans la derni√®re version de Spring Boot/Framework. Je vais d√©crire dans cet article comment j‚Äôai r√©ussi √† migrer toute cette stack et les choix que j‚Äôai fait pour que √ßa fonctionne. Bien √©videmment, cette application n‚Äôest pas une ‚Äúvraie‚Äù application en production. Par exemple, je n‚Äôai qu‚Äôune seule entit√© JPA‚Ä¶ Cependant, je la trouve repr√©sentative et esp√®re (tr√®s modestement) que mon retour d‚Äôexp√©rience pourra servir. La Pull Request correspondante est disponible sur GitHub. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:0:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#"},{"categories":null,"content":" 1 Pr√©-requisUne documentation existe. Vous pouvez la consulter ici. Il existe aussi plusieurs articles sur le blog du projet Spring. Voici un exemple. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:1:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#pr√©-requis"},{"categories":null,"content":" 2 D√©pendances et configuration des plugins","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#d√©pendances-et-configuration-des-plugins"},{"categories":null,"content":" 2.1 JDKPour Spring Boot 3, il faut imp√©rativement utiliser un JDK \u003e=17. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:1","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#jdk"},{"categories":null,"content":" 2.2 Mises √† jourL‚Äôune des premi√®res actions √† r√©aliser est de migrer votre application vers la version 2.7. √Ä l‚Äôheure o√π j‚Äô√©cris cet article, la version de Spring Cloud est encore en version RC. J‚Äôai donc d√ª ajouter le repository ‚Äúmilestone‚Äù de Spring: repositories { maven { url 'https://repo.spring.io/milestone' } mavenCentral() } Ensuite, j‚Äôai utilis√© les versions suivantes pour les diff√©rents composants spring: Spring Boot : 3.0.0 Spring Cloud : 2022.0.0-RC2 Spring Dependency Management : 1.1.0 Dans mon application, j‚Äôutilisais certains plugins Gradle pour la g√©n√©ration du code notamment OpenAPIGenerator. Pour ce dernier, j‚Äôai ajout√© un param√®tre pour le rendre compatible avec spring boot 3: useSpringBoot3 : \"true\" Bref, il faut imp√©rativement tous les mettre √† jour et v√©rifier la compatibilit√©‚ÄØ! ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:2:2","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#mises-√†-jour"},{"categories":null,"content":" 3 Ajout de nouvelles d√©pendancesPour v√©rifier la pertinence de certaines propri√©t√©s dans la nouvelle version, Spring a mis √† disposition ce plugin: runtimeOnly 'org.springframework.boot:spring-boot-properties-migrator' Il permet de notifier √† l‚Äôex√©cution si un param√®tre est d√©pr√©ci√© ou totalement inutile. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:3:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#ajout-de-nouvelles-d√©pendances"},{"categories":null,"content":" 4 Migration namespace javax vers jakartaeeSelon votre code, les d√©pendances que vous pouvez avoir, cette √©tape pourra aller du renommage des import javax vers jakarta √† d‚Äôinnombrables maux de t√™te. Si vous utilisez Spring Boot au-dessus d‚Äôun Tomcat (c.-√†-d. en mode old school), il vous faudra mettre √† jour le conteneur de servlet √† une version compatible. Dans mon application, je n‚Äôai eu qu‚Äô√† modifier les imports dans les entit√©s, filtres et m√©thodes annot√©es par l‚Äôannotation @PostConstruct(). import jakarta.persistence.Column; import jakarta.persistence.Entity; import jakarta.persistence.GeneratedValue; import jakarta.persistence.GenerationType; ... Sur ce sujet, Jetbrains a publi√© un tutoriel sur la migration vers Jakarta. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:4:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#migration-namespace-javax-vers-jakartaee"},{"categories":null,"content":" 5 Distributed Tracing et observabilit√©Spring embarque d√©sormais plusieurs fonctionnalit√©s li√©es √† l‚Äôobservabilit√© sous forme de starters. Dans mon cas, j‚Äôavais embarqu√© opentracing (qui √©tait d√©pr√©ci√© depuis quelques temps) et me connectait sur Jaeger. J‚Äôai suivi cet article paru sur le blog de Spring. J‚Äôai par cons√©quent bascul√© sur Zipkin (pour mon Workshop, l‚Äôutilisation du distributed tracing est un peu la cerise sur le g√¢teau). Voici les starters que j‚Äôai int√©gr√©s : implementation 'io.micrometer:micrometer-tracing-bridge-brave' implementation 'io.zipkin.reporter2:zipkin-reporter-brave' implementation 'io.opentelemetry:opentelemetry-exporter-zipkin' implementation 'org.springframework.boot:spring-boot-starter-aop' J‚Äôai par la suite int√©gr√© les propri√©t√©s suivantes dans la configuration: spring: zipkin: base-url: http://localhost:9411 sender: type: web management: tracing: sampling: probability: 1.0 metrics: distribution: percentiles-histogram: http: server: requests: true Je pense que j‚Äôaurai pu faire fonctionner Jaeger. Je n‚Äôai pas voulu perdre de temps (SnowcampIO arrive bient√¥t‚Ä¶). ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:5:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#distributed-tracing-et-observabilit√©"},{"categories":null,"content":" 6 Securit√©J‚Äôai eu quelques soucis apr√®s avoir mis √† jour Spring Authorization Server et Spring Security. Je pense que la version pr√©c√©dente de Spring √©tait plus permissive sur l‚Äôinjection et le nom des beans charg√©s dans les classes Configuration. J‚Äôai donc revu la validation c√¥t√© gateway et plus particuli√®rement la validation du jeton JWT. J‚Äôai d√ª notamment ajouter le param√®tre jwk-set-uri qui est obligatoire maintenant : resourceserver: jwt: jwk-set-uri: http://localhost:8009 Je n‚Äôai pas eu de r√©els probl√®mes cot√© Authorization Server car j‚Äôavais d√©j√† migr√© vers la version 0.4.0. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:6:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#securit√©"},{"categories":null,"content":" 7 ConclusionVous l‚Äôaurez compris, si vous faites l‚Äôeffort de suivre r√©guli√®rement les versions de Spring, vous devriez venir √† bout facilement de la migration vers la derni√®re version de Spring. N√©anmoins, sur des projets cons√©quents (et je ne parle pas de ceux o√π il n‚Äôy a de tests automatis√©s‚Ä¶) √ßa peut s‚Äôav√©rer co√ªteux. Certaines actions et contournements peuvent prendre du temps (ex. javax ‚Äì\u003e jakarta). Enfin, je vous conseille d‚Äôattendre la premi√®re version mineure et la version d√©finitive de Spring Cloud avant de vous lancer pour ‚Äúde vrai‚Äù. Bien que Spring ait fait un effort de documentation pour la migration, il est plus sage d‚Äôattendre que les premiers correctifs soient publi√©s avant de vous lancer. ","date":"2022-12-22","objectID":"/2022/12/22/migration_springboot3/:7:0","series":null,"tags":["spring","java"],"title":"Migrer son application Spring Boot vers la version 3","uri":"/2022/12/22/migration_springboot3/#conclusion"},{"categories":null,"content":"Apr√®s trois ans d‚Äôinactivit√© pour des raisons que l‚Äôon connait malheureusement toutes et tous, Devoxx Belgium √©tait de retour √† Anvers. Je n‚Äôavais jamais particip√© (en vrai) √† une conf√©rence internationale. C‚Äô√©tait donc une premi√®re pour moi. Pour y aller, j‚Äôai eu trois fois de la chance: J‚Äôai eu cette opportunit√© gr√¢ce √† Worldline - mon employeur J‚Äôai r√©ussi √† avoir un billet pendant les cinq minutes o√π se sont vendus les billets lors du premier batch Ma pr√©sentation au format Quickie a √©t√© retenue. J‚Äôai pr√©sent√© un talk √† Devoxx!!!!!!! Voici mon retour d‚Äôexp√©rience des trois jours de conf√©rence. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:0:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#"},{"categories":null,"content":" 1 Impressions g√©n√©ralesTout d‚Äôabord, j‚Äôai pu assister √† de nombreux Devoxx France. J‚Äôai cru na√Øvement que les deux √©v√®nements se ressembleraient. Je me suis tromp√©. Je ne dirai pas lequel est le meilleur1. Je ne saurais le dire. On est sur un autre type de conf√©rence. Il y a un peu moins de feedback de la communaut√©, m√™me si il y a eu une pr√©sentation de Doctolib et plus de pr√©sentations r√©alis√©es par des grands acteurs du march√© ou par des grands speakers internationaux (ex. Simon Ritter, Simon Brown ou James Gosling). Aussi, alors que la ligne √©ditoriale de Devoxx France s‚Äôest tourn√©e au fil des ann√©es sur d‚Äôautres langages et plateformes telles que NodeJS, Go, Scala, ici on est dans du Java pur et dur. Les (tr√®s) grands speakers de l‚Äô√©cosyst√®me sont pr√©sents et on fait des super talks: James Gosling, Simon Ritter, Mario Fusco, Gavin King ou Jos√© Paumard. En r√©sum√©, le coeur de la communaut√© Java bat √† Anvers pendant une semaine. Une majorit√© de Java champions sont +/- pr√©sents et nous font partager leur expertise. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:1:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#impressions-g√©n√©rales"},{"categories":null,"content":" 2 Les tendancesLes grandes tendances √©taient: L‚ÄôIA et les applications Le projet Loom GraalVM ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:2:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#les-tendances"},{"categories":null,"content":" 3 Quelques conf√©rencesL‚Äôensemble des conf√©rences est d√©j√† publi√© sur Youtube. N‚Äô h√©sitez pas √† les consulter. Il y a beaucoup de talks de qualit√©. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#quelques-conf√©rences"},{"categories":null,"content":" 3.1 Artificial Intelligence: You Are Here by Alan D ThompsonLe Dr Alan D. Thompson est un expert en intelligence artificielle. Il nous a donn√© une pr√©sentation pendant la keynote sur ce que l‚Äô IA peut r√©ellement faire de nos jours. C‚Äôest de plus en plus utilis√© dans notre industrie au travers de Github Copilot, Codegeex,‚Ä¶ Apr√®s nous avoir rappell√© la timeline de l‚Äôadoption de l‚Äô IA, il a illustr√© avec des peintures d√©ssin√©es par une IA comment un ordinateur peut maintenant comprendre une phrase en langage naturel et la traduire en image. Il a √©galement pr√©sent√© le langage de mod√©lisation GPT3. Vous pouvez trouver la vid√©o ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:3:1","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#artificial-intelligence-you-are-here-by-alan-d-thompson"},{"categories":null,"content":" 4 Revolutionizing Java-Based Applications with GraalVM by Alina Yurenko and Thomas WuerthingerDans cette pr√©sentation, les pr√©sentateurs d‚ÄôOracle ont abord√© une autre grande tendance du march√©: le retour au natif qui permet de limiter l‚Äôimpact sur le d√©marrage, la m√©moire et la taille des packages. Au travers d‚Äôun exemple bas√© sur Micronaut, ils ont expliqu√© comment GraalVM peut r√©pondre √† ces enjeux. Ils ont √©galement d√©mystifi√© plusieurs mythes li√©s √† GraalVM. Par exemple, GraalVM supporte la r√©flexion (‚Ä¶ et parfois non). On peut utiliser √©galement [Java Flight Recorder](Java Flight Recorder) pendant la compilation. Le support √† l‚Äôex√©cution des applications est bient√¥t pr√©vu. La developer experience √©tait √©galement √† l‚Äôordre du jour. Comment offrir une bonne exp√©rience alors que la compilation prend plus de temps? Pour r√©pondre √† cette √©pineuse question, ils ont conseill√© de gard√© le mode JIT avec une JVM pendant le d√©veloppement et d‚Äôutiliser l‚Äô AOT pour le d√©ploiement final. Ceci permet de disposer d‚Äôune machine puissante et de garantir la compatibilit√© mat√©rielle et OS de la machine de production. La vid√©o est disponible ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:4:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#revolutionizing-java-based-applications-with-graalvm-by-alina-yurenko-and-thomas-wuerthinger"},{"categories":null,"content":" 5 The lost art of software design by Simon BrownJ‚Äôutilise le mod√®le C4 depuis plusieurs ann√©es. Je l‚Äôai m√™me pr√©sent√© tr√®s bri√®vement dans mon talk. Aussi, j‚Äôai √©t√© tr√®s impressionn√© quand j‚Äôai pu assister √† la pr√©sentation de Simon Brown sur la conception logicielle. Il a expliqu√© pourquoi la conception n‚Äô√©tait pas conflictuelle avec les m√©thodes agiles. Ca permet notamment d‚Äô identtifier et de g√©rer les risques. Au del√† du mod√®le C4, il a montr√© comment identifier et √©valuer les diff√©rents risques avec le ‚ÄúRisk Storming‚Äù. Enfin, il a r√©pondu √† la question √† un million d‚Äôeuros: ‚ÄúQuand arr√™ter la conception?‚Äù Vous trouverez la r√©ponse ici. ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-lost-art-of-software-design-by-simon-brown"},{"categories":null,"content":" 5.1 Ahead Of Time and Native in Spring Boot 3.0 by St√©phane Nicoll \u0026 Brian ClozelUne autre conf√©rence qui met en avant GraalVM! Cette fois on abordait le support de l‚Äô AOT et du mode natif dans la future version de Spring Boot. Les pr√©sentateurs ont expliqu√©s comment Spring supportait le mode natif: le processus appliqu√©, la gestion des m√©tadonn√©es et l‚Äôanalyse r√©alis√©e. En (tr√®s tr√®s bref) r√©sum√©, l‚Äô AOT g√©n√®re des sources dont le chargement des Bean Definition. Ils ont √©galement point√© du doigt des changements que je consid√®re bloquants: On ne peut pas utiliser changer les profils au runtime La surcharge des propri√©t√©s et variable d‚Äôenvironnement n‚Äôest pas possible √† l‚Äôex√©cution On ne peut pas utiliser de Java Agent. Pour ce dernier point, cela risque de poser de nombreux soucis que √ßa soit l‚Äôutilisation d‚Äôun APM tel que Dynatrace ou le support de l‚ÄôAOP. A la fin de cette pr√©sentation, ils ont donn√© quelques recommendations. Parmi celles-ci: Ex√©cuter en d√©veloppement l‚Äôapplication en mode AOT avec une JVM Ex√©cuter les tests en mode natif Vous trouverez la vid√©o ici ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:1","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#ahead-of-time-and-native-in-spring-boot-30-by-st√©phane-nicoll--brian-clozel"},{"categories":null,"content":" 5.2 The Art of Java Language Pattern Matching by Simon RitterSimon Ritter a explor√© toutes les possibilit√© du pattern matching en Java. Toutes les fonctionnalit√©s ne sont pas encore disponibles. On peut n√©anmoins faire beaucoup de choses. Apr√®s un rappel sur les nouvelles fonctionnalit√©s depuis le JDK11 (Sealed classes, Records), Simon Ritter a illustr√© leur utilisation dans ce contexte. Si vous voulez tout conna√Ætre sur cette fonctionnalit√©, je vous conseille fortement de regarder ce talk. Voici la vid√©o ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:5:2","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#the-art-of-java-language-pattern-matching-by-simon-ritter"},{"categories":null,"content":" 6 ConclusionVoila les quelques talks qui m‚Äôont interpell√©. Il y en a beaucoup d‚Äôautres tels que ceux de Julien TOPCU ou Marcy ERICKA CHARELLOIS. Cette premi√®re participation √©tait tr√®s enrichissante. J‚Äôai eu √† plusieurs reprises l‚Äôimpression d‚Äôavoir l‚Äôinformation √† la source (ex. pour Spring). Si j‚Äôai autant de chance, je pense r√©√©diter l‚Äôexp√©rience l‚Äôann√©e prochaine. En tant que speaker pour une conf√©rence ou un workshop? Seul l‚Äôavenir nous le dira! Vous savez, moi je ne crois pas qu‚Äôil y aient de bonnes ou mauvaises conf√©rences. Moi, si je devais r√©sumer ma vie aujourd‚Äôhui avec vous, je dirais que c‚Äôest d‚Äôabord des rencontres, des gens qui m‚Äôont tendu la main‚Ä¶¬†‚Ü©Ô∏é ","date":"2022-10-15","objectID":"/2022/10/15/devoxx-be-22/:6:0","series":null,"tags":["conference","java"],"title":"Ma premi√®re participation √† Devoxx Belgium","uri":"/2022/10/15/devoxx-be-22/#conclusion"},{"categories":null,"content":"Dans mon dernier article, j‚Äôai tent√© de faire un √©tat des lieux des solutions possibles pour impl√©menter des batchs cloud natifs. J‚Äôai par la suite test√© plus en d√©tails les jobs et cron jobs Kubernetes en essayant d‚Äôavoir une vue OPS sur ce sujet. Le principal inconv√©nient (qui ne l‚Äôest pas dans certains cas) des jobs est qu‚Äôon ne peut pas les rejouer. Si ces derniers sont termin√©s avec succ√®s - Vous allez me dire, il faut bien les coder - mais qu‚Äôon souhaite les rejouer pour diverses raisons, on doit les supprimer et relancer. J‚Äôai vu plusieurs posts sur StackOverflow √† ce sujet, je n‚Äôai pas trouv√© de solutions satisfaisantes relatifs √† ce sujet. Attention, je ne dis pas que les jobs et cron jobs ne doivent pas √™tre utilis√©s. Loin de l√†. Je pense que si vous avez besoin d‚Äôun traitement sans cha√Ænage d‚Äôactions, sans rejeu, les jobs et cron jobs sont de bonnes options. Le monitoring et reporting des actions r√©alis√©es peut se faire par l‚Äôobservabilit√© mise en place dans votre cluster K8S. Apr√®s plusieurs recherches, je suis tomb√© sur Spring Data Flow. L‚Äôoffre de ce module de Spring Cloud va au del√† des batchs. Il permet notamment de g√©rer le streaming via une interface graphique ou via son API. Dans cet article, je vais impl√©menter un exemple et le d√©ployer dans Minikube. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:0:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#"},{"categories":null,"content":" 1 Installation et configuration de MinikubeL‚Äôinstallation de minikube est d√©crite sur le site officiel. Pour l‚Äôinstaller, j‚Äôai ex√©cut√© les commandes suivantes: curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube Au premier d√©marrage, vous finirez l‚Äôinstallation minikube start ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-et-configuration-de-minikube"},{"categories":null,"content":" 1.1 Installation de Spring Cloud Data FlowPour installer Spring Cloud Data Flow directement dans Kubernetes, vous pouvez ex√©cuter les commandes suivantes: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release bitnami/spring-cloud-dataflow Apr√®s quelques minutes de t√©l√©chargement, vous devriez avoir le retour suivante √† l‚Äôex√©cution de la commande kubectl get pods kubectl get pods ~ ¬ª kubectl get pods NAME READY STATUS RESTARTS AGE dataflow-mariadb-0 1/1 Running 1 (24h ago) 24h dataflow-rabbitmq-0 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-server-75db59d6cb-lrwp8 1/1 Running 1 (24h ago) 24h dataflow-spring-cloud-dataflow-skipper-9db568cf4-rzsqq 1/1 Running 1 (24h ago) 24h ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#installation-de-spring-cloud-data-flow"},{"categories":null,"content":" 1.2 Acc√®s au dashboardPour acc√©der au dashboard de Spring Cloud Data Flow, vous pouvez lancer les commandes suivantes: export SERVICE_PORT=$(kubectl get --namespace default -o jsonpath=\"{.spec.ports[0].port}\" services dataflow-spring-cloud-dataflow-server) kubectl port-forward --namespace default svc/dataflow-spring-cloud-dataflow-server ${SERVICE_PORT}:${SERVICE_PORT} Ensuite, vous pourrez acc√©der √† la console web via l‚ÄôURL http://localhost:8080/dashboard. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:1:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#acc√®s-au-dashboard"},{"categories":null,"content":" 2 D√©veloppement d‚Äôune TaskJ‚Äôai cr√©e une simple task qui va rechercher la nationalit√© d‚Äôun pr√©nom. Pour ceci, j‚Äôutilise l‚ÄôAPI https://api.nationalize.io/. On passe un pr√©nom en param√®tre et on obtient une liste de nationalit√©s possibles avec leurs probabilit√©s. Vous trouverez les sources de cet exemple sur mon Github. Aussi, la documentation est bien faite, il suffit de la lire. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#d√©veloppement-dune-task"},{"categories":null,"content":" 2.1 InitialisationJ‚Äôai initi√© un projet Spring avec les d√©pendances suivantes: dependencies { implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.springframework.cloud:spring-cloud-starter-task' developmentOnly 'org.springframework.boot:spring-boot-devtools' testImplementation 'org.springframework.boot:spring-boot-starter-test' implementation 'org.springframework.boot:spring-boot-starter-jdbc' runtimeOnly 'org.mariadb.jdbc:mariadb-java-client' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } Attention, les starters et d√©pendances JDBC/MariaDB sont indispensables pour que votre t√¢che puisse enregistrer le statut des ex√©cutions. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#initialisation"},{"categories":null,"content":" 2.2 Construction de la t√¢cheUne t√¢che se cr√©e facilement en annotation une classe ‚ÄúConfiguration‚Äù par l‚Äôannotation @EnableTask @Configuration @EnableTask public class TaskConfiguration { ... } Ensuite l‚Äôessentiel du job s‚Äôeffectue dans la construction d‚Äôun bean CommandLineRunner : @Bean public CommandLineRunner createCommandLineRunner(RestTemplate restTemplate) { return args -\u003e { var commandLinePropertySource = new SimpleCommandLinePropertySource(args); var entity = restTemplate.getForEntity(\"https://api.nationalize.io/?name=\" + Optional.ofNullable(commandLinePropertySource.getProperty(\"name\")).orElse(\"BLANK\"), NationalizeResponseDTO.class); LOGGER.info(\"RESPONSE[{}]: {}\", entity.getStatusCode(), entity.getBody()); }; } Dans mon exemple, j‚Äôaffiche dans la sortie standard le payload de l‚ÄôAPI ainsi que le code HTTP de la r√©ponse. Voici un exemple d‚Äôex√©cution : 2022-08-12 15:11:07.885 INFO 1 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2022-08-12 15:11:07.894 INFO 1 --- [ main] i.t.b.cloudtask.CloudTaskApplication : Started CloudTaskApplication in 17.704 seconds (JVM running for 19.18) 2022-08-12 15:11:10.722 INFO 1 --- [ main] i.t.batch.cloudtask.TaskConfiguration : RESPONSE[200 OK]: NationalizeResponseDTO{name='Alexandre', countries=[CountryDTO{countryId='BR', pr.... ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#construction-de-la-t√¢che"},{"categories":null,"content":" 2.3 PackagingIci rien de nouveau, il suffit de lancer la commande: ./gradlew build ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:2:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#packaging"},{"categories":null,"content":" 3 D√©ploiement","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#d√©ploiement"},{"categories":null,"content":" 3.1 Cr√©ation et d√©ploiement de l‚Äôimage DockerPour d√©ployer notre toute nouvelle t√¢che, nous allons d‚Äôabord cr√©er l‚Äôimage Docker avec buildpack. Tout d‚Äôabord on va se brancher sur minikube pour que notre image soit d√©ploy√©e dans le repository de minikube eval $(minikube docker-env) Ensuite, il nous reste √† cr√©er l‚Äôimage Docker ./gradlew bootBuildImage --imageName=info.touret/cloud-task:latest Pour v√©rifier que votre image est bien pr√©sente dans minikube, vous pouvez ex√©cuter la commande suivante: minikube image ls | grep cloud-task info.touret/cloud-task:latest ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:1","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#cr√©ation-et-d√©ploiement-de-limage-docker"},{"categories":null,"content":" 3.2 Cr√©ation de l‚ÄôapplicationAvant de cr√©er la t√¢che dans l‚Äôinterface, il faut d‚Äôabord r√©f√©rencer l‚Äôimage Docker en cr√©er une application: Il faut d√©clarer l‚Äôimage Docker avec le formalisme pr√©sent√© dans la capture d‚Äô√©cran. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:2","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#cr√©ation-de-lapplication"},{"categories":null,"content":" 3.3 Cr√©ation de la t√¢cheVoici les diff√©rentes actions que j‚Äôai r√©alis√© via l‚Äôinterface: Vous trouverez plus de d√©tails dans la documentation officielle. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:3:3","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#cr√©ation-de-la-t√¢che"},{"categories":null,"content":" 4 Ex√©cutionMaintenant, il nous est possible de lancer notre t√¢che. Vous trouverez dans les copies d‚Äô√©cran ci-dessous les diff√©rentes actions que j‚Äôai r√©alis√© pour ex√©cuter ma toute nouvelle t√¢che. J‚Äôai pu √©galement acc√©der aux logs. Il est √©galement important de noter qu‚Äô apr√®s l‚Äôex√©cution d‚Äôune t√¢che, le POD est toujours au statut RUNNING afin que Kubernetes ne red√©marre pas automatiquement le traitement. kubectl get pods | grep cloud-task cloud-task-7mp72gzpwo 1/1 Running 0 57m cloud-task-pymdkr182p 1/1 Running 0 65m A chaque ex√©cution il y aura donc un pod d‚Äôallou√©. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:4:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#ex√©cution"},{"categories":null,"content":" 5 Aller plus loinParmi les fonctionnalit√©s que j‚Äôai d√©couvert, on peut : relancer un traitement le programmer nettoyer les ex√©cutions les pistes d‚Äôaudit le cha√Ænage des diff√©rentes t√¢ches Gros inconv√©nient pour le nettoyage: je n‚Äôai pas constat√© un impact dans les pods allou√©s. ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:5:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#aller-plus-loin"},{"categories":null,"content":" 6 ConclusionPour r√©sumer, je vais me risquer √† comparer les deux solutions jobs/cron jobs Kubernetes et une solution bas√©e sur Spring Cloud Dataflow. Je vais donc utiliser la liste des caract√©ristiques pr√©sent√©e par M. Richards et N. Ford dans leur livre: Fundamentals of Software Architecture1. Bien √©videmment, cette notation est purement personnelle. Vous noterez que selon o√π on positionne le curseur, l‚Äôune des deux solutions peut s‚Äôav√©rer meilleure (ou pas). Bref, tout d√©pend de vos contraintes et de ce que vous souhaitez en faire. A mon avis, une solution telle que Spring Cloud Dataflow s‚Äôinscrit parfaitement pour des traitements mixtes (streaming, batch) et pour des traitements Big Data. N‚Äôh√©sitez pas √† me donner votre avis (sans troller svp) en commentaire ou si √ßa concerne l‚Äôexemple, directement dans Github. Architecture characteristic K8s job rating Spring Cloud Dataflow rating Partitioning type Domain \u0026 technical Domain \u0026 technical Number of quanta 2 1 1 to many Deployability ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Elasticity ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Evolutionary ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Fault Tolerance ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Modularity ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Overall cost ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Performance ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Reliability ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Scalability ‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê Simplicity ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê Testability ‚≠ê‚≠ê‚≠ê ‚≠ê‚≠ê‚≠ê‚≠ê A lire absolument!¬†‚Ü©Ô∏é ~ Nombre de livrables ind√©pendants fortement coupl√©s¬†‚Ü©Ô∏é ","date":"2022-08-16","objectID":"/2022/08/16/spring-data-flow/:6:0","series":null,"tags":["cloud","kubernetes","batch","spring"],"title":"D√©ployer des batchs cloud native avec Spring Cloud Data Flow","uri":"/2022/08/16/spring-data-flow/#conclusion"},{"categories":null,"content":" Talks You can find below the slides and videos of my latest talks in French and English. I held most of them in both two languages. You can also get my slides on SpeakerDeck and videos on this Youtube playlist: ","date":"2022-07-02","objectID":"/talks/:0:0","series":null,"tags":null,"title":"Talks","uri":"/talks/#"},{"categories":null,"content":" 1 TalksHere is a bunch of talks I presented at various tech events (meetups, conferences). You can find them either in French or English. ","date":"2022-07-02","objectID":"/talks/:1:0","series":null,"tags":null,"title":"Talks","uri":"/talks/#talks"},{"categories":null,"content":" 1.1 The Hitchhiker‚Äôs guide to software architecture design Abstract Designing a new platform is always tricky to set up. How to start? What is the best strategy to adopt while designing a platform? What kind of architecture should we deploy: event streaming, orchestration, or choreography? For a brand-new platform: ‚ÄúDonuts @ Home‚Äù, we will proceed a live architecture study. After analysing the customer needs, brainstorming, and exchanging our ideas, we will choose among all the potential solutions the least worst option. You will be asked to validate our design and the different implementation examples. At the end of this talk, you will have tips and tricks for thinking about it and starting working on architecture studies in complete peace of mind. Slides Video ","date":"2022-07-02","objectID":"/talks/:1:1","series":null,"tags":null,"title":"Talks","uri":"/talks/#the-hitchhikers-guide-to-software-architecture-design"},{"categories":null,"content":" 1.2 Le versioning des APIs REST: dans la vraie vie on fait comment? R√©sum√© Quand on souhaite publier des APIs avec par exemple, une solution d‚Äô API Management, on √©voque r√©guli√®rement le versioning. Cette pratique r√©pond √† des contraintes projet mais apporte malheureusement son lot de complexit√©. Imaginez, vous travaillez sur un produit qui expose des APIs √† plusieurs clients. Vous devez leur proposer des √©volutions et nouvelles fonctionnalit√©s tout en ma√Ætrisant l‚Äôexistant. Quelle strat√©gie adopter? Quelles solutions techniques peut-on mettre en place simplement ? Dans cette pr√©sentation, vous (re)d√©couvrirez des conseils que j‚Äôai pu mettre en pratique et qui m‚Äôont aid√© lors de mes derniers projets. Nous aborderons, au travers d‚Äôun cas concret la strat√©gie √† mettre en oeuvre, les diff√©rentes possibilit√©s d‚Äôimpl√©mentation ainsi que leurs contraintes. A l‚Äôissue de cette pr√©sentation, nous aurons une vue compl√®te et objective des mani√®res d‚Äôappr√©hender le versioning d‚ÄôAPIs. Slides Vid√©o ","date":"2022-07-02","objectID":"/talks/:1:2","series":null,"tags":null,"title":"Talks","uri":"/talks/#le-versioning-des-apis-rest-dans-la-vraie-vie-on-fait-comment"},{"categories":null,"content":" 1.3 Le versioning des APIs REST par la pratique (Workshop) R√©sum√© Quand on souhaite publier des APIs avec par exemple, une solution d‚Äô API Management, on √©voque r√©guli√®rement le versioning. Cette pratique r√©pond √† des contraintes projet mais apporte malheureusement son lot de complexit√©. Imaginez, vous travaillez sur un produit qui expose des APIs √† plusieurs clients. Vous devez leur proposer des √©volutions et nouvelles fonctionnalit√©s tout en ma√Ætrisant l‚Äôexistant. Comment faire √©voluer et proposer vos APIs √† certains clients sans p√©naliser les autres? Quelle strat√©gie adopter? Quelles solutions techniques peut-on mettre en place simplement? Lors de cet atelier, vous (re)d√©couvrirez et mettrez en pratique des conseils que j‚Äôai pu mettre en oeuvre et qui m‚Äôont aid√© lors de mes derniers projets. Au travers d‚Äôun cas concret bas√© sur une architecture microservices, nous d√©finirons la strat√©gie √† mettre en ≈ìuvre, les diff√©rentes possibilit√©s d‚Äôimpl√©mentation ainsi que leurs contraintes. Nous les challengerons ensuite en apportant diff√©rentes √©volutions (ajout d‚Äôun nouveau client ou de nouvelles fonctionnalit√©s). A l‚Äôissue de cet atelier, nous aurons une vue compl√®te et mis en pratique diff√©rentes mani√®res d‚Äôappr√©hender le versioning d‚ÄôAPIs. Slides Workshop ","date":"2022-07-02","objectID":"/talks/:1:3","series":null,"tags":null,"title":"Talks","uri":"/talks/#le-versioning-des-apis-rest-par-la-pratique-workshop"},{"categories":null,"content":" 1.4 Architecture Katas : Improve your system architecture design skills in a funny way Abstract How to learn architecture ? How to improve in this field ? How do we recognize a good or a bad architecture ? Plenty of books and training sessions address this subject. The best thing is to practice! In the same way as CodingDojos, I will present to you architecture katas. Ted NEWARD created them. His idea came from the following observation : ‚ÄúHow are we supposed to get great architects, if they only get the chance to architect fewer than a half-dozen times in their career?‚Äù One solution to this issue could be to practice regularly on several topics to gain experience. After a brief introduction of how to start designing an architecture, I will present architecture katas, and the conduct. To conclude, after this first try I will present the benefits I had benefited. Slides Video ","date":"2022-07-02","objectID":"/talks/:1:4","series":null,"tags":null,"title":"Talks","uri":"/talks/#architecture-katas--improve-your-system-architecture-design-skills-in-a-funny-way"},{"categories":null,"content":" 1.5 Java dans le cloud: avec Spring ou Quarkus? R√©sum√© N√© au d√©but des ann√©es 2000, le framework Spring a mis en avant la facilit√© de d√©veloppement. Plus r√©cemment, il a su s‚Äôadapter aux contraintes techniques li√©es aux applications cloud-natives. De son cot√©, Quarkus est plus r√©cent, il est n√© en 2019 sur la base de MicroProfile avec un objectif clair : tirer le meilleur parti des plateformes Kubernetes en se concentrant sur un d√©marrage rapide et une faible empreinte m√©moire. Nous avons donc un champion et un outsider ! Maintenant lequel choisir en fonction de vos besoins et votre contexte ? Spring ou Quarkus ? Au travers d‚Äôune d√©monstration ‚Äúlive‚Äù nous pr√©senterons un cas contret bas√© sur notre exp√©rience. Ce dernier sera impl√©ment√© avec chacune des deux stacks. Enfin, nous vous mettrons √† contribution au travers de sondages pour dynamiser ensemble notre r√©flexion. Slides Vid√©o ","date":"2022-07-02","objectID":"/talks/:1:5","series":null,"tags":null,"title":"Talks","uri":"/talks/#java-dans-le-cloud-avec-spring-ou-quarkus"},{"categories":null,"content":" 1.6 Kubernetes \u0026 Co, beyond the hype: 10 tips for designers who want to create cloud native apps Abstract Kubernetes and cloud technologies are nowadays the new standard to deploy different cloud native applications: API, BATCHES, microservices and ‚Ä¶ monoliths! These technologies help to solve many issues but with some complexity. It could be difficult for developers and designers to identify the constraints of such architectures. In this presentation, you will (re)discover ten tips and pieces of advice I applied and found useful in my last JAVA projects (Spring, JEE). I will talk about: Application ecosystem Choice of technical solutions Development K8S design constraints And more! Slides Video ","date":"2022-07-02","objectID":"/talks/:1:6","series":null,"tags":null,"title":"Talks","uri":"/talks/#kubernetes--co-beyond-the-hype-10-tips-for-designers-who-want-to-create-cloud-native-apps"},{"categories":null,"content":"Au d√©but de l‚Äôann√©e, j‚Äôai interpel√© mes managers: ‚ÄúJ‚Äôai √©t√© retenu au Camping des Speakers‚Äù! Unanimement, j‚Äôai eu la m√™me r√©ponse: ‚ÄúLe QUOI ???!!!‚Äù Oui j‚Äôen conviens le titre peut para√Ætre √† premi√®re vue tout sauf s√©rieux. Il faut dire que le lieu est un camping en Bretagne dans le golfe du Morbihan. Et pourtant‚ÄØ! La programmation √©tait de qualit√© et a tenu ses promesses. Je vais essayer de retranscrire quelques sujets qui ont retenu mon attention. J‚Äôen oublierai sans doute beaucoup. Je m‚Äôen excuse d‚Äôavance. Tout d‚Äôabord, l‚Äôint√©r√™t de cette conf√©rence r√©side, selon moi, au-del√† des talks donn√©s. Le cadre atypique (un camping dans le Morbihan), des conf√©rences slideless donn√©es en ext√©rieur en mode ‚Äúfeu de camp‚Äù a eu le m√™me effet, enfin il me semble, sur tous les participantes et participants qu‚Äôils soient speakers ou non. Chose originale que j‚Äôai pu constater lors de ces talks, on passait beaucoup de temps √† √©changer avec les speakers pendant leur pr√©sentation. C‚Äô√©tait g√©nial pour les spectateurs, peut-√™tre un peu moins pour la gestion du timing :). Vous l‚Äôaurez compris. L‚Äôenvironnement mis en place par les organisateurs permettait de r√©els √©changes‚Ä¶ Et on avait le temps de le faire. Ensuite, la programmation donnait la part belle √† des sujets non techniques. Par exemple, j‚Äôai enfin pu voir la conf√©rence de mon ancienne coll√®gue Fanny Klauk : ‚ÄúRendez l‚Äôagilit√© aux d√©veloppeur(se)s !‚Äù qui, au travers d‚Äôune histoire dont est le h√©ros, a mis en √©vidence les travers des (grosses) organisations. Les solutions sont ‚Äúnaturelles‚Äù dans la th√©orie, mais permettent de donner plus d‚Äôagilit√© et de sens aux d√©veloppeurs. Un autre sujet ‚Äúnon technique‚Äù m‚Äôa tr√®s int√©ress√©, c‚Äô√©tait ‚ÄúLe voyage du h√©ros de l‚ÄôIT‚Äù d‚Äô Olivier Beautier. Apr√®s nous avoir pr√©sent√© le guide du sc√©nariste de C. VOGLER, on a construit une histoire d‚Äôune d√©veloppeuse. Au travers de cette histoire, on a pu voir entre autres l‚Äôimportance du mentorat dans notre m√©tier. Puis, j‚Äôai vu la conf√©rence d‚ÄôA. PENA ‚ÄúJava √† la vitesse de la lumi√®re gr√¢ce au Graal !‚Äù. Il a pr√©sent√© GraalVM, une comparaison avec HotSpot et les perspectives d‚Äô√©volution de cette JVM. Nul doute que le mode natif sera de plus en plus utilis√© dans les prochaines ann√©es. Ensuite, avec D. APARICIO, nous avons pu voir les travers d‚Äôune (mauvaise) mod√©lisation de donn√©es au travers de son retour d‚Äôexp√©rience ‚ÄúAu secours üò®! J‚Äôai un homonyme !! ‚Äú Il a illustr√© l‚Äôimportance de bien mod√©liser pour ne pas avoir que le nom et pr√©nom comme identifiants dans un syst√®me par ex. ainsi que les impacts dans la vie courante. Vous l‚Äôaurez compris, il y avait beaucoup de conf√©rences int√©ressantes qui donnaient lieu √† des discussions tout aussi passionnantes. Last but not least, j‚Äôai eu la chance de pr√©senter les katas d‚Äôarchitecture, comment les mettre en ≈ìuvre et quels sont les b√©n√©fices qu‚Äôon peut en tirer. Je tiens √† remercier Julien TOPCU et Craft Records pour l‚Äôaccompagnement dans la pr√©paration. Vous pourrez trouver les slides sur ma page speackerdeck. Pour finir, je conseille cette exp√©rience √† toutes les personnes qui veulent apprendre, √©changer dans un environnement convivial et zen. Si vous n‚Äô√™tes toujours pas convaincu, sachez qu‚Äôil y a √©galement des mashmallows. Avec cet argument massue, on devrait convaincre les plus sceptiques! ;) Camping des speakers spirit Merci encore aux organisatrices et organisateurs pour ces deux jours. √Ä l‚Äôann√©e prochaine !! ","date":"2022-06-10","objectID":"/2022/06/10/camping-speakers-2022/:0:0","series":null,"tags":["conference"],"title":"Mon Camping des Speakers","uri":"/2022/06/10/camping-speakers-2022/#"},{"categories":null,"content":"Quand on parle du Cloud et de Kubernetes, g√©n√©ralement on pense aux APIs. Mais qu‚Äôen est-il des batchs? Oui, depuis plusieurs ann√©es, on pensait les √©radiquer, mais ils sont encore l√† et on en a encore besoin pour quelques ann√©es encore. Ils ont m√™me eu une deuxi√®me jeunesse avec le Big Data et l‚Äôexplosion des volum√©tries dans l‚ÄôIT. Je vais essayer de faire un tour d‚Äôhorizon dans cet article des batchs dans un environnement Cloud et plus particuli√®rement dans Kubernetes. Les exemples pr√©sent√©s dans cet article seront (sans doute) approfondis dans un second article et d‚Äôores et d√©j√† disponibles dans mon GitHub. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:0:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#"},{"categories":null,"content":" 1 Pourquoi des batchs dans le Cloud?A ce titre un peu provocateur, j‚Äôajouterais aussi ‚ÄúPourquoi des batchs dans Kubernetes ?‚Äù. Oui, aujourd‚Äôhui encore,comme j‚Äôai pu l‚Äôindiquer pr√©c√©demment, on doit cr√©er des traitements batchs. A cot√© des APIs qui repr√©sentent le cas d‚Äôutilisation ‚Äústandard‚Äù du Cloud, on peut √©galement avoir √† traiter des fichiers volumineux allant de plusieurs centaines de Mo √† quelques Go. Parmi les cas d‚Äôutilisation qui n√©cessitent ce genre de traitement, on pourra avoir: Les reprises de donn√©es (suite √† des erreurs ou lors d‚Äôune initialisation) Traitement suite √† une r√©ception de fichiers (par ex. traitement de fichiers OPENDATA) Si vous √™tes d√©j√† pass√© sur le Cloud pour vos applications transactionnelles, vous vous poserez cette question: Puis-je √©galement d√©ployer des batchs? ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-des-batchs-dans-le-cloud"},{"categories":null,"content":" 1.1 Pourquoi se poser cette question?Les r√©ponses sont multiples. Elles sont tout d‚Äôabord li√©es √† une rationalisation des environnements. Vous avez votre application dans le cloud, votre base de donn√©es y est √©galement g√©r√©e pour √©viter la latence r√©seau. Vous devez donc d√©ployer des traitements tiers au plus proche de celle-ci pour vous soustraire des m√™mes soucis. De plus, l‚Äô√©cosyst√®me li√© au cloud offre des technologies et pratiques qui rendent la vie plus simple (si, si, je vous assure) aux d√©veloppeurs et ops. Le d√©ploiement via l‚ÄôInfra As Code est un bon exemple : Avoir toute l‚Äôinfrastructure li√©e aux traitements batchs et transactionnels versionn√©es et instantiables √† la demande est quelque chose dont on a du mal √† se passer! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:1:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#pourquoi-se-poser-cette-question"},{"categories":null,"content":" 2 Difficult√©(s) par rapport aux APIsQuand on d√©ploie une API dans le cloud, g√©n√©ralement tout va bien. On peut voir rapidement que cet environnement convient bien √† ce genre de traitements. Pour les batchs, c‚Äôest une autre affaire! Selon les soci√©t√©s, il peut y avoir un fort historique et beaucoup plus d‚Äôexigences que pour les APIs. Ces derni√®res pourront √™tre li√©es aux performances, √† la qualit√© de service ou plus simplement √† l‚Äôutilisation. Il faut donc, √† l‚Äôinstar de toute architecture, d√©terminer quel sera l‚Äôenvironnement technique de ce type de traitement. Cette fois, on aura √† concilier performances, fichiers volumineux et reprises sur erreur. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#difficult√©s-par-rapport-aux-apis"},{"categories":null,"content":" 2.1 Quelques technologiesOn pourra retrouver dans notre future architecture les briques suivantes: Une passerelle de fichiers (File Gateway) pour permettre l‚Äôenvoi des fichiers de mani√®re s√©curis√©e Un stockage objet pour la distribution de fichiers ou l‚Äôarchivage. Les √©l√©ments n√©cessaires √† l‚ÄôAPI : bases de donn√©es, HSMs, Cluster Kubernetes,‚Ä¶ ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:2:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#quelques-technologies"},{"categories":null,"content":" 3 Modes de d√©clenchementSi on regarde de plus pr√®s les exigences techniques li√©es aux cas d‚Äôutilisation, on pourrait r√©sumer les diff√©rents modes de d√©clenchement de la mani√®re suivante: Traitement sur r√©ception de fichiers Traitement d√©clench√© par un ordonnanceur/orchestrateur centralis√© (ex. https://dkron.io/) de mani√®re r√©guli√®re ou non. Traitement d√©clench√© par CRON (qui est un ordonnanceur, mais un peu plus roots) J‚Äôai volontairement exclu les traitements sur pr√©sence de messages (ex. Kafka). Je les consid√®re plus li√©s au monde transactionnel. Dans les paragraphes suivants, je vais d√©crire des solutions d‚Äôarchitecture qui permettent de d√©ployer ces traitements dans Kubernetes. J‚Äôaborderai sans doute un exemple dans un autre article ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:3:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#modes-de-d√©clenchement"},{"categories":null,"content":" 4 ContraintesD√®s qu‚Äôon s‚Äôaventure dans ce type de conception, nous aurons, au-del√† des 12 factors, les contraintes suivantes √† traiter: ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#contraintes"},{"categories":null,"content":" 4.1 Gestion des erreurs et indisponibilit√©sDans un cluster Kubernetes, le crash d‚Äôun POD n‚Äôest pas r√©dhibitoire. Le cluster permet de red√©marrer imm√©diatement une autre instance. Pour les APIs, ce n‚Äôest pas un probl√®me. Pour les batchs, c‚Äôest une autre paire de manches. Quid du crash en plein milieu du traitement d‚Äôun fichier? Il faut donc penser √† ce cas (et √† d‚Äôautres) et archiver les fichiers pour un √©ventuel rejeu. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-erreurs-et-indisponibilit√©s"},{"categories":null,"content":" 4.2 Donn√©es et idempotence des traitementsId√©alement, les fichiers doivent avoir des lignes ind√©pendantes qui peuvent √™tre ins√©r√©es individuellement et dans n‚Äôimporte quel ordre. Aussi, chaque modification et traitement de donn√©es doivent √™tre idempotentes. Pourquoi? Pas seulement par ce que c‚Äôest sympa et l‚Äô√©tat de l‚Äôart, mais dans ce nouvel environnement, vous ne pourrez pas forc√©ment garantir l‚Äôordre des traitements. L‚Äôune des solutions potentielles de traitement est de d√©coupler la lecture et l‚Äôinsertion par du queueing (Artemis, Kafka - oui ce n‚Äôest pas du queuing, mais vous avez compris‚Ä¶). Dans ce cas, si votre traitement n‚Äôest pas idempotent, vous devrez lutter avec des doublons en base. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#donn√©es-et-idempotence-des-traitements"},{"categories":null,"content":" 4.3 Gestion des ressourcesImaginez, vous recevez un fichier de 1Go. Vos ressources syst√®mes sont des PODs avec un 1 Go de RAM. Vous voyez le soucis? Cet exemple, qui n‚Äôest pas trop √©loign√© de la r√©alit√©, mets en √©vidence l‚Äôune des contraintes techniques que vous devrez prendre d√®s le d√©but de votre conception. L‚Äôune des solutions serait, par exemple, le traitement quasi syst√©matique du streaming de fichiers et l‚Äôobligation d‚Äôavoir des fichiers avec des lignes de donn√©es ind√©pendantes (c.-√†-d. sans avoir √† faire de liens inter lignes pendant le traitement). ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:4:3","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#gestion-des-ressources"},{"categories":null,"content":" 5 Traitement sur r√©ception de fichiersDans ce cas, nous avons un processus qui est d√©clench√© lors de la r√©ception d‚Äôun fichier. Nous pourrons par exemple avec ce genre d‚Äôarchitecture un fichier qui est envoy√© dans espace de stockage objet. Ce dernier est ensuite trait√© par un programme. J‚Äôai fait le choix ici de mettre en oeuvre un couplage l√¢che (on ne se refait pas) entre l‚Äôespace de r√©ception de fichiers et le traitement. Je traite ici le risque de crash d‚Äôun POD en gardant syst√©matiquement les fichiers dans un stockage objet. De cette mani√®re, si le traitement a √©chou√©, un autre POD pourra le t√©l√©charger et rejouer le processus batch. Ce d√©couplage permet de g√©rer facilement la scalabilit√© et les arr√™ts/relances de PODs. Batch d√©marr√© sur pr√©sence de fichier Dans ce cas, le batch pourra √™tre d√©ploy√© sous la forme d‚Äôun d√©ploiement Kubernetes. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:5:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-sur-r√©ception-de-fichiers"},{"categories":null,"content":" 6 Traitement d√©clench√© √† distance (par ex. par un orchestrateur de traitements)Maintenant, on va aborder les traitements qui sont lanc√©s par un ordonnanceur tiers ou tout simplement lanc√© √† distance. G√©n√©ralement, dans le monde de l‚Äôentreprise, la planification des traitements est centralis√©e au lieu de laisser de le faire sur chaque machine avec des CRON Jobs. Dans ce cas, on a deux mani√®res de proc√©der: Avoir un traitement qui fournit une API permettant de d√©marrer des traitements et d‚Äôavoir leurs statuts. Lancer des jobs. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-d√©clench√©-√†-distance-par-ex-par-un-orchestrateur-de-traitements"},{"categories":null,"content":" 6.1 Avec une APIIci, on con√ßoit les batchs comme des WEBAPPS qui fournissent des traitements batchs sur demande via des APIs. La contrainte est qu‚Äô√† l‚Äôinstar de la solution pr√©c√©dente, le programme tourne toujours et n‚Äôest vraiment utile que lorsqu‚Äôil est appel√© via un endpoint REST. Ce mod√®le de conception peut √™tre utilis√© √† mon avis si la fr√©quence est forte et si l‚Äôint√©gration d‚Äôun Job Kubernetes est probl√©matique pour vous (voir ci-dessous). L‚Äôun des avantages que l‚Äôon pourra trouver est que le mode de d√©ploiement est assez simple et similaire aux APIs. Batch d√©marr√© par une API ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:1","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-une-api"},{"categories":null,"content":" 6.2 Avec des jobsSi votre ordonnanceur peut ex√©cuter le client kubectl, vous pourrez consid√©rer les jobs kubernetes. En r√©sum√©, ils permettent de cr√©er un POD et ex√©cute une action en g√©rant les erreurs potentielles jusqu‚Äô√† compl√©tion du traitement. Par exemple, voici un job permettant de faire un ‚ÄúHello World!‚Äù: apiVersion: batch/v1 kind: Job metadata: name: hello-world spec: template: spec: containers: - name: helloworld image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: Never backoffLimit: 4 Une fois d√©ploy√© avec Helm, vous pouvez les voir avec la commande kubectl get jobs minikube kubectl -- get jobs NAME COMPLETIONS DURATION AGE hello-world 0/1 25s 25s Pour les logs et voir le r√©sultat de la commande lanc√©, cela se passe d‚Äôune mani√®re assez habituelle: minikube kubectl -- logs hello-world-zx4wh Hello World! ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:6:2","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#avec-des-jobs"},{"categories":null,"content":" 7 Traitement d√©clench√© par CRONMaintenant, on va laisser le soin au Cluster Kubernetes de lancer les diff√©rents traitements via une CRON. Bien que je ne suis pas trop fan de ne pas centraliser l‚Äôordonnancement, cela peut √™tre tr√®s utile si votre plateforme est centr√©e sur Kubernetes. Si vous √™tes dans ce cas-l√†, vous pouvez utiliser l‚Äôobjet CronJob qui n‚Äôest ni plus ni moins qu‚Äôun Job ex√©cut√© de mani√®re p√©riodique. apiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \"* * * * *\" jobTemplate: spec: template: spec: containers: - name: helloworld-cron image: busybox command: [\"echo\", \"Hello World!\"] restartPolicy: OnFailure ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:7:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#traitement-d√©clench√©-par-cron"},{"categories":null,"content":" 8 Panorama des solutions logicielles possiblesUne fois qu‚Äôon s‚Äôest pos√© toutes (en tout cas certaines) les questions possibles sur nos exigences techniques et la conception, on peut voir quelles sont les technologies possibles pour impl√©menter des batchs ‚Äúcloud natifs‚Äù. √áa ne sera pas une surprise, je vais m‚Äôattarder √† la plateforme Java. Il est bien √©videmment possible d‚Äôutiliser d‚Äôautres langages et frameworks tels que Go. En Java, vous avez le choix entre diff√©rents frameworks : [Spring avec spring batch]([Spring avec spring batch et/ou integration ) et/ou integration Camel qui peut √™tre utilis√© avec Spring ou Quarkus Quarkus avec la JSR 352 Si vous allez du c√¥t√© du BigData, vous pouvez aussi envisager d‚Äôutiliser des technologies telles qu‚ÄôApache Spark. Ces derni√®res vous permettront de d√©couper ‚Äúplus facilement‚Äù vos traitements. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:8:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#panorama-des-solutions-logicielles-possibles"},{"categories":null,"content":" 9 Le diable se cache dans les d√©tailsD√©ployer un batch dans Kubernetes peut se faire assez facilement (en d√©veloppement) une fois qu‚Äôon a compris quelques principes. Cependant, les soucis peuvent survenir une fois arriv√© en production. La gestion des erreurs est beaucoup plus complexe que les APIs. Il vous faudra donc d√©finir avec les diff√©rentes parties prenantes quel est le meilleur fonctionnement (rejeu) en production. Il vous faudra ainsi bien identifier et √©valuer les risques li√©s √† votre application et voir quelles sont les actions √† mener. Aussi, si vous devez manipuler des fichiers volumineux, il faudra faire attention au syst√®me de fichiers utilis√© et ses performances. Habituellement, avec ce type d‚Äôarchitecture, on utilise g√©n√©ralement du SAN. En fonction de vos exigences, un stockage block pourra √™tre plus adapt√©. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:9:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#le-diable-se-cache-dans-les-d√©tails"},{"categories":null,"content":" 10 ConclusionPour conclure cet article, vous aurez compris que le sujet des batchs dans Kubernetes peut s‚Äôav√©rer assez complexe √† g√©rer. Au-del√† des technologies qui peuvent faire le job (d√©sol√© du mauvais jeu de mots), il vous faudra faire tr√®s attention √† tout l‚Äôenvironnement dans lequel votre programme devra interagir. Les bases, le r√©seau, les performances de votre mat√©riel seront des pr√©requis indispensables. Aussi, il vous faudra faire attention √† la mani√®re dont sont transmises les donn√©es et dont vous les traitez. Bref, il faut √©tudier la solution dans son ensemble du d√©veloppement √† l‚Äôexploitation pour s‚Äôassurer de ne rien oublier. Enfin, cet article n‚Äôest bien √©videmment pas exhaustif que cela soit sur les solutions ou les contraintes √† adresser. J‚Äôai n√©anmoins essay√© d‚Äôapporter quelques cas concrets et retours d‚Äôexp√©rience. J‚Äôessaierai de d√©tailler un cas concret dans un prochain article. ","date":"2022-05-17","objectID":"/2022/05/17/cloud-native-batchs/:10:0","series":null,"tags":["cloud","kubernetes","batch"],"title":"Faire des batchs \"Cloud Native\" dans Kubernetes","uri":"/2022/05/17/cloud-native-batchs/#conclusion"},{"categories":null,"content":" 1 L‚Äôanalyse des risques: kezako ?Souvent utilis√©e dans la prise de d√©cision, l‚Äôanalyse des risques a plusieurs objectifs : Permettre de pond√©rer des risques potentiels Faciliter la prise de d√©cision sur les actions √† r√©aliser pour les pr√©venir ou tout du moins les att√©nuer. Mais d‚Äôabord, revenons aux bases. Comment identifier un risque ? Selon Wikipedia, voici la d√©finition: Le risque est la possibilit√© de survenue d‚Äôun √©v√©nement ind√©sirable, la probabilit√© d‚Äôoccurrence d‚Äôun p√©ril probable ou d‚Äôun al√©a. Bien √©videmment, on a les risques inconnus et ceux qui sont connus. Le pr√©alable √† toute gestion de risque (tout du moins pour la d√©finition d‚Äôarchitecture) est de capitaliser les connaissances et retours d‚Äôexp√©rience qui viennent du terrain. On va donc oublier les risques inconnus dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#lanalyse-des-risques-kezako-"},{"categories":null,"content":" 1.1 Comment les d√©finir ?Tout d‚Äôabord, il faut conna√Ætre les SLOs de la plateforme qu‚Äôon souhaite concevoir. Pourquoi ? Pour v√©rifier si les risques qu‚Äôon identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne √©lectrique sera faiblement impactante pour une application avec une disponibilit√© \u003c 70%. La r√©alisation des SLIs et SLOs est un pr√©alable pour d√©finir le ‚Äúbudget d‚Äôerreur‚Äù. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les att√©nuer. Ensuite, pour chaque risque qu‚Äôon identifiera (souvent √† partir de notre exp√©rience), on t√¢chera de d√©finir les caract√©ristiques suivantes: Cause Probabilit√© Cons√©quence (gravit√©) 1.1.1 Un exempleLa base de donn√©es est indisponible Cause: Syst√®me de fichier plein Probabilit√©: faible Cons√©quence: forte (toute la plateforme est HS) Pour d√©terminer la cause, il y a plusieurs m√©thodes, l‚Äôune des plus c√©l√®bres est celle des cinq pourquoi. Elle permet d‚Äôacc√©der √† la cause du probl√®me. Pour √©tablir la probabilit√©, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour d√©terminer quels ont √©t√© les diff√©rents incidents. Pour chacun, vous devrez d√©finir ces trois caract√©ristiques : cause, probabilit√©, cons√©quence. A cot√© de √ßa, vous aurez √† identifier si possible le temps d‚Äôindisponibilit√© du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#comment-les-d√©finir-"},{"categories":null,"content":" 1.1 Comment les d√©finir ?Tout d‚Äôabord, il faut conna√Ætre les SLOs de la plateforme qu‚Äôon souhaite concevoir. Pourquoi ? Pour v√©rifier si les risques qu‚Äôon identifiera plus tard sont pertinents ou tout du moins impactants. Par exemple: Une panne √©lectrique sera faiblement impactante pour une application avec une disponibilit√© \u003c 70%. La r√©alisation des SLIs et SLOs est un pr√©alable pour d√©finir le ‚Äúbudget d‚Äôerreur‚Äù. Ce dernier nous permettra in fine de quantifier les risques et de voir si il faut les att√©nuer. Ensuite, pour chaque risque qu‚Äôon identifiera (souvent √† partir de notre exp√©rience), on t√¢chera de d√©finir les caract√©ristiques suivantes: Cause Probabilit√© Cons√©quence (gravit√©) 1.1.1 Un exempleLa base de donn√©es est indisponible Cause: Syst√®me de fichier plein Probabilit√©: faible Cons√©quence: forte (toute la plateforme est HS) Pour d√©terminer la cause, il y a plusieurs m√©thodes, l‚Äôune des plus c√©l√®bres est celle des cinq pourquoi. Elle permet d‚Äôacc√©der √† la cause du probl√®me. Pour √©tablir la probabilit√©, les OPS seront vos meilleurs ami.e.s. Vous remontrez dans le temps pour d√©terminer quels ont √©t√© les diff√©rents incidents. Pour chacun, vous devrez d√©finir ces trois caract√©ristiques : cause, probabilit√©, cons√©quence. A cot√© de √ßa, vous aurez √† identifier si possible le temps d‚Äôindisponibilit√© du service. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple"},{"categories":null,"content":" 1.2 Synth√®seUne fois ce travail de fourmi r√©alis√©, vous pourrez le synth√©tiser dans un premier temps avec ce formalisme souvent repris dans la gestion de projet: En r√©sum√©, les actions qui sont oranges ou rouges doivent √™tre trait√©es et avoir un plan d‚Äôaction. Prenons la d√©finition d‚Äôune plateforme: Si votre API doit traiter de mani√®re r√©guli√®re des PAYLOADs volumineux (bon d√©j√†, la ce n‚Äôest pas top). Le temps de traitement peut √™tre tr√®s long et bloquer certaines ressources (ex. des sous transactions). Dans ce cas, la probabilit√© sera √† probable et l‚Äôimpact sera mod√©r√© ou majeur. Par cons√©quent, vous devrez le prendre en compte avec par exemple un circuit breaker. Pour aller encore plus loin, vous pouvez √©galement √©valuer les risques en fonction de votre budget d‚Äôerreur: Est-ce que l‚Äôerreur peut rentrer dans mon budget ou pas? Bref, est-ce acceptable? Si vous allez plus loin, je vous conseille la formation Coursera Site Reliability Engineering: Measuring and Managing Reliability. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:1:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#synth√®se"},{"categories":null,"content":" 2 OK, j‚Äôai identifi√© les risques potentiels. Qu‚Äôest-ce que j‚Äôen fait maintenant?C‚Äôest l√† que d√©marre r√©ellement le travail d‚Äôarchitecture: vous devrez √©valuer chaque risque en fonction des exigences fonctionnelles et technique pour savoir si elles valent la peine d‚Äô√™tre prises en consid√©ration. Si vous avez des risques de faible impact, vous pourrez soit les ‚Äúmettre sous contr√¥le‚Äù pour traiter d‚Äôautres probl√®mes, soit les traiter car ils sont ‚Äúfaciles‚Äù √† traiter et vous permettront d‚Äôagrandir votre ‚Äúbudget d‚Äôerreur‚Äù. Dans ce dernier cas, vous aurez la possibilit√© de laisser ‚Äúde cot√©‚Äù d‚Äôautres erreurs plus compliqu√©es √† traiter car elles rentreront dans votre budget. Bref, c‚Äôest un vrai travail de fourmi qui se base sur l‚Äôexp√©rience du terrain. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:2:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#ok-jai-identifi√©-les-risques-potentiels-quest-ce-que-jen-fait-maintenant"},{"categories":null,"content":" 3 Quid de l‚Äôarchitecture?Si vous avez √©valu√© les risques correctement, vous pourrez ne traiter que les risques qui en valent la peine et par cons√©quent n‚Äô ajouter de la complexit√© que l√† ou √ßa en vaut la peine! Oui, ce travail pr√©alable permet de faire simple! ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:3:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#_quid_-de-larchitecture"},{"categories":null,"content":" 4 Un exemple concretSi je reprends l‚Äôapplication bookstore que j‚Äôai d√©crite dans un pr√©c√©dent article: ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#un-exemple-concret"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L‚ÄôAPI Bookstore doit √™tre disponible 99% Nombre de r√©ponses HTTP = 2XX ou 4XX L‚ÄôAPI Bookstore doit r√©pondre en moins de 1 sec Temps de r√©ponse de l‚ÄôAPI 4.1.2 Volum√©trie 100 transactions par seconde (TPS) 100 utilisateurs simultan√©s ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#exigences-techniques"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L‚ÄôAPI Bookstore doit √™tre disponible 99% Nombre de r√©ponses HTTP = 2XX ou 4XX L‚ÄôAPI Bookstore doit r√©pondre en moins de 1 sec Temps de r√©ponse de l‚ÄôAPI 4.1.2 Volum√©trie 100 transactions par seconde (TPS) 100 utilisateurs simultan√©s ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#slos--slis"},{"categories":null,"content":" 4.1 Exigences techniques 4.1.1 SLOs / SLIsPour cet exemple, je ne vais traiter que deux exigences techniques: SLO SLI L‚ÄôAPI Bookstore doit √™tre disponible 99% Nombre de r√©ponses HTTP = 2XX ou 4XX L‚ÄôAPI Bookstore doit r√©pondre en moins de 1 sec Temps de r√©ponse de l‚ÄôAPI 4.1.2 Volum√©trie 100 transactions par seconde (TPS) 100 utilisateurs simultan√©s ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:1","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#volum√©trie"},{"categories":null,"content":" 4.2 Risques identifi√©sSans aller dans le d√©tail, voici quelques risques que l‚Äôon peut identifier de prime abord dans cette architecture: Indisponibilit√© du service bookstore Indisponibilit√© du service booknumber Indisponibilit√© de la base de donn√©es √† cause d‚Äôune forte volum√©trie En vous basant sur l‚Äôexp√©rience de vos OPS, vous pourrez √©galement ajouter des risques li√©s √† l‚Äôinfrastructure (routeurs, DNS, ‚Ä¶). Je ne vais pas les aborder dans cet article. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:2","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#risques-identifi√©s"},{"categories":null,"content":" 4.3 Qualification des risquesVoici une rapide qualification: Risques Probabilit√© Impact Indisponibilit√© du service bookstore Probable Majeur Indisponibilit√© du service booknumber Probable Majeur Indisponibilit√© de la base de donn√©es Possible Catastrophique Si on se r√©f√®re au premier diagramme, il est obligatoire de les prendre en compte. ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:3","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#qualification-des-risques"},{"categories":null,"content":" 4.4 Solutions d‚Äôarchitecture pour leur prise en compteUne fois les risques identifi√©s, on peut tout d‚Äôabord les confronter √† notre budget d‚Äôerreur pour valider leur prise en compte dans notre conception. Dans notre cas, on va prendre le postulat qu‚Äôil faut r√©ellement les prendre en consid√©ration et trouver une solution adapt√©e. Voici des exemples de solutions qui permettraient de faire descendre leur impact our leur probabilit√©. Risques Probabilit√© Impact Action/Solutions possibles Indisponibilit√© du service bookstore Possible Majeur Load balancing avec deux instances, Utilisation Kubernetes,. Indisponibilit√© du service booknumber Possible Majeur Sur le service book-number: Load balancing avec deux instances, Utilisation Kubernetes,.. Sur le service bookstore: Mettre en place un circuit breaker bas√© sur le timeout d‚Äôappel vers le service book-number pour garantir la SLO Indisponibilit√© de la base de donn√©es Possible Catastrophique - R√©alisation d‚Äôun benchmark pour s‚Äôassurer qu‚Äôune instance est suffisante. - Sinon mise en place m√©canisme HA ou changement de technologie ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:4:4","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#solutions-darchitecture-pour-leur-prise-en-compte"},{"categories":null,"content":" 5 ConclusionL‚Äôanalyse des risques n‚Äôest pas r√©cente et n‚Äôa pas √©t√© invent√©e par le monde de l‚Äôinformatique. Elle est d‚Äôabord apparue dans la gestion de projets et fait d√©sormais partie prenante de la d√©finition d‚Äôarchitectures (enfin √ßa commence‚Ä¶). Il ne faut pas la voir seulement pour un outil de ‚ÄúGO-NO GO‚Äù de r√©union de cellule de crise mais comme une aide √† la d√©cision pour la conception des syst√®mes. Il a toute sa place √† cot√© des diff√©rentes caract√©ristiques que vous devrez prendre en compte ( s√©curit√©, modularit√©, ‚Ä¶). J‚Äôai essay√© de d√©crire comment les identifier et trouver une solution adapt√©e dans l‚Äôexemple. Bien √©videmment, il n‚Äôest pas complet. Je pense n√©anmoins qu‚Äôil permet d‚Äôavoir une id√©e sur le sujet. Le principal avantage d‚Äôutiliser √† la fois les SLOs/SLIs, le budget d‚Äôerreur et l‚Äôanalyse des risques est de n‚Äôapporter de la complexit√© que l√† o√π c‚Äôest n√©cessaire. Pour certains un benchmark sera souvent utile pour confirmer votre d√©cision. Si vous voulez aller plus loin, je vous conseille dans un premier temps de lire ‚ÄúFundamentals of Software Architecture‚Äù. Ce sujet y est abord√©. Enfin,si ce sujet vous int√©resse, vous pouvez vous projeter au d√©l√† de l‚Äôinformatique en lisant les analyses de risques r√©alis√©es par le Minist√®re des finances. Bonne lecture ;-) ","date":"2022-02-09","objectID":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/:5:0","series":null,"tags":["architecture"],"title":"Mieux analyser les risques pour simplifier les architectures (ou pas)","uri":"/2022/02/09/analyser-les-risques-pour-mieux-definir-une-architecture/#conclusion"},{"categories":null,"content":" 1 Pourquoi mettre en oeuvre des GITHUB ACTIONS ?Comme j‚Äôai pu l‚Äôexpliquer dans mon pr√©c√©dent article, je suis pass√© de Wordpress √† GITHUB Pages. Une fois le site d√©ploy√© une premi√®re fois, on voit qu‚Äôon a perdu pas mal d‚Äôautomatisations qui sont r√©alis√©es par d√©faut dans Wordpress. Par exemple, vous devez construire votre site, publier des nouveaux articles et v√©rifier que tout est OK. J‚Äôai donc mis en oeuvre des GITHUB ACTIONS pour automatiser le plus d‚Äôactions possibles et me passer de t√¢ches manuelles souvent r√©barbatives. Si vous souhaitez d√©couvrir les GITHUB ACTIONS, je vous conseille ce site. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:1:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pourquoi-mettre-en-oeuvre-des-github-actions-"},{"categories":null,"content":" 2 Construction du site et d√©ploiementD√®s qu‚Äôon touche √† Jekyll et √† l‚Äôh√©bergement sur Github Pages, on tombe sur certaines actions √† r√©aliser telles que celle-ci: bundle exec jekyll build J‚Äôai donc r√©alis√© les workflows suivants: ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#construction-du-site-et-d√©ploiement"},{"categories":null,"content":" 2.1 Pour une feature branch (dans une Pull Request)Je construis le site sans le d√©ployer pour v√©rifier que la construction est correcte. name: Build Jekyll site on: push: branches-ignore: (1) - main - gh-pages jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: üìÇ setup (2) uses: actions/checkout@v2 # include the lines below if you are using jekyll-last-modified-at # or if you would otherwise need to fetch the full commit history # however this may be very slow for large repositories! # with: # fetch-depth: '0' - name: üíé setup ruby (3) uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: üî® install dependencies \u0026 build site (4) run: bundle exec jekyll build Voila ce que ce workflow r√©alise: Il est ex√©cut√© √† chaque push except√© sur les branches main et gh-pages. Ce sont les branches que j‚Äôutilise pour le d√©ploiement du site apr√®s la validation d‚Äôune pull request. Checkout du projet Initialisation de Ruby et t√©l√©chargement des d√©pendances comme le ferait la commande bundle install. Construction du site ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:1","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-une-feature-branch-dans-une-pull-request"},{"categories":null,"content":" 2.2 Pour la branche mainUne fois que la pull request est valid√©e, le code est pouss√© dans la branche main. On y ex√©cute le code suivant: name: Build and deploy Jekyll site to GitHub Pages on: push: branches: - main jobs: jekyll: runs-on: ubuntu-latest # can change this to ubuntu-latest if you prefer steps: - name: üìÇ setup uses: actions/checkout@v2 - name: üíé setup ruby uses: ruby/setup-ruby@v1 with: ruby-version: 2.6 # can change this to 2.7 or whatever version you prefer bundler-cache: true - name: üî® install dependencies \u0026 build site run: bundle exec jekyll build - name: üöÄ deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./_site Ce dernier reprend le code du workflow suivant ( oui, j‚Äôaurai pu faire des workflows r√©utilisables‚Ä¶ ) et ajoute l‚Äô√©tape de d√©ploiement. Le code g√©n√©r√© sera copi√© dans la branche gh-pages. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:2:2","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#pour-la-branche-main"},{"categories":null,"content":" 3 Publication d‚Äôun articlePour r√©diger un article, j‚Äôutilise le m√©canisme de feature branch et pull request. Pour automatiser la publication, le nommage des articles avec la date etc, j‚Äôai mis en oeuvre le workflow suivant: Je r√©dige les articles (comme celui-ci) et les positionne dans le r√©pertoire _drafts: ls -R _drafts quelques-github-actions-utiles-pour-un-site-jekyll-heberge-sur-github-io.md J‚Äôassocie un milestone √† la pull request. D√®s que ces derniers sont termin√©s, le workflow d√©crit ci-dessous est ex√©cut√©. Il permet, via un script python de: Identifier les articles dans le r√©pertoire _drafts V√©rifier que la date de publication sp√©fici√© dans l‚Äôen-t√™te est ant√©rieure √† la date courante (now()) Copier le fichier dans le r√©pertoire _posts en le renommant avec la date en pr√©fixe. name: Publish Drafts on: (1) milestone: types: [closed] workflow_dispatch: jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 with: ref: main (2) - name: üìÇ setup python uses: actions/setup-python@v2 with: python-version: '3.7.7' # install the python version needed - name: üíé install python packages (3) run: | python -m pip install --upgrade pip - name: üî® execute py script (4) run: python publish_drafts.py - name: üî® commit files (5) run: | if git ls-files -o --exclude-standard; then git config --local user.email \"action@github.com\" git config --local user.name \"GitHub Action\" git add -A git commit -m \"publish drafts\" -a git push origin main else echo \"No file to publish\" fi Explication: D√©clenchement manuel ou √† la cl√¥ture d‚Äôun milestone R√©cup√©ration de la branche main Installation de packages python Ex√©cution du script python r√©alis√© pour l‚Äôoccasion Commit et push Une fois ce workflow r√©alis√©, le workflow vu pr√©c√©demment est automatiquement lanc√© et le site est g√©n√©r√© une nouvelle fois. Bon √ßa fait deux constructions, mais au vu du temps pris, c‚Äôest n√©gligeable. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:3:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#publication-dun-article"},{"categories":null,"content":" 4 UptimeJ‚Äôaurai pu utiliser un tiers service tel que uptime robot. Pour mon besoin, j‚Äôai pr√©f√©r√© opter pour un appel r√©gulier du site et une v√©rification du code HTTP (200). # This is a basic workflow to help you get started with Actions name: Uptime Monitoring on: schedule: (1) - cron: '*/60 * * * *' jobs: ping_site: runs-on: ubuntu-latest name: Ping the site steps: - name: Check the site id: hello uses: srt32/uptime@master with: url-to-hit: \"https://blog.touret.info/robots.txt\" (2) expected-statuses: \"200,301\" Explications D√©clenchement toutes les heures de ce workflow J‚Äôai utilis√© une GITHUB ACTION existante qui ping une URL et v√©rifie le code retour. Dans mon cas, j‚Äôai utilis√© l‚ÄôURL du fichier robots.txt et je v√©rifie le code retour. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:4:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#uptime"},{"categories":null,"content":" 5 ConclusionJ‚Äôai r√©ussi √† plus ou moins automatiser tout le cycle de construction d‚Äôarticles. C‚Äôest encore perfectible et loin de certaines fonctionnalit√©s de Wordpress, mais je n‚Äôen ai pas r√©ellement besoin. Si vous souhaitez r√©utiliser ces workflows et les int√©grer dans sites, vous pouvez les r√©cup√©rer sur ce repo GITHUB. ","date":"2021-12-19","objectID":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/:5:0","series":null,"tags":["github","jekyll","github-actions","planetlibre"],"title":"Mettre en oeuvre des Github Actions utiles pour un site h√©berg√© sur Github pages","uri":"/2021/12/19/mettre-en-oeuvre-github-actions-utiles-pour-un-site-heberge-sur-github-io/#conclusion"},{"categories":null,"content":"L‚Äôid√©e me trottait dans la t√™te depuis quelques mois environ: migrer mon blog de Wordpress vers un site bas√© sur Jekyll et h√©berg√© directement sur Github. La date de renouvellement de ma souscription Wordpress arrivant √† terme, je me suis d√©cid√© √† franchir le pas. s ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:0:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#"},{"categories":null,"content":" 1 L‚Äô h√©bergement de sites web sur GithubGithub permet via son service Github Pages d‚Äôh√©berger des sites statiques (c.-√†-d. pas de base de donn√©es derri√®re) en permettant d‚Äôassocier son nom de domaine. Le certificat est automatiquement g√©n√©r√©. Pour avoir un look un peu sympa, j‚Äôai donc mis en oeuvre les outils suivants: Jekyll Minimal Mistakes Github Actions pour construire le site Markdown pour √©crire les diff√©rents articles ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:1:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#l-h√©bergement-de-sites-web-sur-github"},{"categories":null,"content":" 2 D√©marrer avec Minimal MistakesJe vous conseille d‚Äôaller sur ce site. Tout est bien d√©taill√© est c‚Äôest r√©alisable en quelques minutes seulement. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:2:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#d√©marrer-avec-minimal-mistakes"},{"categories":null,"content":" 3 Migration des donn√©esSans surprise, c‚Äôest la partie la moins dr√¥le. Il faut en r√©sum√©: Exporter les donn√©es si vous √™tes h√©berg√© sur wordpress.com Les r√©-importer dans une instance locale Les exporter au format Jekyll Copier le contenu g√©n√©r√© dans un nouveau site ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#migration-des-donn√©es"},{"categories":null,"content":" 3.1 Exporter les donn√©esAfin d‚Äôexporter les donn√©es et de les transformer, il faut d‚Äôabord exporter les donn√©es (articles + m√©dias) de votre site Wordpress Vous obtiendrez deux archives: la premi√®re pour les articles, la deuxi√®me pour les m√©dias. Cr√©ation d‚Äôune instance Wordpress pour convertir les donn√©es au format Jekyll ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#exporter-les-donn√©es"},{"categories":null,"content":" 3.2 Importer les donn√©esPour faire simple, j‚Äôutilise Docker pour monter une architecture Wordpress sur mon poste. Il faut pour √ßa cr√©er un fichier docker-compose.yml et ins√©rer le contenu suivant: version: \"3.9\" services: db: image: mysql:5.7 volumes: - db_data:/var/lib/mysql restart: always environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress wordpress: depends_on: - db image: wordpress:latest volumes: - wordpress_data:/var/www/html ports: - \"8000:80\" restart: always environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress WORDPRESS_DB_NAME: wordpress volumes: db_data: {} wordpress_data: {} Ensuite, vous aurez √† lancer la commande suivante: docker-compose up Une fois lanc√©, vous aurez √† une instance Wordpress via cette URL : http://localhost:8000 Ensuite, il faut installer l‚Äôextension jekyll-exporter. La proc√©dure peut prendre un peu de temps. Une fois effectu√©e, vous aurez une archive ZIP contenant un site Jekyll avec les images et articles associ√©s. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:3:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#importer-les-donn√©es"},{"categories":null,"content":" 4 Cr√©ation du siteEn attendant que √ßa se termine, j‚Äôai cr√©e un site jekyll avec le starter du th√®me minimal mistakes. J‚Äôai ensuite copi√© les articles (r√©pertoire /_posts) et images (/assets/img). Au premier lancement des commandes suivantes: bundle install bundle exec jekyll serve J‚Äôai eu quelques erreurs. J‚Äôai donc eu √† nettoyer les fichiers via des recherche/remplace dans un √©diteur Par exemple, j‚Äôai supprim√© les r√©f√©rences author et layout author: admin layout: post J‚Äôai √©galement ajout√© pour certains articles une image pour le teaser Exemple: featuredImagePreview: /assets/images/2021/07/rest-book-architecture.png ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#cr√©ation-du-site"},{"categories":null,"content":" 4.1 URL des pages et compatibilit√© Wordpress \u003c\u003e JekyllPour me faciliter la vie dans les URLS et liens en tout genre, j‚Äôai gard√© le format des URLS de Wordpress. Pour que √ßa soit le format par d√©faut de Jekyll, il faut modifier le param√®tre permalink dans le fichier _config.yml permalink: /:year/:month/:day/:title/ ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:1","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#url-des-pages-et-compatibilit√©-wordpress--jekyll"},{"categories":null,"content":" 4.2 Flux RSS pour un tag donn√©J‚Äôutilisais une petite sp√©cificit√© de Wordpress: la cr√©ation d‚Äôun flux RSS pour un tag donn√©. Pour le mettre en place dans Jekyll, il faut configurer le plugin jekyll-feed avec les propri√©t√© suivantes: feed: tags: true ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:4:2","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#flux-rss-pour-un-tag-donn√©"},{"categories":null,"content":" 5 Et maintenant ?J‚Äôai sans doute oubli√© quelques renommages/suppressions r√©alis√©s ici et l√†. N√©anmoins, le principal est √©voqu√© dans cet article. Il ne vous reste plus qu‚Äô√† √©plucher la documentation du th√®me et de jekyll pour finaliser l‚Äô installation de votre nouveau site. ","date":"2021-12-06","objectID":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/:5:0","series":null,"tags":["github","wordpress","planetlibre"],"title":"Migrer son blog Wordpress vers GitHub","uri":"/2021/12/06/migrer-un-blog-wordpress-vers-github-io/#et-maintenant-"},{"categories":null,"content":"Il y a quelques mois d√©j√†, je discutais avec un coll√®gue d‚Äô observabilit√©, opentracing, ‚Ä¶ avec Quarkus. On est tomb√© sur un super exemple r√©alis√© par Antonio Concalves. Ce projet d√©montre les capacit√©s de Quarkus sur les sujets suivants: Circuit Breaker Observabilit√© OpenTracing Tests ‚Ä¶ Et la on peut se demander quid de Spring? Je me doutais que ces fonctionnalit√©s √©taient soient disponibles par d√©faut soient facilement int√©grables vu la richesse de l‚Äô√©cosyst√®me. J‚Äôai donc r√©alis√© un clone de ce projet bas√© sur Spring Boot/Cloud. Je ne vais pas d√©tailler plus que √ßa les diff√©rentes fonctionnalit√©s, vous pouvez vous r√©f√©rer au fichier README. Il est suffisamment d√©taill√© pour que vous puissiez ex√©cuter et les mettre en ≈ìuvre. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:0:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#"},{"categories":null,"content":" 1 Architecture de l‚ÄôapplicationVous trouverez ci-dessous un sch√©ma d‚Äôarchitecture de l‚Äôapplication au format C4. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:1:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#architecture-de-lapplication"},{"categories":null,"content":" 2 Circuit BreakerLors des appels entre le bookstore et le booknumberservice, il peut √™tre int√©ressant d‚Äô impl√©menter un circuit breaker pour pallier aux indisponibilit√©s de ce dernier. Avec Spring, on peut utiliser Resilience4J au travers de Spring Cloud. Tout ceci se fait de mani√®re programmatique Il faut tout d‚Äôabord configurer les circuit breakers au travers d‚Äôune classe Configuration. @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createDefaultCustomizer() { return factory -\u003e factory.configureDefault(id -\u003e new Resilience4JConfigBuilder(id) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()) .circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .build()); } /** * Creates a circuit breaker customizer applying a timeout specified by the \u003ccode\u003ebooknumbers.api.timeout_sec\u003c/code\u003e property. * This customizer could be reached using this id: \u003ccode\u003eslowNumbers\u003c/code\u003e * @return the circuit breaker customizer to apply when calling to numbers api */ @Bean public Customizer\u003cResilience4JCircuitBreakerFactory\u003e createSlowNumbersAPICallCustomizer() { return factory -\u003e factory.configure(builder -\u003e builder.circuitBreakerConfig(CircuitBreakerConfig.ofDefaults()) .timeLimiterConfig(TimeLimiterConfig.custom().timeoutDuration(Duration.ofSeconds(timeoutInSec)).build()), \"slowNumbers\"); } Gr√¢ce √† ces instanciations, on r√©f√©rence les diff√©rents circuit breakers. Maintenant, on peut les utiliser dans le code de la mani√®re suivante: public Book registerBook(@Valid Book book) { circuitBreakerFactory.create(\"slowNumbers\").run( () -\u003e persistBook(book), throwable -\u003e fallbackPersistBook(book) ); return bookRepository.save(book); } Maintenant, il ne reste plus qu‚Äô√† cr√©er une m√©thode de ¬´¬†fallback¬†¬ª utilis√©e si un service est indisponible. Cette derni√®re nous permettra, par exemple, de mettre le payload dans un fichier pour futur traitement batch. ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:2:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#circuit-breaker"},{"categories":null,"content":" 3 Observabilit√©L‚Äôobservabilit√© est sans contexte la pierre angulaire (oui, rien que √ßa‚Ä¶) de toute application cloud native. Sans √ßa, pas de scalabilit√©, de red√©marrage automatique,etc. Les architectures de ce type d‚Äôapplications sont idempotentes. On a donc besoin d‚Äôavoir toutes les informations √† notre disposition. Heureusement, Spring fournit par le biais d‚Äô Actuator toutes les informations n√©cessaires. Ces derni√®res pourront soit √™tre utilis√©es par Kubernetes (ex. le livenessProbe) ou agr√©g√©es dans une base de donn√©es Prometheus. Pour activer certaines m√©triques d‚Äôactuator, il suffit de : Ajouter la/les d√©pendance(s) dependencies { [...] implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'io.micrometer:micrometer-registry-prometheus' [...] } Sp√©cifier la configuration ad√©quate: management: endpoints: enabled-by-default: true web: exposure: include: '*' jmx: exposure: include: '*' endpoint: health: show-details: always enabled: true probes: enabled: true shutdown: enabled: true prometheus: enabled: true metrics: enabled: true health: livenessstate: enabled: true readinessstate: enabled: true datasource: enabled: true metrics: web: client: request: autotime: enabled: true ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:3:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#observabilit√©"},{"categories":null,"content":" 4 OpenTracingSur les applications distribu√©es, il peut s‚Äôav√©rer compliqu√© de concentrer les logs et de les corr√©ler. Certes, avec un ID de corr√©lation, on peut avoir certaines informations. Cependant, il faut que les logs soient bien positionn√©es dans le code. On peut √©galement passer √† travers de certaines informations (ex. connexion aux bases de donn√©es, temps d‚Äôex√©cution des APIS,‚Ä¶). Je ne vous parle pas des soucis de volum√©trie engendr√©es par des index Elasticsearch/Splunk sur des applications √† forte volum√©trie. Depuis quelques temps, le CNCF propose un projet (encore en incubation) : OpenTracing. Ce dernier fait d√©sormais partie d‚ÄôOpenTelemetry. Gr√¢ce √† cet librairie, nous allons pouvoir tracer toutes les transactions de notre application microservices et pouvoir r√©aliser une corr√©lation ¬´¬†out of the box¬†¬ª gr√¢ce √† l‚Äôint√©gration avec Jaeger. Pour activer la fonctionnalit√© il suffit d‚Äôajouter la d√©pendance au classpath: implementation 'io.opentracing.contrib:opentracing-spring-jaeger-cloud-starter:3.3.1' et de configurer l‚ÄôURL de Jaeger dans l‚Äôapplication # Default values opentracing: jaeger: udp-sender: host: localhost port: 6831 enabled: true Une fois l‚Äôapplication reconstruite et red√©marr√©e, vous pourrez visualiser les transactions dans JAEGER: ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:4:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#opentracing"},{"categories":null,"content":" 5 ConclusionJe ne vais pas exposer l‚Äôimpl√©mentation des tests unitaires et d‚Äôint√©gration. Si vous voulez voir comment j‚Äôai r√©ussi √† mocker simplement les appels REST √† une API distante, vous pouvez regarder cette classe pour voir une utilisation du MockServer. Aussi, n‚Äôh√©sitez pas √† cloner, tester ce projet et me donner votre retour. J‚Äôessaierai de le mettre √† jour au fur et √† mesure de mes d√©couvertes (par ex. OpenTelemetry). ","date":"2021-07-26","objectID":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/:5:0","series":null,"tags":["github","java","observability","planetlibre","spring"],"title":"Observabilit√© et Circuit Breaker avec Spring","uri":"/2021/07/26/observabilite-et-circuit-breaker-avec-spring/#conclusion"},{"categories":null,"content":"Quand vous avez une API, et a fortiori une application, il peut √™tre parfois n√©cessaire de passer l‚Äôapplication en mode ¬´¬†maintenance¬†¬ª. Pour certaines applications il est parfois inutile de le traiter au niveau applicatif, car √ßa peut √™tre pris g√©r√© par certaines couches de s√©curit√© ou frontaux web par ex. (Apache HTTPD, WAF) Kubernetes a introduit ( ou popularis√© ) les notions de ¬´¬†probes¬†¬ª et plus particuli√®rement les livenessProbes et readinessProbes. Le premier nous indique si l‚Äôapplication est en √©tat de fonctionnement, le second nous permet de savoir si cette derni√®re est apte √† recevoir des requ√™tes (ex. lors d‚Äôun d√©marrage). Je vais exposer dans cet article comment utiliser au mieux ces probes et les APIs SPRING pour int√©grer dans une API un mode ¬´¬†maintenance¬†¬ª ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:0:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#"},{"categories":null,"content":" 1 Stack utilis√©eDans l‚Äôexemple que j‚Äôai d√©velopp√©, j‚Äôai pu utiliser les briques suivantes: OpenJDK 11.0.10 Spring Boot 2.5.0 (web, actuator) Maven 3.8.1 Bref, rien de neuf √† l‚Äôhorizon üôÇ ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:1:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#stack-utilis√©e"},{"categories":null,"content":" 2 Configuration de Spring ActuatorPour activer les diff√©rents probes, vous devez activer Actuator. Dans le fichier pom.xml, vous devez ajouter le starter correspondant: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-actuator\u003c/artifactId\u003e \u003c/dependency\u003e Puis vous devez d√©clarer ces differentes propri√©t√©s: management.endpoints.enabled-by-default=true management.health.livenessstate.enabled=true management.health.readinessstate.enabled=true management.endpoint.health.show-details=always management.endpoint.health.probes.enabled=true management.endpoint.health.enabled=true Apr√®s avoir red√©marr√© votre application, vous pourrez conna√Ætre son statut gr√¢ce √† un appel HTTP curl -s http://localhost:8080/actuator/health/readiness ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:2:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#configuration-de-spring-actuator"},{"categories":null,"content":" 3 Comment r√©cup√©rer le statut des probes?Avec Spring, vous pouvez modifier les diff√©rents statuts avec les classes ApplicationEventPublisher et ApplicationAvailability. Par exemple, pour conna√Ætre le statut Readiness vous pouvez ex√©cuter le code suivant: @ApiResponses(value = { @ApiResponse(responseCode = \"200\", description = \"Checks if the application in under maitenance\")}) @GetMapping public ResponseEntity\u003cMaintenanceDTO\u003e retreiveInMaintenance() { var lastChangeEvent = availability.getLastChangeEvent(ReadinessState.class); return ResponseEntity.ok(new MaintenanceDTO(lastChangeEvent.getState().equals(ReadinessState.REFUSING_TRAFFIC), new Date(lastChangeEvent.getTimestamp()))); } Et la modification ? Gr√¢ce √† la m√™me API, on peut √©galement modifier ce statut dans via du code: @ApiResponses(value = { @ApiResponse(responseCode = \"204\", description = \"Put the app under maitenance\")}) @PutMapping public ResponseEntity\u003cVoid\u003e initInMaintenance(@NotNull @RequestBody String inMaintenance) { AvailabilityChangeEvent.publish(eventPublisher, this, Boolean.valueOf(inMaintenance) ? ReadinessState.REFUSING_TRAFFIC : ReadinessState.ACCEPTING_TRAFFIC); return ResponseEntity.noContent().build(); } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:3:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#comment-r√©cup√©rer-le-statut-des-probes"},{"categories":null,"content":" 4 Filtre les appels et indiquer que l‚Äôapplication est en maintenanceMaintenant qu‚Äôon a cod√© les m√©canismes de r√©cup√©ration du statut de l‚Äôapplication et de la mise en maintenance, on peut ajouter le m√©canisme permettant de traiter ou non les appels entrants. Pour √ßa on va utiliser un bon vieux filtre servlet. Ce dernier aura la t√¢che de laisser passer les requ√™tes entrantes si l‚Äôapplication n‚Äôest pas en maintenance et de d√©clencher une MaintenanceException le cas √©ch√©ant qui sera trait√© par la gestion d‚Äôerreur globale de l‚Äôapplication ( trait√© via un @RestControllerAdvice). Pour que l‚Äôexception soit bien trait√©e par ce m√©canisme, il faut le d√©clencher via le HandlerExceptionResolver. @Component public class CheckMaintenanceFilter implements Filter { private final static Logger LOGGER = LoggerFactory.getLogger(CheckMaintenanceFilter.class); @Autowired private ApplicationAvailability availability; @Autowired @Qualifier(\"handlerExceptionResolver\") private HandlerExceptionResolver exceptionHandler; /** * Checks if the application is under maintenance. If it is and if the requested URI is not '/api/maintenance', it throws a \u003ccode\u003eMaintenanceException\u003c/code\u003e * * @param request * @param response * @param chain * @throws IOException * @throws ServletException * @throws info.touret.spring.maintenancemode.exception.MaintenanceException the application is under maintenance */ @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { if (availability.getReadinessState().equals(ReadinessState.REFUSING_TRAFFIC) \u0026\u0026 !((HttpServletRequest) request).getRequestURI().equals(API_MAINTENANCE_URI)) { LOGGER.warn(\"Message handled during maintenance [{}]\", ((HttpServletRequest) request).getRequestURI()); exceptionHandler.resolveException((HttpServletRequest) request, (HttpServletResponse) response, null, new MaintenanceException(\"Service currently in maintenance\")); } else { chain.doFilter(request, response); } } } Enfin, voici la gestion des erreurs de l‚ÄôAPI: @RestControllerAdvice public class GlobalExceptionHandler { /** * Indicates that the application is on maintenance */ @ResponseStatus(HttpStatus.I_AM_A_TEAPOT) @ExceptionHandler(MaintenanceException.class) public APIError maintenance() { return new APIError(HttpStatus.I_AM_A_TEAPOT.value(),\"Service currently in maintenance\"); } /** * Any other exception */ @ResponseStatus(HttpStatus.INTERNAL_SERVER_ERROR) @ExceptionHandler({RuntimeException.class, Exception.class}) public APIError anyException() { return new APIError(HttpStatus.INTERNAL_SERVER_ERROR.value(),\"An unexpected server error occured\"); } } ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:4:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#filtre-les-appels-et-indiquer-que-lapplication-est-en-maintenance"},{"categories":null,"content":" 5 ConclusionOn a pu voir comment int√©ragir simplement avec les APIS SPRING pour g√©rer le statut de l‚Äôapplication pour r√©pondre √† cette question :Est-elle disponible ou non? Bien √©videmment, selon le contexte, il conviendra d‚Äôajouter un peu de s√©curit√© pour que cette API ne soit pas disponible √† tout le monde üôÇ Le code expos√© ici est disponible sur Github. Le Readme est suffisamment d√©taill√© pour que vous puissiez tester et r√©utiliser le code. ","date":"2021-06-10","objectID":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/:5:0","series":null,"tags":["actuator","observability","planetlibre","spring","springboot"],"title":"Ajouter un mode ¬´¬†maintenance¬†¬ª √† votre API gr√¢ce √† Spring boot","uri":"/2021/06/10/ajouter-un-mode-maintenance-a-votre-api-grace-a-spring-boot/#conclusion"},{"categories":null,"content":" Pourquoi utiliser GPG ? Par exemple pour signer les commits GIT. Maintenant comment faire quand on est sous Windows 10 et qu‚Äôon souhaite utiliser le sous syst√®me Linux (WSL2)? Sous GNU/Linux, l‚Äôinstallation et l‚Äôutilisation avec git est tr√®s simple. Avec WSL2,‚Ä¶ il faut un peu d‚Äôhuile de coude üôÇ Je vais t√¢cher de d√©crire dans cet article les diff√©rentes manipulations n√©cessaires pour: Importer une cl√© GPG existante Utiliser GPG pour signer mes commits dans WSL2 ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:0:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#"},{"categories":null,"content":" 1 Importer une cl√© GPG existante","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#importer-une-cl√©-gpg-existante"},{"categories":null,"content":" 1.1 Export de la cl√© GPG 1.1.1 Identifier l‚Äô ID de la cl√©Lancez la commande suivante: gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#export-de-la-cl√©-gpg"},{"categories":null,"content":" 1.1 Export de la cl√© GPG 1.1.1 Identifier l‚Äô ID de la cl√©Lancez la commande suivante: gpg --export ${ID} \u003e public.key gpg --export-secret-key ${ID} \u003e private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:1","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#identifier-l-id-de-la-cl√©"},{"categories":null,"content":" 1.2 Import gpg --import public.key gpg --import private.key ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:2","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#import"},{"categories":null,"content":" 1.3 V√©rificationPour v√©rifier que la cl√© est bien configur√©e, vous pouvez lancer la commande suivante: gpg --list-secret-keys --keyid-format LONG alexandre@.... sec rsa4096/CLE_ID 2019-12-20 [SC] ******************** uid [ ultime ] Alexandre \u003calexandre@....\u003e ssb rsa4096/SUB 2019-12-20 [E] Si la cl√© n‚Äôest pas reconnue comme ultime ou comme de confiance, il faudra l‚Äô√©diter: gpg --edit-key CLE_ID Please decide how far you trust this user to correctly verify other users' keys (by looking at passports, checking fingerprints from different sources, etc.) 1 = I don't know or won't say 2 = I do NOT trust 3 = I trust marginally 4 = I trust fully 5 = I trust ultimately m = back to the main menu Your decision? Si vous ne voulez pas trop vous compliquer, je vous conseille de r√©pondre 5. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:1:3","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#v√©rification"},{"categories":null,"content":" 2 Configuration GPG pour WSL2Avant de configurer l‚Äôagent GPG, vous pouvez vous r√©f√©rer √† cet article pour configurer GIT et GPG. La configuration est √©quivalente. Ensuite, cr√©ez le fichier ~/.gnupg/gpg.conf avec le contenu suivant: # Uncomment within config (or add this line) # This tells gpg to use the gpg-agent use-agent # Set the default key default-key CLE_ID Puis cr√©ez le fichier ~/.gnupg/gpg-agent.conf avec le contenu ci-dessous: default-cache-ttl 34560000 max-cache-ttl 34560000 pinentry-program /usr/bin/pinentry-curses Le cache ici est d√©fini en secondes. Il est mis ici √† 400 jours. Ce dernier fichier fait r√©f√©rence au programme pinentry. Vous pouvez v√©rifier sa pr√©sence gr√¢ce √† la commande: ls /usr/bin/pinentry-curses Si vous ne l‚Äôavez pas, vous pouvez l‚Äôinstaller gr√¢ce √† la commande suivante: sudo apt install pinentry-curses Maintenant, on peut configurer l‚Äôenvironnement BASH en modifiant le fichier ~/.bashrc # enable GPG signing export GPG_TTY=$(tty) if [ ! -f ~/.gnupg/S.gpg-agent ]; then eval $( gpg-agent --daemon --options ~/.gnupg/gpg-agent.conf ) fi export GPG_AGENT_INFO=${HOME}/.gnupg/S.gpg-agent:0:1 Red√©marrez ensuite WSL2 pour que √ßa soit pris en compte. A la premi√®re utilisation de GPG ( par ex. lors d‚Äôun commit, vous aurez une interface Ncurses qui appara√Ætra dans votre prompt WSL2. Vous aurez √† renseigner le mot de passe de votre cl√©. ","date":"2021-05-03","objectID":"/2021/05/03/utiliser-gpg-dans-wsl2/:2:0","series":null,"tags":["git","gpg","planetlibre","wsl2"],"title":"Utiliser GPG dans WSL2","uri":"/2021/05/03/utiliser-gpg-dans-wsl2/#configuration-gpg-pour-wsl2"},{"categories":null,"content":"Les confinements se suivent et se ressemblent. Me voil√† √† installer Ubuntu sur un nouvel ordinateur. A l‚Äôinstar de l‚Äôancien laptop que j‚Äôai achet√© pour mon a√Æn√©e, j‚Äôai achet√© un DELL pour ma deuxi√®me fille.J‚Äôai opt√© pour un DELL Inspiron 5301. A l‚Äôinstar de mon autre laptop, je j‚Äôai pas pris de risques. J‚Äôai opt√© pour un DELL qui est pleinement compatible avec Ubuntu. Oui j‚Äôaurai pu installer un ordinateur avec Ubuntu pr√©-install√©, mais je n‚Äôai pas eu le temps de faire un choix ¬´¬†serein¬†¬ª. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:0:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#"},{"categories":null,"content":" 1 Configuration du BIOSVoila les param√®tres que j‚Äôai appliqu√©: Dans le menu \"Storage\" puis \"SATA Operation\": vous devez s√©lectionner AHCI au lieu de RAID. Dans le menu \"Change boot mode settings \u003eUEFI Boot Mode\" , vous devez d√©sactiver le Secure Boot. Une fois r√©alis√©, vous pouvez red√©marrer en appuyant sur la touche F12. Si vous n‚Äôarrivez pas √† revenir sur le BIOS pour indiquer de booter sur votre cl√© USB, vous obtiendrez un √©cran d‚Äôerreur Windows d√ª √† la configuration AHCI. Personnellement, en red√©marrant une ou deux fois, j‚Äôai obtenu un √©cran de d√©marrage avanc√© qui m‚Äôa permis de s√©lectionner le p√©riph√©rique (ma cl√© USB) sur lequel d√©marrer. Maintenant vous pouvez acc√©der √† l‚Äôinstalleur Ubuntu et profiter. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:1:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#configuration-du-bios"},{"categories":null,"content":" 2 InstallationJ‚Äôai eu plusieurs fois des popup ¬´¬†erreur rencontr√©¬†¬ª. Ce n‚Äô√©tait pas bloquant. J‚Äôai continu√©. Tout s‚Äôest d√©roul√© sans encombre. Le mat√©riel est tr√®s bien reconnu. Les seuls logiciels que j‚Äôai install√© sont pour l‚Äôinstant : VLC, Minecraft ( obligatoire dans la famille ) et Chromium. ","date":"2021-04-02","objectID":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/:2:0","series":null,"tags":["dell","planetlibre","ubuntu"],"title":"Installer Ubuntu 20.04 LTS sur un DELL Inspiron 13 5000","uri":"/2021/04/02/installer-ubuntu-20-04-lts-sur-un-dell-inspiron-13-5000/#installation"},{"categories":null,"content":"D√®s qu‚Äôon veut d√©ployer des environnements Kubernetes, helm devient une des solutions √† consid√©rer. Le d√©ploiement des objets standards tels que deployment, autoscaler et autres se fait ais√©ment car ces derniers ne changent pas d‚Äôun environnement √† l‚Äôautre. G√©n√©ralement on d√©ploie la m√™me infrastructure sur tous les environnements du d√©veloppement √† la production. Bien √©videmment on pourra limiter la taille des replicas sur l‚Äôenvironnement de d√©veloppement par exemple mais au fond, le contenu des charts sera identique. Une des difficult√©s que l‚Äôon pourra rencontrer c‚Äôest dans la gestion des fichiers de configuration. Je vais essayer d‚Äôexposer dans cet article comment j‚Äôai r√©ussi √† g√©rer +/- efficacement (en tout cas pour moi) les fichiers de configuration dans les charts HELM. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:0:0","series":null,"tags":null,"title":"G√©rer ¬´¬†efficacement¬†¬ª les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#"},{"categories":null,"content":" 1 Les config maps et secretsLogiquement dans ce type d‚Äôarchitecture, les configmaps et secrets permettent le chargement des variables d‚Äôenvironnement et autres mots de passe. Cependant si vous utilisez certains frameworks qui n√©cessitent des fichiers de configuration, vous devrez charger les fichiers dans des volumes. Pour ces derniers, les volumes n‚Äôont pas besoin d‚Äô√™tre persistents. Par exemple dans la configuration de votre deployment, vous pourrez configurer le montage d‚Äôun volume de la mani√®re suivante: volumeMounts: - mountPath: /config name: configuration-volume readOnly: true - mountPath: /secrets name: secret-volume readOnly: true [...] volumes: - configMap: defaultMode: 420 name: configuration name: configuration-volume - name: secret-volume secret: defaultMode: 420 secretName: secrets Pour int√©grer un fichier binaire, on pourra le faire de la mani√®re suivante dans le template HELM: apiVersion: v1 # Definition of secrets kind: Secret [...] type: Opaque # Inclusion of binary configuration files data: my_keystore_jks {% raw %} .Files.Get \"secrets/my_keystore.jks\" | b64enc }} {% endraw %} Vous pouvez d√©finir les fichiers directement dans vos configmaps. Cependant, si vos fichiers sont volumineux, vous aurez du mal √† les maintenir. Personnellement, j‚Äôopte pour mettre les fichiers de configuration √† cot√© et les charger dans le configmap. On pourra proc√©der de la mani√®re suivante: apiVersion: v1 kind: ConfigMap [...] data: my_conf: {% raw %}{{- (.Files.Glob \"conf/*\").AsConfig | nindent 2 }} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:1:0","series":null,"tags":null,"title":"G√©rer ¬´¬†efficacement¬†¬ª les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#les-config-maps-et-secrets"},{"categories":null,"content":" 2 Livrables agnostiquesUne bonne pratique de d√©veloppement logiciel est d‚Äôexternaliser la configuration de vos environnements (ex. l‚ÄôURL JDBC de la base de donn√©es) des livrables. Les charts HELM n‚Äô√©chappent √† la r√®gle. On peut stocker la configuration de chaque environnement dans le chart, mais dans ce cas, on perdra beaucoup de souplesse lors des mises √† jour des propri√©t√©s et cela nous imposera une nouvelle version. On a plusieurs niveaux d‚Äôexternalisation. Le premier est dans le chart. Vous pouvez externaliser les diff√©rentes valeurs dans le fichier values.yml. Ci dessous un exemple avec un autoscaler: apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: labels: [...] spec: maxReplicas: {% raw %} {{.Values.myapp.maxReplicaCount }}{% endraw %} minReplicas: {% raw %} .Values.myapp.minReplicaCount }}{% endraw %} scaleTargetRef: apiVersion: apps/v1 kind: Deployment [...] targetCPUUtilizationPercentage: {% raw %} .Values.myapp.replicationThreesold }}{% endraw %} Les valeurs sont d√©crites comme suit: myapp: minReplicaCount: \"2\" maxReplicaCount: \"6\" replicationThreesold: 80 Pour externaliser les valeurs d‚Äôenvironnement, vous pourrez donc externaliser un autre fichier values.yml qui sera appliqu√© au d√©ploiement. Les valeurs de ce dernier surchargeront les valeurs d√©finies dans le chart. Il est important de noter √©galement que les donn√©es pr√©sentes dans les fichiers de configuration (ex. fichier application.properties) peuvent √™tre ¬´¬†variabilis√©es¬†¬ª et surcharg√©es par le m√™me m√©canisme. Vous aurez √† utiliser la commande tpl. apiVersion: v1 kind: ConfigMap metadata: name: configuration labels: [...] data: application.properties: |- {% raw %}{{ tpl (.Files.Get \"conf/application.properties\") . | nindent 4}} {% endraw %} ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:2:0","series":null,"tags":null,"title":"G√©rer ¬´¬†efficacement¬†¬ª les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#livrables-agnostiques"},{"categories":null,"content":" 3 ConclusionVous l‚Äôaurez compris, les charts HELM n‚Äô√©chappent pas aux r√®gles d√©j√† connues de gestion des environnements et des livrables. M√™me si il y a quelques subtilit√©s √† conna√Ætre pour int√©grer des fichiers de configuration par exemple, les grands principes restent les m√™mes. ","date":"2021-01-09","objectID":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/:3:0","series":null,"tags":null,"title":"G√©rer ¬´¬†efficacement¬†¬ª les fichiers de configuration dans les charts HELM","uri":"/2021/01/09/gerer-efficacement-les-fichiers-de-configuration-dans-les-charts-helm/#conclusion"},{"categories":null,"content":"Depuis quelques ann√©es, Kubernetes (K8S) et son √©cosyst√®me deviennent l‚Äôenvironnement d‚Äô ex√©cution √† la mode. Certaines personnes veulent d√©ployer sur cet environnement en mettant en avant ses capacit√©s de scalabilit√©. D‚Äôautres font du bashing (souvent) justifi√© sur la complexit√© et le co√ªt de mise en ≈ìuvre d‚Äôune telle plateforme. Vous l‚Äôaurez compris, cette technologie n‚Äô√©chappe pas au cycle du hype et √† la fameuse courbe du Gartner. Apr√®s quelques exp√©riences sur cette plateforme ( et beaucoup sur d‚Äôautres üòÄ ) je vais essayer de peser le pour et le contre qui m‚Äôapparaissent importants. Bien √©videmment, ce n‚Äôest que mon avis, j‚Äôai sans doute omis certaines informations qui pourraient √™tre indispensables pour d‚Äô autres. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:0:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#"},{"categories":null,"content":" 1 Pourquoi et dans quelles conditions il ne faut pas utiliser K8S ?Avant de pr√©senter les avantages des applications cloud, je vais essayer de r√©aliser l‚Äôanti th√®se de mon propos. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-et-dans-quelles-conditions-il-ne-faut-pas-utiliser-k8s-"},{"categories":null,"content":" 1.1 En avez vous (vraiment) besoin ?Vaste sujet et question d√©licate pour la population informaticienne qui a tendance √† suivre les tendances du march√©. Cycle Hype Avant de foncer t√™te baiss√©e dans cette technologie qui est tr√®s int√©ressante au demeurant, il est important de se poser ces quelques questions: Est-ce que mes SLO sont contraignantes? Quel le cycle de d√©ploiement de mes applications? Qui g√®re les environnements ? Bref, il faut savoir si le jeu en vaut la chandelle. Si vous avez une application qui doit scaler dynamiquement, encaisser les pics, et avoir du zero downtime durant les mises √† jour, Kubernetes est fait pour vous. Si vous avez une application de gestion qui n‚Äôa pas d‚Äôexigences fortes si ce n‚Äôest de r√©pondre aux besoins fonctionnels, l‚Äôutilisation de Kubernetes est discutable. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#en-avez-vous-vraiment-besoin-"},{"categories":null,"content":" 1.2 √ätes vous taill√© pour ?Kubernetes et son √©cosyst√®me peuvent s‚Äôav√©rer complexes √† appr√©hender. Si votre entreprise opte pour une utilisation ¬´¬†on premise¬´¬†, c‚Äôest pire. Vous devrez avoir une √©quipe d√©di√©e qui g√©rera cette plateforme et offrir une expertise aux √©quipes de d√©veloppement. Ne vous trompez pas. Si votre r√¥le est de d√©velopper des applications m√©tier, il vous sera tr√®s difficile d‚Äôavoir √©galement une expertise sur l‚Äôadministration de cette plateforme. Vous pourrez l‚Äôutiliser et √™tre √† l‚Äôaise, mais l‚Äôadministration d‚Äôune telle technologie est tr√®s compliqu√©e. Le seul conseil que je pourrais vous donner, c‚Äôest de ne partir sur Kubernetes que si vous avez une √©quipe support √† disposition. C‚Äôest vrai si vous utilisez des services du Cloud tels que Google Cloud ou AWS. √áa l‚Äôest encore plus si vous utilisez des services ¬´¬†on premise¬†¬ª tels qu‚Äô Openshift. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#√™tes-vous-taill√©-pour-"},{"categories":null,"content":" 1.3 Est-ce que vos d√©veloppements sont ¬´¬†cloud native¬†¬ª ?Au del√† de la plateforme, vous devrez monter en comp√©tence sur le d√©veloppement et la conception de vos applications. Il vous faudra prendre en consid√©ration les 12 facteurs cl√©s dans vos applications. Il n‚Äôest pas forc√©ment la peine de passer sur des microservices. Il est √©galement possible de faire des monolithes modulaires qui peuvent √™tre l√©gers et stateless. Beaucoup de ces facteurs sont commun√©ment admis comme des bonnes pratiques de d√©veloppement logiciel (ex. Il faut une int√©gration continue). Aussi, cela va sans dire, il faut √©galement monter (r√©ellement) en comp√©tence sur les conteneurs et leurs contraintes. Si vous n‚Äôavez pas l‚Äôhabitude de travailler avec des conteneurs ( construction, d√©ploiement, disponibilit√© d‚Äôune registry). Il est pr√©f√©rable de d√©finir une trajectoire avec des √©tapes interm√©diaires. Bref, tous ces sujets doivent √™tre adress√©s et compris pour toutes les parties prenantes de vos √©quipes que √ßa soit les d√©veloppeurs, les chefs de projet et les √©quipes m√©tiers √† une moindre mesure. Cette technologie repr√©sente r√©ellement un grand pas √† franchir. Si vous ne vous sentez pas de le faire, ou si vous devez gagner en maturit√© sur ces sujets, attendez avant de vous lancer sur Kubernetes. On ne pourra jamais vous reprocher de ne pas opter sur Kubernetes si vous ne remplissez pas tous les pr√©-requis. Pour ce qui est du contraire‚Ä¶ ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#est-ce-que-vos-d√©veloppements-sont-cloud-nativehttpswwwredhatcomfrtopicscloud-native-apps-"},{"categories":null,"content":" 1.4 Avez vous des interactions avec des services tiers qui sont compatible avec Kubernetes ?Quand vous restez dans votre cluster Kubernetes, g√©n√©ralement, tout va bien. D√®s que vous avez des interactions avec des services tiers, √ßa peut se compliquer. En effet, g√©n√©ralement vous devrez vous connecter √† des services tiers qui ne sont pas orient√© cloud : des boitiers crypto, des passerelles de transfert, ‚Ä¶ Il se peut que certains protocoles soient √©galement incompatibles avec Kubernetes. Il vous faudra vous assurer que tout la galaxie de logiciels et syst√®mes gravitant autour de votre application sera compatible avec une telle architecture. Ceci n‚Äôest pas une mince affaire. L‚Äôaide d‚Äôune √©quipe support (voir ci-dessus) vous sera d‚Äôune grande utilit√©. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:1:4","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#avez-vous-des-interactions-avec-des-services-tiers-qui-sont-compatible-avec-kubernetes-"},{"categories":null,"content":" 2 Pourquoi sauter le pas ?","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#pourquoi-sauter-le-pas-"},{"categories":null,"content":" 2.1 La scalabilit√© et la r√©sistance √† la pannePersonnellement, la premi√®re fonctionnalit√© qui m‚Äôa int√©ress√© c‚Äôest la gestion de la scalabilit√©. Si vous avez des objectifs de 99.9% de disponibilit√©. Kubernetes sera une plus value ind√©niable dans votre architecture. Apr√®s quelques jours heures √† batailler avec les fichiers YAML, vous pourrez g√©rer automatiquement la scalabilit√© en fonction de plusieurs indicateurs qu‚Äôils soient techniques (ce sont les plus faciles √† g√©rer) ou un peu plus m√©tier en utilisant Prometheus ‚Äì et oui encore une technologie suppl√©mentaire √† conna√Ætre. En effet, au lieu de vous en soucier une fois arriv√© en production, vous aurez lors du d√©veloppement l‚Äôobligation de prendre en consid√©ration l‚Äôobservabilit√© de votre application. Par exemple, vous aurez √† renseigner si votre application est pr√™te et/ou disponible pour traiter les requ√™tes. Ces indicateurs vous permettront de scaler automatiquement et de re-cr√©er si n√©cessaire un POD en cas de panne. J‚Äôai trouv√© que cette pratique √©tait vertueuse. Bien √©videmment, pas besoin d‚Äô√™tre sur Kubernetes pour avoir de l‚Äôobservabilit√© dans des applications. Par contre, ici, c‚Äôest obligatoire et impl√©ment√© d√®s le d√©veloppement. La scalabilit√© automatique est aussi tr√®s int√©ressante. On a souvent vu des serveurs en production qui n‚Äô√©taient pas suffisamment utilis√©s. Ici vous n‚Äôaurez que les instances n√©cessaires pour votre cas d‚Äôutilisation. La contrainte que l‚Äôon peut voir √† cette fonctionnalit√© et qu‚Äôon ne maitrise pas compl√®tement le nombre d‚Äôinstances disponibles. C‚Äôest Kubernetes qui s‚Äôen charge en prenant en compte le param√©trage que vous aurez renseign√© dans vos templates HELM. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:1","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#la-scalabilit√©-et-la-r√©sistance-√†-la-panne"},{"categories":null,"content":" 2.2 Le d√©ploiementAvant de d√©ployer (dans la vraie vie), vous aurez √† mettre en place un pipeline CI/CD qui orchestre les diff√©rents d√©ploiements sur tous vos environnements. Attention, ce n‚Äôest pas une mince affaire üôÇ ! Une fois r√©alis√©, vous verrez automatiquement le gain. Vos d√©ploiements seront r√©ellement fluides. Bon OK, on peut le faire sur des VMS standards. Mais on peut am√©liorer la proc√©dure de d√©ploiement pour mettre en place du zero downtime pour ne pas interrompre le service lors d‚Äôun d√©ploiement. strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:2","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#le-d√©ploiement"},{"categories":null,"content":" 2.3 L‚ÄôInfrastructure As Code Quand on pense √† Kubernetes, et au cloud, on ne pense pas trop √† l‚ÄôInfrastructure As Code au d√©but. Cependant, cette pratique est pour moi l‚Äôune des plus utiles. En effet, avoir votre syst√®me d√©crit dans des fichiers, versionn√©s vous permet de le tester d√®s le d√©veloppement. √áa √©vite ( dans la majorit√© des cas ) les erreurs lors des installations d‚Äôenvironnement. La mise √† jour des logiciels est largement acc√©l√©r√©e. Bien √©videmment, il existe Terraform et Ansible pour le provisionning des environnements. Ici je trouve qu‚Äôon pousse le concept encore plus loin. L‚Äôautomatisation est √† mon avis pouss√© √† paroxysme. Prenons par exemple la gestion des syst√®mes d‚Äôexploitation. La mise √† jour sur des serveurs physiques ou virtuels peut prendre √©norm√©ment de temps et g√©n√©rer des erreurs. Avec de l‚Äôinfra as code, ceci est test√© et valid√© automatiquement via des tests unitaires d√®s l‚Äôenvironnement de d√©veloppement. On peut suivre la gestion des environnements via un gestionnaire de sources et la promotion vers les autres environnements (recette[1-n], pr√©-production, production) est grandement acc√©l√©r√©e. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:2:3","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#linfrastructure-as-code"},{"categories":null,"content":" 3 ConclusionBon, vous l‚Äôaurez peut √™tre compris, cette galaxie de technologies est int√©ressante et peut vous aider dans vos projets. Avant d‚Äôarriver √† l‚Äôutiliser sereinement, il vous faudra sans doute d√©finir une trajectoire et appr√©hender plusieurs sujets avant d‚Äôarriver √† d√©ployer vos applications sur un cloud interne ou externe. J‚Äôesp√®re que cet article vous aura permis de mettre en √©vidence les pour et contre d‚Äôune telle technologie et le cas √©ch√©ant vous donnera envie de franchir le pas. ","date":"2020-10-08","objectID":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/:3:0","series":null,"tags":["helm","hype","k8s","kubernetes"],"title":"K8S, HELM et Cie: au del√† de la hype","uri":"/2020/10/08/k8s-helm-et-cie-au-dela-de-la-hype/#conclusion"},{"categories":null,"content":"Derri√®re ce nom pompeux qui peut effrayer, je vais essayer d‚Äôexpliquer dans cet article comment on peut versionner facilement ses partitions et les publier sur le web. En cherchant comment mettre de la documentation technique avec des diagrammes PlantUml dans des repos GITLAB et g√©n√©r√©s avec des pipelines, je me suis mis dans la t√™te de faire la m√™me chose avec des partitions üôÇ Depuis plusieurs ann√©es, j‚Äôutilise lilypond pour cr√©er mes partitions. C‚Äôest un peu difficile de s‚Äôy mettre, mais une fois la syntaxe assimil√©e, la saisie d‚Äôune partition est beaucoup plus efficace. Le rendu des partitions est vraiment optimis√©. Si vous voulez plus de d√©tails sur le pourquoi du comment je vous conseille cette page. Vous trouverez des exemples sur le site. J‚Äôai donc eu l‚Äôid√©e de: Stocker ces partitions sur un repo github (jusque l√† rien d‚Äôexceptionnel) G√©n√©rer automatiquement les partitions au format PDF, PNG et MIDI via une github action (√ßa commence √† devenir int√©ressant‚Ä¶) Les publier avec les github pages (tant qu‚Äô√† faire üôÇ) ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:0:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#"},{"categories":null,"content":" 1 StockagePourquoi stocker dans un r√©f√©rentiel de sources tel que Github ? Pour les non informaticiens : les partitions sont stock√©es au format texte. \\version \"2.12.1\" \\header { title=\"Try a little tenderness\" composer=\"Harry Woods, Jimmy Campbell \u0026 Reg Connely\" subtitle = \"Commitments Version\" %poet = \"Poete\" instrument = \"Piano\" editor = \"L'√©diteur\" %meter=\\markup {\\bold {\"Remarque sur le rhythme\"}} style = \"Soul\" maintainer = \"Alexandre Touret\" maintainerEmail = \"alexandre.touret@free.fr\" maintainerWeb = \"http://blog.touret.info\" lastupdated = \"\" source = \"Music room\" footer = \"Footer\" copyright =\\markup {\\fontsize #-1.5 \"Delivered by A TOURET\"} } upper= \\relative c'{ \\clef treble \\time 4/4 \\tempo 4=176 \\key g \\major d'2 (b4 e d2 b4 a g2 g2 \u003ce g,\u003e2) \u003cfis, c' d\u003e \\bar \"||\" GIT et GITHUB permettent de versionner facilement et pouvoir faire facilement un retour arri√®re en cas d‚Äôerreur. Aussi, GITHUB offre des fonctionnalit√©s ¬´¬†sociales¬†¬ª et collaboratives qui facilitent la revue des modifications ( en cas de travail √† plusieurs ). Bref, √ßa offre la s√©curit√© d‚Äôune sauvegarde et la possibilit√© d‚Äôun retour arri√®re en cas d‚Äôerreur. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:1:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#stockage"},{"categories":null,"content":" 2 G√©n√©rer les partitions avec une github actionLes github actions sont des outils permettant: GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. J‚Äôai donc d√©cid√© de cr√©er un workflow qui permet de g√©n√©rer les partitions au format lilypond. J‚Äôai mis √† disposition le code sur github sous licence GNU GPLv3. Elle est utilisable telle quelle. Pour cr√©er l‚Äôaction, il faut cr√©er un fichier action.yml √† la racine du repo. Voici le contenu name: 'Lilypond Generator' description: 'Run Lilypond tool with the given set of arguments to generate music sheets' author: '@alexandre-touret' inputs: args: description: 'Arguments for Lilyponid' required: true default: '-h' runs: using: 'docker' image: 'Dockerfile' args: - ${{ inputs.args }} branding: icon: 'underline' color: 'blue' Vous aurez compris que ce fichier fait r√©f√©rence √† une image Docker. Cette derni√®re n‚Äôest ni plus ni moins qu‚Äôune Debian avec lilypond d‚Äôinstall√©. Pour l‚Äôutiliser dans un repo github, on peut cr√©er une action qui l‚Äôutilise. Voici un exemple: jobs: build_sheets: runs-on: ubuntu-latest env: LILYPOND_FILES: \"*.ly\" steps: - name: Checkout Source uses: actions/checkout@v1 - name: Get changed files id: getfile run: | echo \"::set-output name=files::$(find ${{github.workspace}} -name \"${{ env.LILYPOND_FILES }}\" -printf \"%P\\n\" | xargs)\" - name: LILYPOND files considered echo output run: | echo ${{ steps.getfile.outputs.files }} - name: Generate PDF music sheets uses: alexandre-touret/lilypond-github-action@master with: args: -V -f --pdf ${{ steps.getfile.outputs.files }} A la derni√®re ligne on peut passer les arguments n√©cessaires √† lilypond. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:2:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#g√©n√©rer-les-partitions-avec-une-github-action"},{"categories":null,"content":" 3 PublicationLa c‚Äôest l‚Äô√©tape la plus facile :). Il suffit d‚Äôactiver les github pages et de commiter et pusher les partitions g√©n√©r√©es - name: Push Local Changes run: | git config --local user.email \"${{ secrets.GIT_USERNAME }}\" git config --local user.name \"${{ secrets.GIT_EMAIL }}\" git add . git commit -m \"Add changes\" -a - name: Push changes uses: ad-m/github-push-action@master with: github_token: ${{ secrets.GITHUB_TOKEN }} Il suffit de cr√©er une page index.md √† la racine et d‚Äôajouter des liens vers les partitions g√©n√©r√©es ( dans mon cas, √ßa se passe dans le r√©pertoire /docs ). Vous pouvez trouver un exemple ici. ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:3:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#publication"},{"categories":null,"content":" 4 ConclusionVoila comment on peut g√©n√©rer un site avec des partitions cr√©es avec Lilypond. Vous trouverez les diff√©rents liens ci-dessous. Peut √™tre que je publierai cette action sur le marketplace une fois que j‚Äôaurai publi√© une documentation digne de ce nom :). Action Exemple d‚Äôutilisation Exemple de site g√©n√©r√© ","date":"2020-08-24","objectID":"/2020/08/24/music-scores-as-code/:4:0","series":null,"tags":["docker","github","lilypond","planetlibre"],"title":"Music Scores As Code","uri":"/2020/08/24/music-scores-as-code/#conclusion"},{"categories":null,"content":"A mes heures perdues, je travaille sur un ¬´¬†POC/side project qui n‚Äôaboutira pas et je m‚Äôen fiche¬†¬ª bas√© sur Quarkus. J‚Äô ai choisi d‚Äôutiliser les langages et composants suivants : Kotlin Quarkus Gradle Kubernetes pour le d√©ploiement Oui, tant qu‚Äô√† faire, autant aller dans la hype ‚Ä¶ Mon projet est sur GITHUB. Pour automatiser certaines actions et, disons-le, par fiert√© personnelle, j‚Äôai choisi d‚Äôautomatiser certaines actions par la mise en ≈ìuvre de pipelines CI/CD. Depuis peu, GITHUB a int√©gr√© un m√©canisme de pipeline : GITHUB Actions. √áa permet, entre autres, de lancer des processus automatis√© sur un push ou sur une action pour un commit GIT. La force de l‚Äôoutil est, selon moi, de facilement s‚Äôint√©grer avec beaucoup de services du cloud ( sonarcloud, google cloud, heroku,‚Ä¶). On aime ou on n‚Äôaime pas, mais chez Microsoft, l‚Äôint√©gration ils savent faire. Par exemple, si on veut lancer une compilation lors d‚Äôun push, on peut placer un fichier .github/workflows/build.xml avec le contenu : name: CI on: [push] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build with Gradle without testing run: ./gradlew build -x test Cot√© GITHUB, vous verrez l‚Äôex√©cution sur un √©cran d√©di√© Vous pouvez cr√©er autant de workflows que vous souhaitez (si votre projet est en libre acc√®s). Pour chaque workflow, on peut d√©finir et utiliser des jobs. Les logs d‚Äôex√©cution sont disponibles dans ce m√™me √©cran: ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:0:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#"},{"categories":null,"content":" 1 Worflows impl√©ment√©sJ‚Äôai choisi d‚Äôimpl√©menter les workflows suivants: CI: Build sur la feature branch CD: Build sur master branch et d√©ploiement On obtient donc dans mon cas: Ce n‚Äôest pas parfait. Loin de l√†. Dans la ¬´¬†vraie vie¬†¬ª, pour une √©quipe de dev, je l‚Äôam√©liorerai sans doute par un build docker dans les features branches, une validation formelle et bloquante de l‚Äôanalyse sonar, etc. Pour un dev perso √ßa suffit largement. Le contenu de la branche master est compil√© et une image docker est cr√©e pour √™tre d√©ploy√©e automatiquement dans GKE. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:1:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#worflows-impl√©ment√©s"},{"categories":null,"content":" 2 Analyse SONARJ‚Äôai choisi d‚Äôutiliser sonarcloud pour analyser mon code. C‚Äôest gratuit pour les projets opensource. L‚Äôanalyse se fait simplement: sonarCloudTrigger: name: SonarCloud Trigger runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: SonarCloud Scan env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }} run: ./gradlew jacocoTestReport sonarqube Dans ce job j‚Äôutilise deux secrets. Ce sont des tokens qui permettent de ne pas stocker en dur les donn√©es dans les repos GITHUB. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:2:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#analyse-sonar"},{"categories":null,"content":" 3 Cr√©ation d‚Äôune image Docker et d√©ploiement dans le registry GITHUBIci aussi, √ßa se fait simplement. La preuve : jobs: publish: runs-on: ubuntu-latest steps: - uses: actions/checkout@v1 - name: Set up JDK 11 uses: actions/setup-java@v1 with: java-version: 11 - name: Build in JVM Mode with Gradle without testing run: ./gradlew quarkusBuild [1] - name: Branch name run: echo running on branch ${GITHUB_REF##*/} - name: Build the Docker image Quarkus JVM run: docker build -f src/main/docker/Dockerfile.jvm -t docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest . [2] - name: Login against github docker repository env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: docker login -u ${GITHUB_ACTOR} -p ${GITHUB_TOKEN} docker.pkg.github.com [3] - name: Publish the Docker image Quarkus JVM run: docker push docker.pkg.github.com/${GITHUB_REPOSITORY}/music-quote-jvm:latest [4] Cr√©ation du binaire Cr√©ation de l‚Äôimage docker en utilisant la commande docker et le Dockerfile fourni par Quarkus Identification sur la registry Docker de GITHUB D√©ploiement de l‚Äôimage Pour plus de d√©tails sur la variable GITHUB_TOKEN, vous pouvez lire cet article de la documentation. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:3:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#cr√©ation-dune-image-docker-et-d√©ploiement-dans-le-registry-github"},{"categories":null,"content":" 4 D√©ploiement dans Google Kubernetes EngineMon application est pour l‚Äôinstant architectur√©e comme suit (attention c‚Äôest compliqu√©): Pour la d√©ployer dans Google Kubernetes Engine, j‚Äôai besoin d‚Äô impl√©menter cette ¬´¬†architecture¬†¬ª par les objets Kubernetes suivants: J‚Äôutilise les objets suivants: Des services pour exposer la base de donn√©es ainsi que l‚Äôapplication Un deployment pour l‚Äôapplication Des pods car √† un moment, il en faut‚Ä¶ Un statefulset pour la base de donn√©es Vous pourrez trouver la d√©finition de tous ces objets au format yaml via ce lien. J‚Äôai fait tr√®s simple. Logiquement j‚Äôaurai du cr√©er un volume pour les bases de donn√©es ou utiliser une base de donn√©es en mode PAAS. Pour lancer le d√©ploiement, il faut au pr√©alable cr√©er un secret ( fait manuellement pour ne pas stocker d‚Äôobjet yaml dans le repository GITHUB) pour se connecter au repo GITHUB via la commande suivante: kubectl create secret docker-registry github-registry --docker-server=docker.pkg.github.com --docker-username=USER--docker-password=PASSWORD --docker-email=EMAIL On peut faire pareil pour les connexions base de donn√©es. J‚Äôai mis dans un configmap pour ne pas trop me prendre la t√™te‚Ä¶ Apr√®s le d√©ploiement via le pipeline se fait assez simplement: [...] - uses: GoogleCloudPlatform/github-actions/setup-gcloud@master with: version: '286.0.0' service_account_email: ${{ secrets.GKE_SA_EMAIL }} service_account_key: ${{ secrets.GKE_SA_KEY }} project_id: ${{ secrets.GKE_PROJECT }} # Get the GKE credentials so we can deploy to the cluster - run: |- gcloud container clusters get-credentials \"${{ secrets.GKE_CLUSTER }}\" --zone \"${{ secrets.GKE_ZONE }}\" # Deploy the Docker image to the GKE cluster - name: Deploy run: |- kubectl apply -f ./k8s J‚Äôutilise les ¬´¬†actions¬†¬ª fournies par Google. ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:4:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#d√©ploiement-dans-google-kubernetes-engine"},{"categories":null,"content":" 5 ConclusionPour que √ßa marche il y a pas mal d‚Äô√©tapes pr√©alables ( des tokens √† g√©n√©rer, un utilisateur technique, ‚Ä¶). J‚Äôai essay√© de les r√©f√©rencer dans le README du projet. Si vous voulez tester l‚Äôint√©gration Kubernetes dans le cloud google, sachez que vous pouvez disposer d‚Äôun cr√©dit de 300‚Ç¨ valable un an. Attention, avec ce genre d‚Äôarchitecture, √ßa part vite‚Ä¶ ","date":"2020-05-10","objectID":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/:5:0","series":null,"tags":["github","gradle","kubernetes"],"title":"Utiliser des GITHUB Actions pour d√©ployer dans Google Kubernetes Engine","uri":"/2020/05/10/utiliser-des-github-actions-pour-deployer-dans-google-kubernetes-engine/#conclusion"},{"categories":null,"content":"Mon PC Lenovo a un SSD. Le temps de d√©marrage est actuellement de 11 sec. √áa commence √† faire pas mal‚Ä¶ J‚Äôai eu donc envie de me pencher sur l‚Äôoptimisation du d√©marrage ( encore une fois) . Voici comment gagner (facilement) quelques secondes au d√©marrage. Boot time Tout d‚Äôabord, vous devez analyser les services qui prennent du temps au d√©marrage. Vous pouvez le faire avec cette commande: systemd-analyze plot \u003e plot.svg J‚Äôai obtenu le graphique suivant: Boot initial ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:0:0","series":null,"tags":["debian","planetlibre"],"title":"Am√©liorer le temps de d√©marrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#"},{"categories":null,"content":" 1 Configuration GRUBLa premi√®re manipulation √† r√©aliser est de d√©sactiver le timeout de GRUB. Pour cel√†, vous pouvez modifier la variable GRUB_TIMEOUT dans le fichier /etc/default/grub: GRUB_TIMEOUT=0 Ensuite, vous devez mettre √† jour la configuration GRUB en ex√©cutant cette commande: sudo update-grub2 Au prochain reboot, vous ne verrez plus le menu GRUB. ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:1:0","series":null,"tags":["debian","planetlibre"],"title":"Am√©liorer le temps de d√©marrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-grub"},{"categories":null,"content":" 2 Configuration NetworkManagerDans mon cas, le service NetworkManager-wait-online.service prenait pr√®s de 9 secondes. Apr√®s avoir lu plusieurs billets et rapports de bug, je me suis aper√ßu que je pouvais le d√©sactiver au boot. Vous pouvez le faire en lan√ßant la commande suivante sudo systemctl disable NetworkManager-wait-online.service ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:2:0","series":null,"tags":["debian","planetlibre"],"title":"Am√©liorer le temps de d√©marrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-networkmanager"},{"categories":null,"content":" 3 Configuration AptUn autre service qui prenait pas mal de temps √©tait apt-daily.timer qui v√©rifiait au boot qu‚Äôil y avait des mises √† jour de l‚ÄôOS. Apr√®s quelques recherches, j‚Äô ai vu qu‚Äôon pouvait soit le d√©sactiver ( ce qui n‚Äôest pas recommand√© pour les mises √† jour de s√©curit√© ) soit d√©caler la recherche. J‚Äôai choisi cette solution. Vous devez donc ex√©cuter la commande suivante: sudo systemctl edit apt-daily.timer Et renseigner le contenu suivant: [Timer] OnBootSec=15min OnUnitActiveSec=1d AccuracySec=1h RandomizedDelaySec=30min Ce service sera donc lanc√© 15 minutes apr√®s le boot. Ce qui est largement suffisant. [EDIT] Vous pouvez appliquer la m√™me configuration pour le service apt-daily-upgrade en ex√©cutant la commande: sudo systemctl edit apt-daily-upgrade.timer Ensuite, vous pouvez recharger la configuration en ex√©cutant cette commande: sudo systemctl daemon-reload ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:3:0","series":null,"tags":["debian","planetlibre"],"title":"Am√©liorer le temps de d√©marrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#configuration-apt"},{"categories":null,"content":" 4 R√©sultatsApr√®s ces quelques manipulations qui peuvent prendre 5 minutes grand maximum, j‚Äôai r√©ussi √† optimiser le boot en r√©duisant le d√©marrage √† 5 secondes! Boot (apr√®s) Vous pourrez trouver le d√©tail ci-dessous: D√©tail du Boot (apr√®s) ","date":"2020-04-24","objectID":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/:4:0","series":null,"tags":["debian","planetlibre"],"title":"Am√©liorer le temps de d√©marrage de Debian 10","uri":"/2020/04/24/ameliorer-le-temps-de-demarrage-de-debian-10/#r√©sultats"},{"categories":null,"content":"Avec les contraintes li√©es au confinement, les r√©p√©titions se font de plus en plus rares. Pour ne pas perdre la main, il y a quelques logiciels qui permettent de jouer d‚Äôun instrument et d‚Äô improviser tout en ayant une bande son en fond musical. Il y a plusieurs logiciels payants/propri√©taires sur diff√©rentes plateformes: Band in a box irealpro Garage band Jjazzlabs J‚Äôai d√©couvert ce dernier r√©cemment en naviguant sur le site Linux Mao. Il a l‚Äôavantage d‚Äô√™tre gratuit (le moteur est sous licence LGPL 3.0, pas le logiciel en tant que tel), de fonctionner sous GNU/LINUX, d‚Äô offrir un son pas mal du tout et de permettre la configuration de la dynamique au fur et √† mesure du morceau. Je vais expliquer comment l‚Äôinstaller sur Debian. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:0:0","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#"},{"categories":null,"content":" 1 Configuration Midi","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:0","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration-midi"},{"categories":null,"content":" 1.1 Activation des p√©riph√©riques virtuels MIDICr√©er un fichier /etc/modules.load.d/midi.conf avec le contenu suivant: snd-virmidi Ensuite cr√©er le fichier /etc/modprobe.d/midi.conf avec le contenu suivant: options snd-virmidi midi_devs=1 Logiquement √† ce stade, lors du prochain reboot, vous aurez un p√©riph√©rique virtuel MIDI activ√©. En attendant vous pouvez lancer la commande suivante $ sudo modprobe snd-virmidi midi_devs=1 ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:1","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#activation-des-p√©riph√©riques-virtuels-midi"},{"categories":null,"content":" 1.2 Synth√©tiser du MIDIPour faire fonctionner ce logiciel, il faut installer une banque de son ( au format SF2) et un logiciel permettant de l‚Äôutiliser pour synth√©tiser du MIDI. La banque de son recommand√©e est disponible via ce lien. T√©l√©chargez l√† et copiez la dans un r√©pertoire accessible. Pour le second, il vous faudra installer fluidsynth. Voic les quelques commandes √† lancer: $ sudo apt install fluid-synth qsynth ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:2","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#synth√©tiser-du-midi"},{"categories":null,"content":" 1.3 Petite v√©rification‚Ä¶Avant d‚Äô aller plus loin dans la configuration de fluidsynth, vous pouvez vous assurer que tout est OK en r√©cup√©rant un fichier MIDI et en lan√ßant la commande suivante: $ fluidsynth -a pulseaudio -m alsa_seq -l -i /opt/JJazzLab-2.0-Linux/JJazzLab-SoundFont.sf2 MIDI_sample.mid Normalement vous devriez avoir du son. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:3","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#petite-v√©rification8230"},{"categories":null,"content":" 1.4 Configurer fluidsynthLancez qsynth et cliquez sur le bouton ¬´¬†configuration¬†¬ª Vous trouverez ci-dessous la configuration que j‚Äôai appliqu√©. Elle diff√®re l√©g√®rement de celle pr√©sent√©e dans la documentation. Pensez √† red√©marrer fluidsynth apr√®s application de ces nouveaux param√®tres. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:4","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-fluidsynth"},{"categories":null,"content":" 1.5 Configurer aconnectDisclaimer: La c‚Äôest la partie la plus obscure‚Ä¶ Il faut maintenant ¬´¬†brancher¬†¬ª la sortie du synth√©tiseur virtuel MIDI √† fluidsynth pour que le son MIDI soit interpr√©t√© par ce dernier √† travers sa banque de son. Ce n‚Äôest pas intuitif, je vous avais pr√©venu ‚Ä¶ Je ne vous parle pas de la pseudo interface graphique √† aconnect. La ligne de console est plus parlante ( c‚Äôest pour dire ) . Ex√©cutez la commande suivante: $ aconnect -lo client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Dans mon cas, je vais avoir √† connecter le client 24:0 au synth√©tiseur 128:0 gr√¢ce √† la commande : $ aconnect 24:0 128:0 Maintenant, si on relance la commande aconnect -lo on obtient le r√©sultat suivant: client 14: 'Midi Through' [type=noyau] 0 'Midi Through Port-0' client 24: 'Virtual Raw MIDI 2-0' [type=noyau,card=2] 0 'VirMIDI 2-0 ' Connexion √Ä: 128:0 client 128: 'FLUID Synth (JJLAB)' [type=utilisateur,pid=17838] 0 'Synth input port (JJLAB:0)' Connect√© Depuis: 24:0 Attention, cette commande devra √™tre lanc√©e ( ainsi que fluidsynth) avant chaque d√©marrage de jjazzlab. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:1:5","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configurer-aconnect"},{"categories":null,"content":" 2 Installation de JjazzlabT√©l√©chargez les binaires sur ce site, puis d√©compressez l‚Äôarchive dans le r√©pertoire /opt par ex. Vous devez √©galement installer java $ sudo apt install openjdk-11-jdk Ensuite, vous devez cr√©er le fichier ~/.local/share/applications/jjazzlab.desktop avec le contenu suivant: [Desktop Entry] Type=Application Name=JJazzLab GenericName=JJazzLab Icon= Exec=\"/opt/JJazzLab-2.0-Linux/bin/jjazzlab\" Terminal=false Categories=Audio;Music;Player;AudioVideo; Maintenant vous pouvez directement d√©marrer JJazzlab via le menu. ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:2:0","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#installation-de-jjazzlab"},{"categories":null,"content":" 3 ConfigurationUne fois jjazzlab d√©marr√©, vous devez aller dans le menu ¬´¬†Tools\u003eOptions¬†¬ª et s√©lectionnez les valeurs suivantes: Ouvrez un fichier example (ex. sunny ) Cliquez sur le menu d√©crit par un clavier Puis configurez comme suit: Maintenant vous pouvez t√©l√©charger les standards fournis sur le site et improviser dessus üôÇ ","date":"2020-04-12","objectID":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/:3:0","series":null,"tags":["debian","musique","planetlibre"],"title":"R√©p√©ter avec JjazzLab tout seul dans son garage","uri":"/2020/04/12/repeter-avec-jjazzlab-tout-seul-dans-son-garage/#configuration"},{"categories":null,"content":"Voici un rapide article sur un probl√®me rencontr√© r√©cemment. Lors de l‚Äôex√©cution d‚Äôun container docker, j‚Äôai eu une erreur SIGSEGV 139. Un crash avec aucune log. Bref que du bonheur üôÇ Avant d‚Äôaller plus loin voici mon environnement: Debian 10 Docker CE 19.03.8 Apr√®s quelques recherches, je me suis rendu compte qu‚Äôon pouvait reproduire ce comportement en ex√©cutant cette commande: docker run -it gcc:4.8.5 Une des raisons trouv√©es serait un probl√®me de compatibilit√© avec le noyau 4.8.5 (oui √ßa remonte‚Ä¶). Une solution est d‚Äôactiver l‚Äô√©mulation vsyscall. Voici la configuration √† effectuer: Dans le fichier /etc/default/grub, ajouter la ligne suivante: GRUB_CMDLINE_LINUX_DEFAULT=\"quiet vsyscall=emulate\" Puis lancer les commandes suivantes: $ sudo update-grub $ sudo reboot Maintenant le container devrait pouvoir s‚Äôex√©cuter correctement. ","date":"2020-04-01","objectID":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Erreur 139 √† l'ex√©cution d'un container docker","uri":"/2020/04/01/erreur-139-a-lexecution-dun-container-docker/#"},{"categories":null,"content":"Suite aux premi√®res annonces de distanciation sociale ( avant que le confinement soit effectif ) j‚Äôai achet√© en catastrophe un PC portable. Les crit√®res √©taient : 8Go de RAM, un disque SSD ‚Ä¶ et la compatibilit√© GNU/LINUX :). N‚Äôayant pas trop de temps pour chercher la bonne affaire ( technologique et financi√®re ), j‚Äô ai achet√© un Dell Inspiron 14-3493. Je n‚Äôai pas pris trop de risques. Bien que livr√© avec Windows 10, ce mod√®le est d√©j√† certifi√© compatible Ubuntu. L‚Äôinstallation d‚ÄôUbuntu se passe tr√®s bien. C‚Äôest pli√© en moins de 30mn. Du coup, je ne la d√©taillerai pas dans cet article ‚Äì si vous √™tes int√©ress√©, vous pouvez consulter cet article. Pour les pr√©-requis, c‚Äôest une autre paire de manches ‚Ä¶ Voil√† les diff√©rentes actions que j‚Äôai r√©alis√© au pr√©alable ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:0:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#"},{"categories":null,"content":" 1 Red√©marrer l‚Äôordinateur et acc√©der au BIOSL√†, j‚Äôai un peu gal√©r√© pour acc√©der au BIOS. La seule manipulation que j‚Äôai trouv√© et de lancer le menu ¬´¬†D√©marrage avanc√©¬†¬ª puis s√©lectionner ¬´¬†Utiliser un p√©riph√©rique¬†¬ª. Vous pouvez donc s√©lectionner le disque dur. Au boot en appuyant sur la touche F12 et/ou F2, vous pouvez acc√©der au BIOS. ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:1:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#red√©marrer-lordinateur-et-acc√©der-au-bios"},{"categories":null,"content":" 2 Configuration du BIOSVoila les param√®tres que j‚Äôai appliqu√©: Dans le menu ‚ÄúSATA Operation‚Äù: vous devez s√©lectionner AHCI au lieu de RAID. Dans le menu ‚ÄúChange boot mode settings \u003eUEFI Boot Mode‚Äù , vous devez d√©sactiver le Secure Boot. Une fois r√©alis√©, vous pouvez red√©marrer en appuyant sur la touche F2 et/ou F12. Si vous n‚Äôarrivez pas √† revenir sur le BIOS pour indiquer de booter sur votre cl√© USB, vous obtiendrez un √©cran d‚Äôerreur Windows d√ª √† la configuration AHCI. Personnellement, en red√©marrant une ou deux fois, j‚Äôai obtenu un √©cran de d√©marrage avanc√© qui m‚Äôa permis de s√©lectionner le p√©riph√©rique (ma cl√© USB) sur lequel d√©marrer. Maintenant vous pouvez acc√©der √† l‚Äôinstalleur Ubuntu et profiter üôÇ ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:2:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#configuration-du-bios"},{"categories":null,"content":" 3 Apr√®s l‚ÄôinstallationJe n‚Äôai rien fait de particulier si ce n‚Äôest configurer le trackpad. Pour cela, j‚Äôai install√© gnome-tweaks. Mis √† part √ßa, tout fonctionne tr√®s bien! ","date":"2020-03-23","objectID":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/:3:0","series":null,"tags":["planetlibre","ubuntu"],"title":"Installer Ubuntu 18.04 LTS sur un Dell Inspiron 14-3493","uri":"/2020/03/23/installer-ubuntu-18-04-lts-sur-un-dell-inspiron-14-3493/#apr√®s-linstallation"},{"categories":null,"content":"Java 8 est encore largement utilis√© dans les entreprises aujourd‚Äôhui. Il y a m√™me certains frameworks qui n‚Äôont pas encore saut√© le pas. Je vais essayer d‚Äôexposer dans cette article les √©tapes √† r√©aliser pour migrer (simplement) votre application JAVA8 en JAVA 11. Dans cet article, je prendrai comme postulat que l‚Äôapplication se construit avec Maven. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:0:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#"},{"categories":null,"content":" 1 Pr√©-requisTout d‚Äôabord v√©rifiez votre environnement d‚Äôex√©cution cible! Faites un tour du cot√© de la documentation et regardez le support de JAVA. Si vous utilisez des FRAMEWORKS qui utilisent des FAT JARS, faites de m√™me (ex. pour spring boot, utilisez au moins la version 2.1.X). Ensuite, vous aurez sans doute √† mettre √† jour maven ou gradle. Pr√©f√©rez les derni√®res versions. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:1:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#pr√©-requis"},{"categories":null,"content":" 2 Configuration mavenLes trois plugins √† mettre √† jour obligatoirement sont : maven-compiler-plugin maven-surefire-plugin maven-failsafe-plugin ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#configuration-maven"},{"categories":null,"content":" 2.1 Maven compiler plugin \u003cplugin\u003e \u003cartifactId\u003emaven-compiler-plugin\u003c/artifactId\u003e \u003cversion\u003e3.8.1\u003c/version\u003e \u003cconfiguration\u003e \u003crelease\u003e11\u003c/release\u003e \u003cencoding\u003eUTF-8\u003c/encoding\u003e \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:2:1","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-compiler-plugin"},{"categories":null,"content":" 3 maven surefire / failsafe pluginPour ces deux plugins, ajouter la configuration suivante: \u003cplugin\u003e \u003cartifactId\u003emaven-surefire-plugin\u003c/artifactId\u003e \u003cversion\u003e2.22.2\u003c/version\u003e \u003cconfiguration\u003e [...] \u003cargLine\u003e--illegal-access=permit\u003c/argLine\u003e [...] \u003c/configuration\u003e \u003c/plugin\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:3:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#maven-surefire--failsafe-plugin"},{"categories":null,"content":" 4 Mise √† jour des librairiesBon,la il n‚Äôy a pas de magie. Vous devez mettre √† jour toutes vos librairies. Mis √† part si vous utilisez des librairies exotiques, la plupart supportent JAVA 11 maintenant. C‚Äôest une bonne opportunit√© de faire le m√©nage dans vos fichiers pom.xml üôÇ ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:4:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#mise-√†-jour-des-librairies"},{"categories":null,"content":" 5 APIS supprim√©es du JDKSi vous faites du XML, SOAP ou que vous utilisiez l‚ÄôAPI activation, vous devez d√©sormais embarquer ces librairies. Le JDK ne les inclut plus par d√©faut. Par exemple: \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-core\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.sun.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-impl\u003c/artifactId\u003e \u003cversion\u003e2.3.0.1\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003ejavax.xml.bind\u003c/groupId\u003e \u003cartifactId\u003ejaxb-api\u003c/artifactId\u003e \u003cversion\u003e2.3.1\u003c/version\u003e \u003c/dependency\u003e ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:5:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#apis-supprim√©es-du-jdk"},{"categories":null,"content":" 6 Modularisation avec JIGSAWBon l√† ‚Ä¶ je vous d√©conseille de partir directement sur la modularisation, surtout si vous migrez une application existante. Bien que la modularit√© puisse aider √† r√©duire vos images docker en construisant vos propres JRE et d‚Äôam√©liorer la s√©curit√©, elle apporte son lot de complexit√©. Bref pour la majorit√© des applications, je vous d√©conseille de l‚Äôint√©grer. ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:6:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#modularisation-avec-jigsaw"},{"categories":null,"content":" 7 ConclusionAvec toutes ces manipulations, vous devriez pouvoir porter vos applications sur JAVA11. Il y aura sans doute quelques bugs. Personnellement, j‚Äôen ai eu avec CGLIB vs Spring AOP sur une classe instrument√©e avec un constructeur priv√©. Sur ce coup j‚Äôai contourn√© ce probl√®me ( je vous laisse deviner comment üôÇ ). ","date":"2020-02-03","objectID":"/2020/02/03/passer-votre-application-java8-en-java11/:7:0","series":null,"tags":["java","planetlibre"],"title":"Passer votre application Java8 en Java11","uri":"/2020/02/03/passer-votre-application-java8-en-java11/#conclusion"},{"categories":null,"content":"Depuis quelques temps je me mets √† Gradle. Apr√®s de (trop?) nombreuses ann√©es √† utiliser Maven (depuis la version 0.9‚Ä¶), je me risque √† modifier mon environnement de build. Du moins sur des projets d√©mo. Quand on a fait pas mal de Maven, on est un peu d√©rout√© au d√©but. On a d‚Äôun cot√©, la plupart des actions qui sont configur√©es de mani√®re implicite et de l‚Äôautre on peut tout coder/√©tendre ou presque. Je ne vais pas me risquer √† faire un comparatif des deux outils. Gradle ( donc fortement orient√© ) en a fait un. Je vais plut√¥t d√©crire avec cet article comment on peut d√©marrer rapidement en configurant son environnement pour √™tre utilis√© en entreprise. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:0:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#"},{"categories":null,"content":" 1 InstallationLe plus simple est d‚Äôutiliser SDKMAN. Voici la manipulation pour l‚Äôinstaller: $ curl -s \"https://get.sdkman.io\" | bash $ source \"$HOME/.sdkman/bin/sdkman-init.sh\" $ sdk install gradle 6.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:1:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#installation"},{"categories":null,"content":" 2 Configuration d‚Äôun proxyEt oui comment souvent, passer le proxy d‚Äôentreprise est la moiti√© du boulot :). Pour le configurer de mani√®re globale (c.-√†-d. pour tous vos projets) sur votre poste de travail, vous devez cr√©er un fichier gradle.properties dans le r√©pertoire $HOME/.gradle : systemProp.http.proxyHost=proxy systemProp.http.proxyPort=8888 systemProp.http.nonProxyHosts=localhost|127.0.0.1 systemProp.https.proxyHost=proxy systemProp.https.proxyPort=8888 systemProp.https.nonProxyHosts=localhost|127.0.0.1 ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:2:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-proxy"},{"categories":null,"content":" 3 Configuration d‚Äôun miroir Nexus ou ArtifactoryA l‚Äôinstar du proxy, on va essayer de mettre en place une configuration globale. Pour ce faire, on va utiliser les init scripts. Cette fonctionnalit√© est tr√®s int√©ressante. Elle permet de centraliser des actions et configurations. Pour cr√©er un script, il faut tout d‚Äôabord cr√©er un fichier .gradle dans le r√©pertoire $HOME/.gradle/init.d. Voici un exemple pour Nexus: allprojects { buildscript { repositories { mavenLocal() maven {url \"https://url-nexus\"} } } repositories { mavenLocal() maven { url \"https://url-nexus\"} } } ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:3:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-dun-miroir-nexus-ou-artifactory"},{"categories":null,"content":" 4 Configuration du d√©ploiement dans Nexus / ArtifactoryLe d√©ploiement dans Nexus est possible via le plugin maven publish. La configuration fournie dans la documentation est tellement bien faite ( comme le reste d‚Äôailleurs ) que je ne vais que mettre un lien vers celle-l√†: Voici le lien. ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:4:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#configuration-du-d√©ploiement-dans-nexus--artifactory"},{"categories":null,"content":" 5 ConclusionApr√®s ces quelques actions vous pourrez d√©marrer des builds avec gradle tout en √©tant compatible avec un environnement ¬´¬†Maven¬†¬ª. Enjoy üôÇ ","date":"2019-12-30","objectID":"/2019/12/30/premiers-pas-avec-gradle/:5:0","series":null,"tags":["gradle","java"],"title":"Premiers pas avec Gradle","uri":"/2019/12/30/premiers-pas-avec-gradle/#conclusion"},{"categories":null,"content":"Je suis en train de mettre en ≈ìuvre des tests de performance avec Gatling. Un des principaux outils libres de tests de performance. J‚Äôai eu r√©cemment √† r√©soudre un ¬´¬†petit¬†¬ª soucis : je souhaitai partager des variables entre plusieurs sc√©narios. Il existe pas mal de solutions sur stackoverflow. J‚Äôai condens√© certaines d‚Äôentre elles pour les adapter √† mon besoin. Ces variables sont issues de ex√©cution d‚Äôune seule requ√™te et sont automatiquement inject√©es dans les sc√©narios suivants. Ce m√©canisme permet par exemple de r√©cup√©rer un jeton d‚Äôun serveur d‚Äôidentification et de l‚Äôinjecter pour le sc√©nario que l‚Äôon souhaite tester. Pour ce faire, il faut ajouter une variable de type LinkedBlockingDeque et injecter le contenu choisi via la session val holder = new LinkedBlockingDeque[String]() ... val firstScenario = scenario(\"First Simulation\") .exec(http(\"first scenario\") .post(\"/base/url1\") .check(jsonPath(\"$.my_variable\").find.saveAs(\"variable\"))) .exec(session =\u003e { holder.offerLast(session(\"variable\").as[String]) session} ); Maintenant on peut l‚Äôutiliser dans un autre sc√©nario comme feeder: val secondScenario = scenario(\"Second Simulation\") .feed(sharedDataFeeder) Voici l‚Äôexemple complet En esp√©rant que cela puisse aider √† certain.e.s d‚Äôentre vous üôÇ ","date":"2019-11-21","objectID":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/:0:0","series":null,"tags":["gatling","planetlibre","scala"],"title":"Partager des variables entre sc√©narios gatling","uri":"/2019/11/21/partager-des-variables-entre-scenarios-gatling/#"},{"categories":null,"content":"Une fois n‚Äôest pas coutume, voici un article qui reprend des basiques de la programmation. J‚Äôaborde une stack JAVA, mais c‚Äôest applicable √† d‚Äôautres langages. Il existe une fonctionnalit√© tr√®s int√©ressante dans Spring (et dans J(akarta)EE) que l‚Äôon oublie assez souvent : l‚ÄôAOP ou encore la programmation par aspect. Cette mani√®re de programmer permet notamment de s√©parer le code fonctionnel et technique. Si vous faites du JAVA, vous utilisez d√©j√† l‚ÄôAOP. En effet, quand vous faites une insertion en base via JPA dans un EJB ou un bean annot√© @Transactional, une transaction est initi√©e au d√©but de la m√©thode et ferm√©e √† la fin. Avec Spring et notamment dans Spring boot, voici comment initier l‚ÄôAOP. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:0:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#"},{"categories":null,"content":" 1 Configuration mavenAjouter le starter AOP: \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-aop\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:1:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#configuration-maven"},{"categories":null,"content":" 2 Activation des aspectsDans la configuration ci-dessous, je prendrai comme exemple le logging des m√©thodes ( un log en d√©but de m√©thode et un log en fin ).¬†La d√©finition des aspects se fait dans des classes annot√©es par @Configuration. @Configuration @Aspect @ConditionalOnProperty(name = \"debug.enabled\", havingValue = \"true\") public class DebuggingConfiguration { private static final Logger LOGGER = LoggerFactory.getLogger(DebuggingConfiguration.class); private static final String WITHIN_MY_PACKAGE = \"within(my.package..*)\"; /** * Log before execution * * @param joinPoint the current method */ @Before(WITHIN_MY_PACKAGE) public void logBeforeExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"Beginning of method : [{}]\", joinPoint.getSignature().getName()); } } /** * Log after execution * * @param joinPoint the current method */ @After(WITHIN_MY_PACKAGE) public void logAfterExecution(JoinPoint joinPoint) { if (LOGGER.isTraceEnabled()) { LOGGER.trace(\"End of method : [{}]\", joinPoint.getSignature().getName()); } } } L‚Äôutilisation de l‚Äô annotation @ConditionalOnProperty me permet d‚Äôactiver cette classe de configuration seulement si la propri√©t√© debug.enabled est initialis√©e √† true. Les annotations @Before et @After indiquent √† Spring AOP quand ex√©cuter ces m√©thodes ou sur quelles m√©thodes. Dans mon cas, quand les m√©thodes appel√©es sont d√©finies dans les classes d‚Äôun package d√©fini. Pour plus de d√©tails sur la syntaxe et les possibilit√©s, vous pouvez vous r√©f√©rer √† la documentation. ","date":"2019-11-05","objectID":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/:2:0","series":null,"tags":["aop","java","planetlibre","spring","springboo"],"title":"Programmmation par aspect avec Spring AOP","uri":"/2019/11/05/programmmation-par-aspect-avec-spring-aop/#activation-des-aspects"},{"categories":null,"content":"Apr√®s avoir soumis mon article sur le coaching des d√©veloppeurs, je me suis rendu compte que j‚Äôai oubli√© pas mal de points qui, √† bien y r√©fl√©chir, me paraissent essentiels. Dans mon pr√©c√©dent article ( the first blood pour le coup ) je me suis attard√© sur le ¬´¬†quoi¬†¬ª : toutes les actions que j‚Äôai test√© dans l‚Äôencadrement des jeunes d√©veloppeurs et des d√©veloppeurs en g√©n√©ral. Maintenant, je vais essayer de m‚Äôattarder sur le ¬´¬†comment¬†¬ª : ma d√©marche, la posture que l‚Äôon doit adopter ( ce n‚Äôest que mon ressenti ) etc. Je vais commencer par ce dernier point. Quand on est architecte, d√©veloppeur s√©nior ou bien encore tech lead, on est amen√© √† encadrer techniquement des d√©veloppeurs. Vous pouvez adopter plusieurs postures: A ce stade de lecture de cet article, vous vous dites, quelle est la bonne photo et donc la posture √† adopter ? A mon avis, elles sont √† proscrire individuellement. Je pense qu‚Äôil faut les panacher. Tout d‚Äôabord, il faut se souvenir de notre d√©but de carri√®re et se rappeler du code que l‚Äôon a r√©alis√©. J‚Äôai par exemple gard√© les premiers programmes r√©alis√©s en entreprise ( Servlet, JSP, JAVA 1.2, des m√©thodes de 3km de long, de la duplication de code en veux tu en voila, ‚Ä¶) . √áa me permet de relativiser, d‚Äô√™tre assez compr√©hensif et d‚Äô√©viter de prendre les gens de haut. Cependant, cette prise de conscience ne doit pas vous emp√™cher de faire progresser votre entourage et surtout de leur faire √©viter les √©cueils que vous avez v√©cu. Les ateliers et documentation que vous pourrez leur transmettre sont donc primordiaux. Par exemple, faire lire ¬´¬†Clean Code¬†¬ª ou ¬´¬†Effective Java¬†¬ª aux d√©veloppeurs ‚Äì je ne l‚Äôoblige pas mais incite fortement ‚Äì est un moyen de leur faire gagner du temps dans leur apprentissage du code. Ensuite, m√™me si vos padawans vous voient soit comme Pascal le grand fr√®re ou ma√Ætre Yoda (pour flatter mon √©go), il ne faut pas oublier les exigences que vous avez fix√©. L‚Äôindustrie logicielle a gagn√©e en maturit√© en favorisant par exemple l‚Äôindustrialisation via les outils de CI/CD ou bien encore en facilitant l‚Äôapplication de principes de qualit√© via des outils d‚Äôanalyse des d√©pendances (dependency track) et du code (sonarqube). Vous devez vous adapter, favoriser l‚Äôadoption de ces pratiques et imposer quelques √©tapes qualit√© de pr√©f√©rence automatis√©e via de la CI. Pour favoriser l‚Äôadoption de toutes vos exigences, je conseille d‚Äôy aller progressivement. Il ne faut pas oublier que votre objectif est de faire ¬´¬†grandir¬†¬ª vos coll√®gues. Pour cela essayez de les adapter et les faire √©voluer dans le temps. Par exemple, pour les tests unitaires, commencez pas mettre en place les diff√©rents indicateurs qui vous permettront de mesurer la couverture de code. Ensuite, exigez un niveau de couverture de code (ex. 30%). Suivez le, via les quality gates SonarQube et enfin augmentez le progressivement : 30% , 40%,‚Ä¶ Si vous commencez d√®s le d√©but par un objectif trop haut, ce dernier para√Ætra inatteignable et d√©couragera tout le monde. Mieux vaut commencer volontairement tr√®s bas pour favoriser l‚Äôadoption. Dans un autre domaine, pour vos workflows GIT, vous pouvez commencer dans un premier temps par le workflow de feature branch. Ce dernier posera les bases des pipelines CI, des merge requests et des bonnes pratiques li√©es √† la gestion de configuration. Une fois tout le c√©r√©monial li√© √† GIT assimil√© par votre √©quipe, passer √† GITFLOW sera beaucoup simple. Bref, cette d√©marche revient √† parler de conduite du changement. Il faut identifier vos exigences minimales. Celles-ci doivent √™tre accept√©es par votre hi√©rarchie ET par vos coll√®gues. Sans √ßa vous √©chouerez! Si ils vous soumettent quelques id√©es ou adaptations, n‚Äôh√©sitez pas √† les incorporer. √áa peut faciliter l‚Äôadoption! Ensuite, planifiez une progression sur 1 ou 2 ans. Cela donnera √† vos coll√®gues dans un premier temps des premiers objectifs atteignables puis une marge de progression leur permettant de s‚Äôam√©liorer. Enfin, n‚Äôh√©sitez pas √† faire un bilan ( par ex. a","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ? The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#"},{"categories":null,"content":" 1 ConclusionA mon avis le management et l‚Äôencadrement de personnes n‚Äôest pas √† prendre √† la l√©g√®re. Votre attitude ainsi que la d√©marche que vous voulez mettre en ≈ìuvre feront autant voir plus que toute la documentation et formations que vous mettrez en place. ","date":"2019-09-11","objectID":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ? The last blood","uri":"/2019/09/11/comment-coacher-des-jeunes-developpeurs-the-last-blood/#conclusion"},{"categories":null,"content":"Auparavant, dans nos tests, quand on voulait mocker des m√©thodes ¬´¬†final¬†¬ª ou statiques, on devait passer par PowerMock. Depuis peu, si on utilise Mockito ( \u003e2.1) , on n‚Äôa plus besoin d‚Äôajouter PowerMock pour mocker des m√©thodes ¬´¬†final¬†¬ª. Bon il reste toujours la gestion des m√©thodes statiques √† g√©rer autrement qu‚Äôavec Mockito, mais cela va dans le bon sens. Voici comment activer en quelques commandes le mocking des m√©thodes ¬´¬†final¬†¬ª. Dans le r√©pertoire src/test/resources, il faut cr√©er un r√©pertoire mockito-extensions avec un fichier nomm√© org.mockito.plugins.MockMaker. src/test/resources ‚îî‚îÄ‚îÄ mockito-extensions ‚îî‚îÄ‚îÄ org.mockito.plugins.MockMaker A l‚Äôint√©rieur de ce fichier, vous devrez ajouter le contenu suivant : mock-maker-inline Avec cette configuration, vous pourrez dor√©navant mocker des m√©thodes ¬´¬†final¬†¬ª üôÇ Enjoy ","date":"2019-08-16","objectID":"/2019/08/16/mocker-des-methodes-final-avec-mockito/:0:0","series":null,"tags":["java","mockito","planetlibre","tests-unitaires"],"title":"Mocker des m√©thodes ¬´¬†final¬†¬ª avec Mockito","uri":"/2019/08/16/mocker-des-methodes-final-avec-mockito/#"},{"categories":null,"content":"Juste pour un pense b√™te, voici comment param√©trer GIT et GITHUB/GITLAB pour signer les commits avec GPG. ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:0:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"V√©rifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#"},{"categories":null,"content":" 1 Configuration GPGEx√©cutez la commande suivante : gpg --full-generate-key S√©lectionnez une cl√© RSA (question 1) de 4096 bits (question 2). Une fois cette commande effectu√©e, vous pouvez r√©cup√©rer votre cl√© GPG avec cette commande: gpg --list-secret-keys --keyid-format LONG alexandre@.... /home/alexandre/.gnupg/pubring.kbx ---------------------------------- sec rsa4096/XXXXXXXXXX 2019-08-09 [SC] XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX uid [ ultime ] Alexandre Touret \u003cmon.mail.github.ou.gitlab@monprovider.fr\u003e ssb rsa4096/XXXXXXXXXX 2019-08-09 [E] Ensuite, il faut ex√©cuter cette commande gpg --armor --export XXXXXXXXXX ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:1:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"V√©rifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-gpg"},{"categories":null,"content":" 2 Configuration GITIndiquez la cl√© GPG √† GIT git config --local user.signingkey XXXXXXXXXXXX Et indiquez que vous voulez signer tous vos commits git¬†config --local commit.gpgsign true Si vous ne faites pas cette derni√®re commande, vous devrez ajouter l‚Äôoption -S √† chaque ex√©cution de la commande git commit. Exemple: git -a -S -m \"Ajout javadoc\" ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:2:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"V√©rifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-git"},{"categories":null,"content":" 3 Configuration GITHUBSur Github ( il y a la m√™me chose sur gitlab), vous pouvez dans vos param√®tres ajouter cette cl√© . De cette mani√®re, vos prochains commits envoy√©s seront v√©rifi√©s. En esp√©rant que √ßa serve √† d‚Äôautres üôÇ ","date":"2019-08-09","objectID":"/2019/08/09/verifier-les-commit-git-avec-gpg/:3:0","series":null,"tags":["git","github","gitlab","gpg","planetlibre"],"title":"V√©rifier les commit GIT avec GPG","uri":"/2019/08/09/verifier-les-commit-git-avec-gpg/#configuration-github"},{"categories":null,"content":"En changeant de soci√©t√© l‚Äôann√©e derni√®re j‚Äôai eu l‚Äôimpression de monter d‚Äôun cran dans la pyramide des ages. Pour faire plus simple, je me suis senti un peu plus vieux. Si vous avez quelques ann√©es d‚Äôexp√©rience dans le d√©veloppement ou tout simplement dans la technique, vous avez d√©j√† eu l‚Äôoccasion de coacher ou d‚Äôencadrer techniquement des jeunes dipl√¥m√©s. Et oui, c‚Äôest un signe ! Maintenant vous avez assez de recul ( pour ne pas dire que vous √™tes vieux/vieille) pour encadrer techniquement des jeunes ing√©nieur.e.s Certes vous n‚Äôavez pas fait le choix de partir vers la gestion de projet ou le management. Cependant l‚Äôencadrement technique ( vous pouvez l‚Äôappeler mentorat, tutorat, apprentissage,‚Ä¶ ) est n√©cessaire pour faire monter en comp√©tence les nouveaux arrivants et les rendre autonomes. Je vais essayer de mettre en lumi√®re quelques pratiques que je mets en ≈ìuvre et que j‚Äôai pu remettre au go√ªt du jour depuis un an. Si vous avez des id√©es, avis, n‚Äôh√©sitez pas √† les mettre en commentaire. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:0:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#"},{"categories":null,"content":" 1 DocumentationIl y a plusieurs types de documentation que je partage. Tout d‚Äôabord, j‚Äôai partag√© quelques sites et ouvrages qui me paraissent indispensables. Clean¬†Code arrive en premier. Effective Java en second. A mon avis, √ßa ne sert pas √† grand chose d‚Äôaller plus loin dans le d√©veloppement si on n‚Äôa pas acquis les notions d√©crites dans ces livres! Puis vient le refactoring puis les design patterns. Ensuite, j‚Äôessaye de partager via notre chat interne les quelques solutions trouv√©es dans les projets. Enfin, j‚Äô essaye de m‚Äô astreindre √† mettre √† jour la documentation. Oui c‚Äôest un combat de tous les jours üòÄ √áa commence par les exemples de code. J‚Äôessaye d‚Äô avoir des repos git assez lisibles (c.-√†-d. avec un README intelligible) et un code √† jour correspondant aux normes en vigueur. Un exemple, j‚Äôai cr√©e un projet permettant d‚Äô illustrer la mise en ≈ìuvre des tests unitaires et d‚Äôint√©gration dans un projet standard (spring, tomcat, docker,‚Ä¶). Ces √©l√©ments n√©cessitent un travail important, que √ßa soit √† la cr√©ation ou pour tenir √† jour la documentation. Cependant, √ßa me permet de ne pas me r√©p√©ter, et d‚Äô illustrer via un cas pratique ce que j‚Äôattends dans les Merge Requests. En effet, chaque d√©veloppement est assujetti √† une Definition of Done ( tests, qualit√©, ‚Ä¶) . Il faut donc que la qualit√© de la documentation soit en rendez vous ! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:1:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#documentation"},{"categories":null,"content":" 2 VeilleAu del√† de la documentation, je ¬´¬†pousse¬†¬ª aux diff√©rents dev, les articles que je trouve pertinent pendant ma veille technologique. J‚Äôinvite √©galement tout le monde √† en faire. Je ne peux pas les obliger. Maintenant comme je peux le dire r√©guli√®rement. Si on souhaite rester dans la technique, il faut se tenir √† jour. La veille (sites web, confs, livres,‚Ä¶) en est le meilleur moyen. ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:2:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#veille"},{"categories":null,"content":" 3 Ateliers / WorkshopsOrganiser un workshop ou atelier d‚Äôune heure ou deux max est un bon moyen de f√©d√©rer les troupes. J‚Äôessaye d‚Äôorganiser deux types d‚Äôatelier. Le premier est uni directionnel : Une personne pr√©sente un sujet technique et les autres en profitent. √áa permet tout d‚Äôabord de diffuser plus simplement certains messages. Par exemple, j‚Äôai organis√© une pr√©sentation de 30 mn sur l‚Äôutilisation de NULL dans le code et l‚Äôutilisation des Optional. Le deuxi√®me est plus long √† pr√©parer. C‚Äôest un atelier organis√© √† la mani√®re d‚Äôun hands on sur un sujet tr√®s pr√©cis. Pendant 1H ou 2H, l‚Äô√©quipe planche sur un sujet. La session est organis√© et anim√© id√©alement par un ou plusieurs membres de l‚Äô√©quipe ( √ßa ne vous emp√™che pas d‚Äôavoir votre mot √† dire lors de la pr√©paration üòÄ ). R√©cemment j‚Äôai co-organis√© un hands on ¬´¬†Clean Code¬†¬ª en illustrant quelques notions qui nous paraissaient essentielles. Ces √©v√®nements sont √©videmment chronophages mais offrent un certains retour sur investissement. Outre la pr√©sentation technique des diff√©rents sujets, les membres de l‚Äô√©quipe se forment et apprennent. Ils peuvent voir en situation les diff√©rentes notions que vous √©voquez (en fait je les rab√¢che) lors des MR ou pendant les revues de code. Aussi, je pense que √ßa contribue √† une certaine √©mulation technologique. √áa prend du (beaucoup de) temps, mais √ßa en vaut la peine! L‚Äôid√©al dans ce genre d‚Äôexercice est quand tout le monde propose des sujets. Pas seulement l‚Äôarchitecte ou le lead dev. Les d√©veloppeurs peuvent prendre le lead dans cet exercice. Ca permet d‚Äôune part de les valoriser, de les faire monter en comp√©tence. Quoi de mieux pour approfondir un sujet que de monter un talk et/ou hands on dessus ? ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:3:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#ateliers--workshops"},{"categories":null,"content":" 4 Revues de codeJe ne vais pas aborder dans ce chapitre les revues de code que l‚Äôon peut faire dans le cadre des projets, lors des MR par exemple. Pour certaines personnes, surtout les juniors, je fais r√©guli√®rement une revue de code alternative. Je passe une 1/2 heure, une heure max sur un bout de code que le dev m‚Äôaura s√©lectionn√©. Je lis le code avec le d√©veloppeur et je donne quelques axes d‚Äôam√©lioration: design patterns, tests unitaires, refactoring,‚Ä¶ Tout y va. √áa permet de se poser et d‚Äôaborder quelques sujets: la programmation fonctionnelle, les IO en java,‚Ä¶ ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:4:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#revues-de-code"},{"categories":null,"content":" 5 Pour aller plus loinBien √©videmment, beaucoup d‚Äôautres actions peuvent √™tre mises en place. La plupart de l‚Äôaccompagnement que je peux r√©aliser se fait quotidiennement, dans les projets. Pour aller un peu plus loin, un coll√®gue a mis en place un syst√®me de mentorat pour accompagner les jeunes d√©veloppeurs et acc√©l√©rer leur mont√©e en comp√©tence. Cette id√©e est tr√®s int√©ressante et peut √™tre appliqu√©e dans beaucoup de contextes. Si vous avez des id√©es, questions, remarques, pratiques que vous d√©veloppez chez vous, n‚Äôh√©sitez pas √† les partager! ","date":"2019-07-17","objectID":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/:5:0","series":null,"tags":null,"title":"Comment coacher des jeunes d√©veloppeurs ?","uri":"/2019/07/17/comment-coacher-des-jeunes-developpeurs/#pour-aller-plus-loin"},{"categories":null,"content":"Si vous provisionnez vos VM VirtualBox avec Vagrant, vous avez sans doute eu l‚Äôid√©e d‚Äôautomatiser le provisionning des machines virtuelles. Dans mon cas une VM GNU/Linux bas√©e sur Debian 9. Pour cela, soit vous faite tout manuellement et apr√®s les mises √† jour deviennent fastidieuses, soit vous appliquez un script shell au d√©marrage de vagrant, soit vous utilisez Ansible. Ansible est un outil opensource permettant d‚Äôautomatiser le provisionning et la mise √† jour des environnements √† distance (via SSH). L‚Äôavantage par rapport √† des outils tels que Puppet, est qu‚Äôil ne n√©cessite pas l‚Äôinstallation d‚Äôagent. Je vais essayer de vous montrer comment mettre en place le provisionning via Ansible pour VirtualBox. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:0:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#"},{"categories":null,"content":" 1 Configuration de VagrantDans le fichier Vagrantfile, on active le provisionning via Ansible: config.vm.provision \"ansible_local\" do |ansible| ansible.playbook = \"site.yml\" ansible.install_mode = \"pip\" ansible.version = \"2.7.10\" end Cette configuration fait r√©f√©rence √† un fichier ¬´¬†playbook¬†¬ª site.yml. C‚Äôest la configuration qui sera appliqu√© lors du provisionning . Que √ßa soit √† la cr√©ation ou pour les mises √† jour. Voici un exemple de contenu: - name: VirtualBox hosts: all become: yes become_user: \"root\" become_method: \"sudo\" roles: - common: vars_files: - vars/environment.yml Ce fichier est la racine de notre configuration Ansible. On y r√©f√©rence les r√¥les appliqu√©s et les fichiers d‚Äô environnement. Voici un exemple de r√¥le: - name: \"Remove useless packages from the cache\" apt: autoclean: yes force_apt_get: yes - name: \"Remove dependencies that are no longer required\" apt: autoremove: yes force_apt_get: yes - name: \"Update and upgrade apt packages (may take a while)\" become: true apt: upgrade: dist update_cache: yes force_apt_get: yes - name: \"Install useful packages\" become: true apt: name: - gcc - g++ - ... - zsh - firewalld state: present update_cache: no - name: ansible create directory example file: path: \"{{ home }}/.m2\" state: directory owner: \"{{ username }}\" group: \"{{ username }}\" - name: Install Maven settings.xml copy: src: settings.xml dest: \"{{ home }}/.m2/settings.xml\" owner: \"{{ username }}\" group: \"{{ username }}\" - name: \"Install Maven\" raw: \"curl -sL \\\"http://mirror.ibcp.fr/pub/apache/maven/maven-3/{{ maven_version }}/binaries/apache-maven-{{ maven_version }}-bin.tar.gz\\\" -o /opt/apache-maven.tar.gz \u0026\u0026 tar -zxf /opt/apache-maven.tar.gz -C /opt\" become: true become_user: root become_method: sudo - name: \"Change Maven Rights\" file: path: /opt/* state: touch modification_time: \"preserve\" access_time: \"preserve\" owner: \"{{ username }}\" group: \"{{ username }}\" Les variables d‚Äôenvironnement permettent de variabiliser certains champs de vos r√¥les. On peut trouver par exemple les versions de certains outils d√©ploy√©s maven_version: 3.5.4 username: vagrant home: /home/vagrant docker_compose_version: 1.22.0 Il y a une quantit√© impressionnante de modules Ansible que l‚Äôon peut utiliser. Que √ßa soit pour lancer des commandes shell ou lancer des services. Contrairement √† la cr√©ation d‚Äôun script shell qui pourrait faire les m√™mes actions √† la cr√©ation, on peut facilement g√©rer la mise √† jour de la VM car Ansible d√©tecte les modifications lors de son ex√©cution. ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-de-vagrant"},{"categories":null,"content":" 1.1 Configuration sp√©cifique pour VirtualBoxPour VirtualBox, j‚Äôai ajout√© deux fichiers de configuration suppl√©mentaires √† la racine: 1.1.0.1 ansible.cfg [defaults] hostfile = hosts 1.1.0.2 hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#configuration-sp√©cifique-pour-virtualbox"},{"categories":null,"content":" 1.1 Configuration sp√©cifique pour VirtualBoxPour VirtualBox, j‚Äôai ajout√© deux fichiers de configuration suppl√©mentaires √† la racine: 1.1.0.1 ansible.cfg [defaults] hostfile = hosts 1.1.0.2 hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#ansiblecfg"},{"categories":null,"content":" 1.1 Configuration sp√©cifique pour VirtualBoxPour VirtualBox, j‚Äôai ajout√© deux fichiers de configuration suppl√©mentaires √† la racine: 1.1.0.1 ansible.cfg [defaults] hostfile = hosts 1.1.0.2 hosts [local] localhost ansible_connection=local ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:1:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#hosts"},{"categories":null,"content":" 2 Provisionning","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:0","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#provisionning"},{"categories":null,"content":" 2.1 A la cr√©ationle provisionning peut se faire au lancement de vagrant via la commande: vagrant up Pour faire une mise √† jour Directement dans la box, vous pouvez lancer les commandes suivantes : sudo mount -t vboxsf vagrant /vagrant Puis, vous pouvez lancer les commandes suivantes dans la box: su - cd /vagrant export ANSIBLE_CONFIG=/vagrant ansible-playbook site.yml ","date":"2019-06-25","objectID":"/2019/06/25/ansible-pour-les-provisionner-tous/:2:1","series":null,"tags":["ansible","debian","planetlibre"],"title":"Ansible pour les provisionner tous !","uri":"/2019/06/25/ansible-pour-les-provisionner-tous/#a-la-cr√©ation"},{"categories":null,"content":"En attendant de prendre mon train, j‚Äôessaye de me remettre de cette nouvelle √©dition. Cette ann√©e JAVA est revenu au premier plan. Que √ßa soit via la sp√©cification microprofile, quarkus , graalvm ou encore par les probl√©matiques de migration JDK 8 -\u003e 11. On a pas mal vu des architectures micro services √† base de service mesh (istio) et kubernetes. A cot√© des sujets techniques, un des sujets majeurs¬†√©tait le bien √™tre et la bienveillance au travail. Les vid√©os des conf√©rences seront bient√¥t retransmises sur le channel Youtube de DevoxxFR. D‚Äôune mani√®re g√©n√©rale, le niveau des conf√©rences est toujours tr√®s bon. J‚Äôai particuli√®rement appr√©ci√© les confs suivantes. N‚Äôh√©sitez pas √† les visionnez une fois qu‚Äôelles seront disponibles sur Youtube. Cycle de vie des applications dans k8s Cr√©er facilement des microservices avec Eclipse microprofile Back to Basics, ne perdez plus de temps avec les dates Hexagonal at scale Comment concevoir une API REST D‚ÄôArchitecte √† MetaArchitecte: Une √©volution n√©cessaire Il y a aussi certaines conf√©rences ou j‚Äôai eu un bon √©cho : Oubliez JavaEE, voil√† JakartaEE Back to Basics, ne perdez plus de temps avec les dates Comprendre les GC √† faible latence Je pense qu‚Äôil y a encore bien d‚Äôautres conf√©rences qui ont √©t√© tr√®s int√©ressantes. J‚Äô ai quelques heures de visionnage √† pr√©voir dans mon agenda üôÇ . Quoi qu‚Äôil en soit, merci aux organisateurs pour cette √©dition. C‚Äô√©tait top! Rendez vous l‚Äôann√©e prochaine ! ","date":"2019-04-20","objectID":"/2019/04/20/devoxx-2019/:0:0","series":null,"tags":["conf√©rence","devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2019","uri":"/2019/04/20/devoxx-2019/#"},{"categories":null,"content":"Apr√®s avoir mis √† jour mon mot de passe Spotify ( oui, il faut modifier r√©guli√®rement ses mots de passe ) , j‚Äôai eu un petit soucis sur MoodeAudio ( version 4.4) et notamment sur la connexion avec Spotify. Apr√®s quelques recherches sur le forum de moodeaudio, j‚Äôai trouv√© la correction qui allait bien. Voici comment faire : D‚Äôabord on se connecte via SSH sur le raspberry pi $ ssh pi@192.168.0.xx Puis on lance la commande: $ sudo mv /var/local/www/spotify_cache/credentials.json /home/pi/ $ sudo reboot Normalement, Spotify Connect devrait fonctionner apr√®s le red√©marrage üôÇ ","date":"2019-03-15","objectID":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/:0:0","series":null,"tags":["moodeaudio","planetlibre","spotify"],"title":"Au secours! Spotify Connect ne fonctionne plus sur MoodeAudio","uri":"/2019/03/15/au-secours-spotify-connect-ne-fonctionne-plus-sur-moodeaudio/#"},{"categories":null,"content":"Dans la s√©rie j‚Äô√©quipe ma maison en Raspberry PI, j‚Äôai d√©cid√© de me doter d‚Äôune station radio connect√©e qui me permettrait de ¬´¬†moderniser¬†¬ª un peu ma cha√Æne HI-FI. Mes besoins sont: Connexion en analogique √† une cha√Æne HI-FI Jouer des MP3/FLAC stock√©s dans un NAS Jouer des web radios (ex. FIP, TSF JAZZ) Connexion SPOTIFY Une interface web sympa Apr√®s quelques recherches, j‚Äôai donc opt√© pour une solution bas√©e sur un DAC JustBoom, un Raspberry PI et la distribution MoodeAudio. Voici le DAC que l‚Äôon branche directement sur le port GPIO du Raspberry PI: L‚Äôinstallation et la configuration du DAC se sont tr√®s bien pass√©es. L‚Äôinstallation se fait comme avec des LEGOs. Pour la configuration, j‚Äôai test√© dans un premier temps Volumio puis MoodeAudio. Pour¬†l‚Äôinstant, je reste sur cette derni√®re. Toutes les fonctionnalit√©s que je souhaite sont en standard. Pas besoin de plugins tiers. Toutes les √©tapes d‚Äô installation et de configuration pour que le DAC soit reconnu sont d√©crites ici. Les gens de chez JustBoom ont bien document√© la configuration pour les principales distributions. Le seul reproche que je trouve √† MoodeAudio est l‚Äôergonomie. Sur un t√©l√©phone, ce n‚Äôest pas top. Surtout sur l‚Äôacc√®s aux menus d‚Äôadministration. J‚Äôai du √©galement ajouter des radios manuellement alors que dans Volumio, avec le plugin TuneIn, √ßa pouvait se faire automatiquement. Je me suis bas√© sur les informations fournies par ce site. Quoi qu‚Äôil en soit, tout ce que je souhaitais fonctionne super bien! Spotify Connect, l‚Äô√©coute de TSF JAZZ, la lecture des morceaux de ma biblioth√®que fonctionnent nickel ! ","date":"2019-03-07","objectID":"/2019/03/07/une-radio-connectee-diy/:0:0","series":null,"tags":["planetlibre","raspberry-pi"],"title":"Une radio connect√©e DIY","uri":"/2019/03/07/une-radio-connectee-diy/#"},{"categories":null,"content":"J‚Äôai fini l‚Äô√©dition 2019 du Touraine Tech. Tout d‚Äôabord, merci aux organisateurs pour l‚Äôaccueil et l‚Äôorganisation. C‚Äô√©tait vraiment top! Cette ann√©e, je n‚Äôai pas pu faire beaucoup de conf√©rences. Mon Hands on m‚Äôayant retenu une bonne partie de l‚Äôapr√®s midi, que √ßa soit durant le talk ou apr√®s pour d√©compresser üôÇ Mon hands on portait sur l‚Äôarchitecture, j‚Äôai eu une vingtaine de personnes qui l‚Äôon suivi et ont pratiqu√© sur diff√©rents sujets. {% include gallery caption=‚ÄúUn kata d‚Äôarchitecture √† TNT‚Äù layout=‚Äúhalf‚Äù %} La cave √† vin connect√© a remport√© un franc succ√®s, du moins pendant la pr√©sentation des sujets üôÇ Voici le feedback que j‚Äôai eu sur ma pr√©sentation: RDV l‚Äôann√©e prochaine ! ","date":"2019-02-02","objectID":"/2019/02/02/touraine-tech-2019-2/:0:0","series":null,"tags":["architecture","conf√©rence","handson","TNT19"],"title":"Touraine Tech 2019","uri":"/2019/02/02/touraine-tech-2019-2/#"},{"categories":null,"content":"Voici ma deuxi√®me contribution pour une s√©rie d‚Äôarticles sur l‚Äôopensource pour le blog de mon entreprise. Les articles pr√©c√©dents traitaient de l‚Äôhistoire de l‚Äôopensource puis des diff√©rentes formes que peut prendre l‚Äôopen source. Cette fois j‚Äôaborde les business models du monde opensource. Bonne lecture üôÇ ","date":"2019-01-23","objectID":"/2019/01/23/deuxieme-crossover-opensource-business-models/:0:0","series":null,"tags":["planetlibre"],"title":"Deuxi√®me crossover : Opensource business models","uri":"/2019/01/23/deuxieme-crossover-opensource-business-models/#"},{"categories":null,"content":"Voila la description de la conf√©rence/ hands que j‚Äôanimerai au Touraine Tech est en ligne üôÇ Vous trouverez le descriptif sur cette page. ","date":"2019-01-14","objectID":"/2019/01/14/objectif-top-architecte/:0:0","series":null,"tags":["tourainetech"],"title":"Objectif Top Architecte !","uri":"/2019/01/14/objectif-top-architecte/#"},{"categories":null,"content":"Mon sujet de talk ¬´¬†Objectif Top Architecte¬†¬ª a √©t√© retenu pour l‚Äô√©dition 2019 de Touraine Tech. R√©servez le 1 f√©vrier 2019 dans votre agenda ! Tout d‚Äôabord merci aux organisateurs pour leur confiance. Je suis vraiment¬†honor√© d‚Äô√™tre s√©lectionn√© une deuxi√®me ann√©e cons√©cutive. Cette ann√©e, j‚Äôanimerai un hands on sur l‚Äôarchitecture. Je vais t√¢cher de vulgariser quelques principes qui me paraissent importants et animer un ¬´¬†coding dojo de l‚Äôarchitecture¬†¬ª. Pas besoin d‚Äô√™tre architecte ou (vraiment, ‚Ä¶ mais vraiment pas besoin) d‚Äôavoir une certification TOGAF pour y participer üôÇ ","date":"2018-12-25","objectID":"/2018/12/25/touraine-tech-2019/:0:0","series":null,"tags":["architecture","conf√©rence","tourainetech"],"title":"Touraine Tech 2019","uri":"/2018/12/25/touraine-tech-2019/#"},{"categories":null,"content":"Il y a quelques jours, je cherchais comment tracer rapidement et simplement les entr√©es sorties d‚Äôune API REST en appliquant quelques formatages, des filtres, et des insertions en base si besoin. Travaillant sur une stack SpringBoot, vous allez me dire : oui tu peux faire des filtres. Pour √™tre franc, j‚Äôai essay√© d‚Äô appliquer des interceptor et filtres mais dans mon contexte, √ßa ne collait pas. Me voil√† donc √† la recherche d‚Äôune solution faisant le taff et qui soit peu intrusive dans mon contexte. J‚Äôai trouv√© par hasard au fil de mes lectures sur Stackoverflow le framework logbook r√©alis√© par ‚Ä¶ Zalando ( et oui, ils ne font pas que des chaussures) en licence MIT.¬†Ce composant ne fait qu‚Äôune seule chose, mais il le fait bien ! Il permet entre autres de s‚Äôint√©grer dans une stack JAVA ( JAX-RS ou SpringMVC), de filtrer, r√©cup√©rer les diff√©rentes informations des requ√™tes et r√©ponses et enfin de formatter selon l‚Äôenvie (ex. JSON). Voici un exemple de mise en ≈ìuvre dans un projet SpringBoot: Dans le¬†fichier pom.xml, ajouter cette d√©pendance: \u003cdependency\u003e \u003cgroupId\u003eorg.zalando\u003c/groupId\u003e \u003cartifactId\u003elogbook-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.11.2\u003c/version\u003e \u003c/dependency\u003e Dans une de vos classes Configuration, d√©finir la factory de Logbook @Bean public Logbook createLogBook() { // too easy : return Logbook.create(); return Logbook.builder() .condition(Conditions.requestTo(\"/helloworld\")) .formatter(new JsonHttpLogFormatter()).build(); } Dans mon cas j‚Äôai fait un filtre en n‚Äôincluant que l‚Äô API /helloworld et j‚Äôai formatt√© en JSON. On peut √©galement modifier le processus d‚Äô√©criture pour ne pas √©crire dans un fichier mais en base par ex. Ensuite, j‚Äôai ajout√© la configuration du logger dans le fichier application.properties logging.level.org.zalando.logbook:TRACE Et voila ! Dans la console, lors d‚Äôun appel ou d‚Äôune r√©ponse √† mon API, j‚Äôai le message suivant : 018-12-01 15:14:18.373 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"remote\",\"type\":\"request\",\"correlation\":\"c6b345013835273f\",\"protocol\":\"HTTP/1.1\",\"remote\":\"127.0.0.1\",\"method\":\"GET\",\"uri\":\"http://127.0.0.1:8080/helloworld\",\"headers\":{\"accept\":[\"/\"],\"host\":[\"127.0.0.1:8080\"],\"user-agent\":[\"curl/7.52.1\"]}} 2018-12-01 15:14:18.418 TRACE 3605 --- [nio-8080-exec-1] org.zalando.logbook.Logbook : {\"origin\":\"local\",\"type\":\"response\",\"correlation\":\"c6b345013835273f\",\"duration\":48,\"protocol\":\"HTTP/1.1\",\"status\":200,\"headers\":{\"Content-Length\":[\"11\"],\"Content-Type\":[\"text/plain;charset=UTF-8\"],\"Date\":[\"Sat, 01 Dec 2018 14:14:18 GMT\"]},\"body\":\"Hello world\"} Vous remarquerez que les requ√™tes / r√©ponses peuvent d√©sormais √™tre associ√©s gr√¢ce √† un identifiant de corr√©lation. On peut facilement d√©terminer le temps de traitement d‚Äôune requ√™te ou encore faciliter les recherches. Vous trouverez tout le code dans ce repo github. ","date":"2018-12-01","objectID":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/:0:0","series":null,"tags":["logbook","planetlibre","spring","springboot"],"title":"Tracer (facilement) les entr√©es sorties d'une API REST","uri":"/2018/12/01/tracer-facilement-les-entrees-sorties-dune-api-rest/#"},{"categories":null,"content":"En attendant d‚Äôavoir plus d‚Äôimagination, voici un rapide tuto pour g√©rer plusieurs r√©f√©rentiels GIT avec des cl√©s SSH diff√©rentes. Imaginons que vous deviez vous connecter sur diff√©rents serveurs GIT (ex. github et gitlab) avec des emails diff√©rents et donc des cl√©s RSA diff√©rentes ( oui je sais ce cas n‚Äôarrive pas souvent ). Le tout sous Windows et GNU/LINUX. Sous GNU/LINUX ont peut le g√©rer diff√©remment via la commande ssh-add. Pour pouvoir g√©rer ceci de mani√®re simple, j‚Äôai fait la manipulation suivante : Dans le r√©pertoire ~/.ssh, j‚Äôai cr√©e les diff√©rentes cl√©s avec la doc fournie par GITHUB. Puis, j‚Äôai cr√©e le fichier ~/.ssh/config avec le contenu suivant: Host monhost1.fr HostName monhost1.fr User git IdentityFile ~/.ssh/id_rsa Host monhost2.fr HostName monhost2.fr User git IdentityFile ~/.ssh/nouvellecle_rsa Et voil√† ! Apr√®s avoir fait les diff√©rentes configurations cot√© serveur (c.-a-d. ajout des cl√©s publiques), je peux interagir avec les diff√©rents serveurs (pull, push). En esp√©rant que √ßa puisse servir √† d‚Äôautres. ","date":"2018-11-16","objectID":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/:0:0","series":null,"tags":["git","planetlibre"],"title":"G√©rer plusieurs cl√©s et plusieurs repo GIT","uri":"/2018/11/16/gerer-plusieurs-cles-et-plusieurs-repo-git/#"},{"categories":null,"content":"je n‚Äôai pas √©crit beaucoup de choses sur mon blog ces derniers temps. C‚Äô√©tait en partie d√ª au fait que j‚Äô√©tais en train d‚Äô√©crire un article avec R. SEMETEYS pour le blog de mon entreprise. Cet article est disponible ici. Il essaye de synth√©tiser l‚Äôhistoire de l‚Äôopen source. J‚Äôesp√®re que vous ne serez pas rebut√© par l‚Äôanglais ( c‚Äôest un exercice üôÇ ) Bonne lecture üôÇ ","date":"2018-10-29","objectID":"/2018/10/29/premier-cross-over/:0:0","series":null,"tags":["planetlibre"],"title":"Premier cross over \u0026#8230;","uri":"/2018/10/29/premier-cross-over/#"},{"categories":null,"content":"Bon, √ßa fait quelques temps que je n‚Äôai rien post√©‚Ä¶ Voici un rapide tuto pour installer docker-ce sur une debian9. Oui, je sais, docker est d√©j√† pr√©sent sur les d√©p√¥ts, mais si vous souhaitez avoir une version un peu plus r√©cente, vous pouvez passer par l‚Äôinstallation de la version ce fournie par docker. ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:0:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#"},{"categories":null,"content":" 1 Pr√©-requisSupprimer les √©ventuelles installations de docker et docker-compose #apt-get remove docker docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:1:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#pr√©-requis"},{"categories":null,"content":" 2 InstallationLancer les commandes suivantes: # apt-get install apt-transport-https ca-certificates # curl -fsSL https://download.docker.com/linux/debian/gpg | apt-key add \u0026#8211; # add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/debian \\ $(lsb_release -cs) \\ stable\" Puis lancer # apt update # apt install docker-ce ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation"},{"categories":null,"content":" 2.1 Installation de docker-composeLancer les commandes suivantes: # curl -L \"https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # chmod a+x /usr/local/bin/docker-compose ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:2:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#installation-de-docker-compose"},{"categories":null,"content":" 3 Configuration des droitsPour lancer docker depuis un utiliser non root, il faut lancer les commandes suivantes: # groupadd docker # adduser monutilisateur docker # usermod -aG docker monutilisateur Apr√®s ceci, vaut mieux red√©marrer le pc ‚Ä¶ ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:3:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-des-droits"},{"categories":null,"content":" 4 Configuration du d√©monVoici quelques config √† appliquer pour que le d√©mon soit accessible par des outils tels que le plugin maven¬†ou encore configurer l‚Äôacc√®s √† un proxy ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-d√©mon"},{"categories":null,"content":" 4.1 Configuration du portEx√©cuter la commande: # systemctl edit docker.service Entrer le code suivant: [Service] ExecStart= ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock Et l‚Äôenregistrer sous /etc/systemd/system/docker.service.d/docker.conf ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:1","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-port"},{"categories":null,"content":" 4.2 Configuration du proxyAvec la m√™me commande # systemctl edit docker.service Entrer la configuration suivante: [Service] Environment=\"HTTP\\_PROXY=http://mon\\_proxy:mon_port/\" Environment=\"NO_PROXY=127.0.0.1\" ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:2","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#configuration-du-proxy"},{"categories":null,"content":" 4.3 Activation des configurationsLancer les commandes suivantes: # systemctl daemon-reload # systemctl restart docker ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:4:3","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#activation-des-configurations"},{"categories":null,"content":" 5 ValidationMaintenant, vous pouvez valider votre configuration avec la commande: $ docker run hello-world ","date":"2018-09-26","objectID":"/2018/09/26/installer-docker-ce-sur-debian-9/:5:0","series":null,"tags":["debian","docker","planetlibre"],"title":"Installer docker ce sur Debian 9","uri":"/2018/09/26/installer-docker-ce-sur-debian-9/#validation"},{"categories":null,"content":"L‚Äô√©dition 2018 de DEVOXX touche bient√¥t √† sa fin. Pour ceux qui ne connaissent pas cette conf√©rence, c‚Äôest LA conf√©rence sur le d√©veloppement en France. A titre personnel, je peux plus apprendre en trois jours √† cette conf√©rence qu‚Äôen formation. Tout d‚Äôabord un grand merci aux organisateurs. Ils assurent r√©ellement. Bon, pour l‚Äôann√©e prochaine, n‚Äôh√©sitez √† retenir ma conf√©rence üòâ Si vous n‚Äôavez pas eu la chance d‚Äôassister aux trois jours, il faut savoir que vous pourrez voir les rediff sur la chaine youtube. ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:0:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#"},{"categories":null,"content":" 1 Les tendancesVoici les tendances que¬†j‚Äôai retenu : Spring, spring et encore spring Du r√©actif en veux tu en voila Du DDD sinon rien Du devops et le plus impressionnant pour moi √©tait la conf√©rence de JOSHUA BLOCH(!!!) sur Effective Java. Pas tant dans le contenu, car il reprenait peu ou prou celui du livre, mais de voir une personne de ce calibre (dans le monde JAVA, c‚Äôest une rock star) en France, c‚Äôest assez impressionnant. Les keynotes ( dont celle sur le smart building) √©taient dans l‚Äôensemble tr√®s int√©ressantes. Les conf√©rences √©taient √©galement d‚Äôun tr√®s bon niveau. J‚Äôai pu d√©couvrir par exemple l‚Äôavance que peut avoir l‚ÄôEstonie sur l‚ÄôIT ( voir le projet x-road )¬†par rapport √† la France qui lance le projet french road. Voici quelques conf√©rences qui m‚Äôont plu et interpel√© : √ätre architecte en 2018 Le smart building Chaos Engineering Effective java Pourquoi vous avez besoin d‚Äôune clean architecture S√©curit√© des web applications Apr√®s java 8, java 9 et 10 Je ne vais pas trop les d√©crire ( voire pas du tout ), elles seront disponibles prochainement sur¬†la chaine youtube . ","date":"2018-04-20","objectID":"/2018/04/20/devoxx-2018/:1:0","series":null,"tags":["devoxx","devoxxfr","java","planetlibre"],"title":"Devoxx 2018","uri":"/2018/04/20/devoxx-2018/#les-tendances"},{"categories":null,"content":"Depuis quelques jours, je teste Apache Camel pour la mise en ≈ìuvre¬†de m√©diations. Apache Camel est un framework assez ancien. Il est similaire √† Spring Int√©gration et permet l‚Äô impl√©mentation de patterns d‚Äôint√©gration. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:0:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#"},{"categories":null,"content":" 1 Les patterns d‚Äôint√©grationQu‚Äôest-ce qu‚Äôun pattern d‚Äôint√©gration allez-vous me dire ? C‚Äôest une solution d‚Äôarchitecture ou plus simplement une recette de cuisine permettant d‚Äôavoir une solution toute pr√™te √† une probl√©matique d‚Äôint√©gration donn√©e. L‚Äôensemble de ces patterns est d√©crit sur ce site ( ne vous attardez pas sur le look des ann√©es 90 ‚Ä¶ ). Exemple : Camel permet simplement de g√©rer l‚Äôint√©gration via un DSL. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#les-patterns-dint√©gration"},{"categories":null,"content":" 1.1 Choix d‚Äôimpl√©mentationsOn peut faire pas mal de choses avec ce FRAMEWORK et de plusieurs mani√®res. J‚Äôai fait les choix d‚Äôimpl√©mentation suivants : Tout se fera avec SPRING ‚Ä¶ et pas en XML üôÇ Il faut que toutes les m√©diations soient testables J‚Äôex√©cute le code dans un FATJAR ( pourquoi avec springboot ) ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:1:1","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#choix-dimpl√©mentations"},{"categories":null,"content":" 2 Configuration de la routeApache Camel d√©finit les m√©diations dans des routes. Elles se d√©finissent assez rapidement . Les routes commencent par une instruction from et se terminent par une ou plusieurs instructions to. Pour mon exemple, j‚Äôextrais les donn√©es d‚Äôune table et les stocke dans un fichier. Tout se configure par des URLs. La premi√®re permet d‚Äôextraire les donn√©es via JPA/HIBERNATE. Une entit√© Address permet le requ√™tage. La seconde permet le stockage dans un fichier texte JSON. Elles sont externalis√©es dans des fichiers de configuration pour faciliter les tests et accessibles via SPRING. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:2:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#configuration-de-la-route"},{"categories":null,"content":" 3 Lancement de la routeLe lancement de la route se fait dans une m√©thode main() : ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:3:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#lancement-de-la-route"},{"categories":null,"content":" 4 TestsCamel fournit une API de test assez bien fournie. Elle permet notamment de mocker des endpoints existants (ex. : le fichier de sortie de mon cas de test). Dans mon cas, j‚Äôai d√©cid√© de remplacer la base de donn√©es que j‚Äôinterroge en input par une base HSQLDB charg√©e en m√©moire. Le fichier de sortie est, lui, remplac√© dynamiquement par un mock. Pour ce faire, j‚Äôai utilis√© les ¬´¬†adviceWith¬†¬ª ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:4:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#tests"},{"categories":null,"content":" 5 Pour aller plus loinIl y a pas mal d‚Äôexemples sur le GITHUB de CAMEL. Vous pouvez √©galement acheter le livre ¬´¬†Camel In Action¬†¬ª. Ca ne vaut pas Effective Java üôÇ , mais vu qu‚Äôil est √©crit par le principal d√©veloppeur, c‚Äôest une tr√®s bonne r√©f√©rence. ","date":"2018-04-10","objectID":"/2018/04/10/integration-et-mediation-avec-apache-camel/:5:0","series":null,"tags":["camel","integration","planetlibre"],"title":"Int√©gration et m√©diation avec Apache Camel","uri":"/2018/04/10/integration-et-mediation-avec-apache-camel/#pour-aller-plus-loin"},{"categories":null,"content":"Et oui, il y a un equalizer dans debian‚Ä¶.Pulse Audio dispose d‚Äôun equalizer. Bon ce n‚Äôest encore tr√®s user friendly, mais √ßa fonctionne! ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:0:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#"},{"categories":null,"content":" 1 Installation de l‚Äôequalizer apt-get install pulseaudio-equalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:1:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#installation-de-lequalizer"},{"categories":null,"content":" 2 ActivationAjouter les lignes suivantes dans le fichier /etc/pulse/default.pa load-module module-equalizer-sink load-module module-dbus-protocol Relancer le d√©mon pulseaudio \\# pulseaudio -k \u0026\u0026 pulseaudio -D A ce stade, vous devriez avoir dans le panneau de configuration la r√©f√©rence √† l‚Äôequalizer ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:2:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#activation"},{"categories":null,"content":" 3 LancementEn ligne de commande ( je vous disais que ce n‚Äô√©tait pas trop user-friendly), lancer la commande $ qpaeq \u0026 On obtient cette interface: Arriv√© √† ce niveau, je suis quand m√™me un peu d√©√ßu/ Il n‚Äôy a pas une vrai int√©gration dans debian ( pas de lanceur pour l‚Äôequalizer ) et il n‚Äôy a pas de presets configur√©s ( #souvienstoiwinamp) J‚Äôai essay√© de poster mon soucis sur IRC, mais je n‚Äôai pas encore eu de r√©ponse. Je pense soumettre un bug dans les prochains jours. ","date":"2018-03-25","objectID":"/2018/03/25/activer-l-equalizer-sur-debian-9/:3:0","series":null,"tags":["debian","planetlibre","pulseaudio"],"title":"Activer l' equalizer sur Debian 9","uri":"/2018/03/25/activer-l-equalizer-sur-debian-9/#lancement"},{"categories":null,"content":"Vagrant est un outil permettant de construire des environnements de travail virtualis√©s h√©berg√©s sur vmware, virtualbox ou encore docker. Il permet par exemple de construire et g√©rer une VM dans un seul et m√™me workflow et d‚Äô√©viter les exports et partages de machines virtuelles ( tout est d√©clar√© dans un seul et m√™me fichier ). Voici comment je l‚Äôai install√© sur ma debian 9. ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:0:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#"},{"categories":null,"content":" 1 InstallationLe paquet fourni dans la distribution n‚Äôest pas compatible avec la version de virtualbox fournie dans le repo virtualbox.org. j‚Äôai donc install√© la version disponible sur le site de vagrant. # dpkg -i vagrant\\_2.0.2\\_x86_64.deb ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:1:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation"},{"categories":null,"content":" 2 Configuration","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#configuration"},{"categories":null,"content":" 2.1 ProxySi vous avez un proxy, il faut effectuer le param√©trage suivant $ export http_proxy=¬†¬ªhttp://user:password@host:port¬†¬ª $ export https_proxy=¬†¬ªhttp://user:password@host:port¬†¬ª $ vagrant plugin install vagrant-proxyconf $ export VAGRANT\\_HTTP\\_PROXY=¬†¬ªhttp://user:password@host:port¬†¬ª $ export VAGRANT\\_NO\\_PROXY=¬†¬ª127.0.0.1\u0026Prime; $vagrant box add \\ precise64 https://files.hashicorp.com/precise64.box `$ export VAGRANT_DEFAULT_PROVIDER`=virtualbox [/code] ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:2:1","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#proxy"},{"categories":null,"content":" 3 Installation d‚Äôune VMVoici un exemple pour une VM virtualbox bas√©e sur ubuntu $ mkdir ~/vagrant $ cd ~/vagrant $ vagrant init pristine ubuntu-budgie-17-x64 $ vagrant up [/code] Avec ces quelques commandes j‚Äôobtiens un environnement ubuntu h√©berg√© sur virtualbox sans avoir √† installer et configurer la vm. Pour l‚Äôinstant je ne rentre pas trop dans les d√©tails de la construction des images. Peut-√™tre que je m‚Äôy plongerai prochainement ","date":"2018-03-15","objectID":"/2018/03/15/installation-de-vagrant/:3:0","series":null,"tags":["planetlibre","vagrant"],"title":"Installation de Vagrant","uri":"/2018/03/15/installation-de-vagrant/#installation-dune-vm"},{"categories":null,"content":"J‚Äôai eu la chance d‚Äô√™tre s√©lectionn√© pour la premi√®re √©dition de la conf√©rence TouraineTech. Tout d‚Äôabord, je tiens √† remercier toute l‚Äô√©quipe du Touraine Tech pour l‚Äôaccueil et l‚Äôorganisation de cette conf√©rence. Ma pr√©sentation s‚Äôintitulait: Jenkins2 le retour (d‚Äôexp√©rience). Je faisais un retour d‚Äôexp√©rience sur la mise en ≈ìuvre de Jenkins 2 et des pipelines. Elle √©tait au format quickie¬†(15mn). J‚Äôai pas mal pr√©par√© la pr√©sentation car c‚Äô√©tait ma premi√®re dans ce domaine. Je trouve que √ßa s‚Äôest pas trop mal pass√©. J‚Äôai fait quelques erreurs dans mes slides ou tout du moins je trouve que je n‚Äôai pas eu l‚Äôeffet escompt√©. Quoi qu‚Äôil en soit, je suis plut√¥t content du r√©sultat. Les retours ont √©t√© assez satisfaisants. Voici le retour des participants : Pour ceux qui regrettaient de ne pas avoir de d√©mos, j‚Äôen suis d√©sol√©, mais le format de 15mn ne s‚Äôy pr√™tait pas trop . Si j‚Äôavais eu plus de temps, j‚Äôaurai fait des d√©monstrations qui auraient beaucoup mieux illustr√© mon propos. Pour conclure, je pense r√©√©diter cette exp√©rience. √áa m‚Äôa vraiment plu. Je n‚Äôai plus qu‚Äô√† trouver un sujet pour l‚Äô√©dition 2019 de Touraine Tech üôÇ ","date":"2018-02-26","objectID":"/2018/02/26/ma-presentation-au-touraine-tech/:0:0","series":null,"tags":["planetlibre","tourainetech"],"title":"Ma pr√©sentation au Touraine Tech","uri":"/2018/02/26/ma-presentation-au-touraine-tech/#"},{"categories":null,"content":"Dans la s√©rie, j‚Äôessaye de sauvegarder toutes mes configurations, voici ce que j‚Äôai fait pour configurer correctement cygwin. Pour ceux qui ne connaissent pas ou qui n‚Äôont pas la chance d‚Äôutiliser windows au travail, cygwin est un shell avec tous les outils GNU. En attendant d‚Äôavoir windows 10 ( au travail ) et un BASH int√©gr√©, il n‚Äôy a pas mieux. Du moins √† mon humble avis. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:0:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#"},{"categories":null,"content":" 1 GIT","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#git"},{"categories":null,"content":" 1.1 Compl√©tionOn a besoin des fichiers suivants git-completion.bash git-prompt.sh Je les ai t√©l√©charg√© et plac√© dans le r√©pertoire $HOME. ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:1","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#compl√©tion"},{"categories":null,"content":" 1.2 Activation de la configuration et affichage de la branche en cours dans le promptJ‚Äôai activ√© la configuration git en ex√©cutant les scripts pr√©c√©demment t√©l√©charg√©s. Voici la personnalisation que j‚Äôai param√©tr√© dans la variable d‚Äôenvironnement PS1: J‚Äô ai √©galement activ√© des propri√©t√©s qui √©taient en commentaire dans ce fichier. Je ne les ai pas list√©e pour ne pas trop surcharger l‚Äôarticle üôÇ ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:2","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#activation-de-la-configuration-et-affichage-de-la-branche-en-cours-dans-le-prompt"},{"categories":null,"content":" 1.3 Configuration Nom et s√©curit√© ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:1:3","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#configuration-nom-et-s√©curit√©"},{"categories":null,"content":" 2 VIMQue serait un prompt sans vim ? J‚Äôai install√© une suite de plugin : The ultimate vimrc. Il faut cloner le repo GIT et lancer un script git clone --depth=1 https://github.com/amix/vimrc.git ~/.vim_runtime sh ~/.vim_runtime/install_awesome_vimrc.sh ","date":"2018-02-16","objectID":"/2018/02/16/ma-configuration-cygwin/:2:0","series":null,"tags":["cygwin","git","gnu/linux","planetlibre"],"title":"Ma configuration CYGWIN","uri":"/2018/02/16/ma-configuration-cygwin/#vim"},{"categories":null,"content":"D√©sol√© de remettre √ßa. Je remets sur mon blog ma configuration Debian. Histoire de ne pas la perdre tant qu‚Äôelle est dans mon historique . Voici ce que j‚Äôai r√©alis√© post-installation: ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:0:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#"},{"categories":null,"content":" 1 Ajout d√©p√¥ts suppl√©mentairesDans le fichier /etc/apt/sources.list, ajouter les repo contrib et non-free . Activer √©galement les mises √† jour de s√©curit√©. deb http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch main non-free contrib deb http://security.debian.org/debian-security stretch/updates main non-free contrib deb-src http://security.debian.org/debian-security stretch/updates main non-free contrib # stretch-updates, previously known as 'volatile' deb http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib deb-src http://ftp.fr.debian.org/debian/ stretch-updates main non-free contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:1:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#ajout-d√©p√¥ts-suppl√©mentaires"},{"categories":null,"content":" 2 Logiciels tiers","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#logiciels-tiers"},{"categories":null,"content":" 2.1 Etcher#echo \"deb https://dl.bintray.com/resin-io/debian stable etcher\" | sudo tee /etc/apt/sources.list.d/etcher.list \u003cpre\u003e#apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 379CE192D401AB61 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#etcher"},{"categories":null,"content":" 2.2 Virtualbox # wget -q https://www.virtualbox.org/download/oracle_vbox_2016.asc -O- | sudo apt-key add - Dans le fichier /etc/apt/sources.list.d/virtualbox.list deb https://download.virtualbox.org/virtualbox/debian stretch contrib ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#virtualbox"},{"categories":null,"content":" 2.3 SpotifyDans le fichier /etc/apt/sources.list.d/spotify.list deb http://repository.spotify.com stable non-free ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:2:3","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#spotify"},{"categories":null,"content":" 3 Installation paquets suppl√©mentaires # apt-get update # apt-get install firmware-iwlwifi virtualbox-5.2\\ ttf-mscorefonts-installer easytag tuxguitar-jsa htop\\ frescobaldi gparted grsync ntfs-config chromium autofs\\ openjdk-8-jdk openjdk-8-jre gnome-tweak-tool ntfs-config \\ ntfs-3g cifs-utils geogebra-gnome arduino libmediainfo \\ libmediainfo0v5 network-manager-openvpn-gnome dirmngr \\ spotify-client spotify-client-gnome-support \\ etcher apt-transport-https etcher-electron vim \\ fonts-powerline audacity ffmpeg lame unrar rar gdebi \\ sound-juicer traceroute scala net-tools nmap \\ gnome-shell-pomodoro hplip dig dnsutils build-essential \\ linux-headers-amd64 firmware-linux-nonfree lshw ethtool \\ libsane-hpaio xsane autofs vlc ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:3:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-paquets-suppl√©mentaires"},{"categories":null,"content":" 4 Configuration autofsPour ceux qui ne connaissent pas , autofs est un outil permettant de monter directement des partages nfs et cicfs √† l‚Äôutilisation et non au d√©marrage de l‚Äôordinateur. Dans le fichier /etc/auto.master /mnt/SERV1/nfs /etc/auto.nfs --ghost, --timeout=60 /mnt/SERV1/cifs /etc/auto.SERV1.cifs --ghost, --timeout=60 /mnt/SERV2 /etc/auto.cifs --ghost, --timeout=60 ensuite ins√©rer la configuration ad√©quate dans les fichiers r√©f√©renc√©s : ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#configuration-autofs"},{"categories":null,"content":" 4.1 auto.cicfs data -fstype=cifs,credentials=/home/USER/.cred-file,user=littlewing,uid=1000,gid=1000 ://192.168.0.XX/REPERTOIRE Les identifiants / mots de passe sont stock√©s dans un fichier .cred-file stock√© √† la racine du r√©pertoire utilisateur. Voici un exemple : username=user password=password Le fichier auto.SERV1.cifs reprend la m√™me structure ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autocicfs"},{"categories":null,"content":" 4.2 auto.nfs REP1 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP1 REP2 -fstype=nfs,rw,intr 192.168.0.XX:/volume1/REP2 ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:4:2","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#autonfs"},{"categories":null,"content":" 5 Installation d‚ÄôatomJ‚Äôai choisi d‚Äôinstaller atom via le package .deb fourni par github. Afin d‚Äôautomatiser l‚Äôinstallation et la mise √† jour, voici le script que j‚Äôai r√©alis√© : #!/bin/sh SETUP_ROOT=/tmp wget -O $SETUP_ROOT/atom.deb \"https://atom.io/download/deb\" echo \"Installation du paquet...\" dpkg -i $SETUP_ROOT/atom.deb echo \"Fini :)\" Ce script est plac√© dans le r√©pertoire /usr/local/sbin et lanc√© comme suit : # upgrade-atom.sh ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:5:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-datom"},{"categories":null,"content":" 6 Installation de FirefoxAfin d‚Äôavoir la derni√®re version de firefox, voici le script que j‚Äôai r√©alis√©: #!/bin/sh SETUP_ROOT=/tmp BIN_ROOT=/usr/local/firefox DATE=`date +%Y-%m-%d` OLD_EXE=/usr/lib/firefox-esr/firefox-esr wget -O $SETUP_ROOT/FirefoxSetup.tar.bz2 \"https://download.mozilla.org/?product=firefox-latest\u0026os=linux64\u0026lang=fr\" echo \"Extraction de l'archive...\" tar xjf $SETUP_ROOT/FirefoxSetup.tar.bz2 -C /usr/local echo \"Changement des droits utilisateur\" chown -R :users $BIN_ROOT chmod a+x $BIN_ROOT/firefox echo \"Sauvegarde de l'ancien binaire et Creation des liens symboliques\" if [ -e $OLD_EXE ] then OLD_BINARY=${OLD_EXE}_orig_${DATE} mv $OLD_EXE $OLD_BINARY fi ln -s $BIN_ROOT/firefox $OLD_EXE chmod a+x $OLD_EXE echo \"Fini :)\" ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:6:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#installation-de-firefox"},{"categories":null,"content":" 7 MinecraftVoila l‚Äô√©tape la plus importante, du moins pour mes enfants ‚Ä¶ J‚Äôai cr√©e le script /usr/local/bin/minecraft.sh #!/bin/bash cd /usr/local/minecraft java -Xmx1G -Xms512M -cp /usr/local/minecraft/Minecraft.jar net.minecraft.bootstrap.Bootstrap J‚Äôai plac√© le JAR en question dans le r√©pertoire /usr/local/minecraft. Enfin, j‚Äôai cr√©e le fichier ¬´¬†lanceur gnome¬†¬ª /usr/share/applications/minecraft.desktop [Desktop Entry] Name=Minecraft Comment= Categories=Game;BoardGame; Exec=/usr/local/bin/minecraft.sh Icon=Minecraft_Block Terminal=false Type=Application StartupNotify=true J‚Äôai √©galement mis une icone SVG dans le r√©pertoire /usr/share/icons/ ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:7:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#minecraft"},{"categories":null,"content":" 8 Optimisation du bootApr√®s toutes ces installations, il faut v√©rifier que les performances, notamment au d√©marrage ne sont pas trop alt√©r√©es Pour avoir le d√©tail du boot, il faut utiliser la commande systemd-analyze #systemd-analyze blame 8.113s NetworkManager-wait-online.service 2.549s apt-daily-upgrade.service 803ms networking.service 228ms colord.service 213ms dev-sda1.device 145ms systemd-timesyncd.service 128ms ModemManager.service 102ms autofs.service .... On peut √©galement voir le chemin critique avec cette commande: #systemd-analyze critical-chain The time after the unit is active or started is printed after the \"@\" character. The time the unit takes to start is printed after the \"+\" character. graphical.target @8.944s ‚îî‚îÄmulti-user.target @8.944s ‚îî‚îÄautofs.service @8.841s +102ms ‚îî‚îÄnetwork-online.target @8.841s ‚îî‚îÄNetworkManager-wait-online.service @723ms +8.113s ‚îî‚îÄNetworkManager.service @642ms +80ms ‚îî‚îÄdbus.service @612ms ‚îî‚îÄbasic.target @612ms ‚îî‚îÄpaths.target @612ms ‚îî‚îÄacpid.path @610ms ‚îî‚îÄsysinit.target @608ms ‚îî‚îÄsystemd-backlight@backlight:acpi_video0.service @1.042s +8ms ‚îî‚îÄsystem-systemd\\x2dbacklight.slice @1.042s ‚îî‚îÄsystem.slice @119ms ‚îî‚îÄ-.slice @108ms ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#optimisation-du-boot"},{"categories":null,"content":" 8.1 D√©sactivation des servicesPar exemple, si vous voulez d√©sactiver le service virtualbox au d√©marrage # systemctl disable vboxautostart-service.service et ainsi de suite pour tous les services inutiles au d√©marrage ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:8:1","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#d√©sactivation-des-services"},{"categories":null,"content":" 9 Analyse du d√©marrage d‚Äôun servicePour analyser le d√©marrage d‚Äôun service, on peut utiliser la commande journalctl # journalctl -b -u NetworkManager-wait-online.service ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:9:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#analyse-du-d√©marrage-dun-service"},{"categories":null,"content":" 10 ConclusionApr√®s toutes ces √©tapes, j‚Äôai un syst√®me op√©rationnel. Il manque pas mal d‚Äôoutils ( ex. maven, npm, intellij,‚Ä¶). Ces outils tiennent plus du poste de d√©veloppement. ","date":"2018-02-10","objectID":"/2018/02/10/ma-configuration-debian-9/:10:0","series":null,"tags":["planetlibre","debian"],"title":"Ma Configuration Debian 9","uri":"/2018/02/10/ma-configuration-debian-9/#conclusion"},{"categories":null,"content":"Bon, mon site part en carafe. J‚Äôai donc d√©cid√© de changer d‚Äôh√©bergeur.Pour faire bref, je quitte l‚Äôautre.net et je migre mon blog sur wordpress.com. Bien √©videmment, je n‚Äôai aucune sauvegarde‚Ä¶Je repars de z√©ro A bient√¥t! ","date":"2018-02-09","objectID":"/2018/02/09/migration-de-mon-blog/:0:0","series":null,"tags":null,"title":"Migration de mon blog","uri":"/2018/02/09/migration-de-mon-blog/#"},{"categories":null,"content":"I expose on this blog my tests, technical musings and explorations. Among other things, it helps me remind me what I did months/years ago and sharing it to the community. By default, the content is published under the Creative Commons CC BY license. Obviously, I accept remarks, fixes (nobody is perfect). However, I reserve the right to publish or not a comment. Usually, I publish it except if I consider it as a troll. Stop You probably understood it‚Äôs a personal blog and not a professional one. I publish also articles on the Worldline Technical Blog. Finally, views and opinions exposed on this blog are my own and not of my employer. And now, enjoy! üôÇ ","date":"2018-02-08","objectID":"/about/:0:0","series":null,"tags":null,"title":"About","uri":"/about/#"},{"categories":null,"content":" Photo by Jac Alexandru You can reach / follow me through: ¬†Twitter/X ¬†LinkedIn ¬†GitHub ¬†BlueSky ¬†Mastodon ¬†Youtube ","date":"2018-02-08","objectID":"/contact/:0:0","series":null,"tags":null,"title":"Contact","uri":"/contact/#"}]